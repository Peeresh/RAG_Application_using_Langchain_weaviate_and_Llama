{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5f697262aa90471bb0b4ca72e86d31f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_095ab6795be54ff4a518e7655ef5354c",
              "IPY_MODEL_67f0762286eb46069288e4824b3ba56c",
              "IPY_MODEL_93b542d18ec34aef975a551eb459b357"
            ],
            "layout": "IPY_MODEL_08c35bb20afa443f8770697b18676849"
          }
        },
        "095ab6795be54ff4a518e7655ef5354c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2dc8c7d9adb84fed808ea734525a4e83",
            "placeholder": "​",
            "style": "IPY_MODEL_393bbee2c3b94a8e9213c24685762b33",
            "value": "modules.json: 100%"
          }
        },
        "67f0762286eb46069288e4824b3ba56c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9b800855e7f425a91437e7017052cc3",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f1fa05a10b5c4a84b988f5dba0f82522",
            "value": 349
          }
        },
        "93b542d18ec34aef975a551eb459b357": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7278c5e2b1544de9b86c848551000d94",
            "placeholder": "​",
            "style": "IPY_MODEL_6420b6abcc7e43a5811bfd98091c093b",
            "value": " 349/349 [00:00&lt;00:00, 25.6kB/s]"
          }
        },
        "08c35bb20afa443f8770697b18676849": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2dc8c7d9adb84fed808ea734525a4e83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "393bbee2c3b94a8e9213c24685762b33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9b800855e7f425a91437e7017052cc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1fa05a10b5c4a84b988f5dba0f82522": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7278c5e2b1544de9b86c848551000d94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6420b6abcc7e43a5811bfd98091c093b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd87423a2bbd40e4a90789b6ed475877": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e213fe90dd0448a3b0b93856df94eca3",
              "IPY_MODEL_b1b5a5e1a91e4de4a0c70d45e4a36517",
              "IPY_MODEL_88ef375594454a68a38671a3a2543f95"
            ],
            "layout": "IPY_MODEL_e9d48a6bb7df4cb98f6e3704516d7f58"
          }
        },
        "e213fe90dd0448a3b0b93856df94eca3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a070996e45524469910e35bb23097364",
            "placeholder": "​",
            "style": "IPY_MODEL_6a8beb7bb0154940bd3e4c19acf2ddf4",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "b1b5a5e1a91e4de4a0c70d45e4a36517": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_403acc930d53415b9ea874989077c4a4",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_75312eec023743d2a0a05e3a42827940",
            "value": 116
          }
        },
        "88ef375594454a68a38671a3a2543f95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b2c408f362540c7b7cfa8d01f1e09a5",
            "placeholder": "​",
            "style": "IPY_MODEL_af2bff1c867d42c3bf792db543eb2073",
            "value": " 116/116 [00:00&lt;00:00, 6.26kB/s]"
          }
        },
        "e9d48a6bb7df4cb98f6e3704516d7f58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a070996e45524469910e35bb23097364": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a8beb7bb0154940bd3e4c19acf2ddf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "403acc930d53415b9ea874989077c4a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75312eec023743d2a0a05e3a42827940": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5b2c408f362540c7b7cfa8d01f1e09a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af2bff1c867d42c3bf792db543eb2073": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f38f671a961d4b3f86405ee1c55bf859": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b58f99cdf7bf445ca60bb825ad09b47f",
              "IPY_MODEL_4a8cc66eacad4effbe74a46d8bf41d32",
              "IPY_MODEL_3cc6c1b702be46b38dc174d8c95b0839"
            ],
            "layout": "IPY_MODEL_3436b7041a934abd9a46294d7ffc5784"
          }
        },
        "b58f99cdf7bf445ca60bb825ad09b47f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_236f09f37c204b66b0e1319c315ce807",
            "placeholder": "​",
            "style": "IPY_MODEL_e4f5e303d1a948bfaa44dc320d3e8655",
            "value": "README.md: "
          }
        },
        "4a8cc66eacad4effbe74a46d8bf41d32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26c2481afe2e434b80b93f559795bdca",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_21577fd5d08b48aaaa2fd2caae90e0c6",
            "value": 1
          }
        },
        "3cc6c1b702be46b38dc174d8c95b0839": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c5f5c62e052458493b32dee017aa711",
            "placeholder": "​",
            "style": "IPY_MODEL_dac750ce30594de9b5537eb2ffde97f1",
            "value": " 11.6k/? [00:00&lt;00:00, 494kB/s]"
          }
        },
        "3436b7041a934abd9a46294d7ffc5784": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "236f09f37c204b66b0e1319c315ce807": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4f5e303d1a948bfaa44dc320d3e8655": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26c2481afe2e434b80b93f559795bdca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "21577fd5d08b48aaaa2fd2caae90e0c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0c5f5c62e052458493b32dee017aa711": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dac750ce30594de9b5537eb2ffde97f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e27e2193ca354c1591b9d09c8211b9ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bdf1493a68804219888f6df488cfc65d",
              "IPY_MODEL_3f28c8ad2e30449b9c7fdff95b104dba",
              "IPY_MODEL_c8a9ea3cee8b47b99385457797b8a53e"
            ],
            "layout": "IPY_MODEL_b6965b7ae85f45dcb8d19895bfdf8ed1"
          }
        },
        "bdf1493a68804219888f6df488cfc65d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6857b85763e44d45a0c168a62322eab1",
            "placeholder": "​",
            "style": "IPY_MODEL_35dfba8a70a94758a039abfc4d33b2e3",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "3f28c8ad2e30449b9c7fdff95b104dba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e890736d66e4322a4fda3c1ed5aad55",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_23f0af3e680c4b59a5192293e69261b5",
            "value": 53
          }
        },
        "c8a9ea3cee8b47b99385457797b8a53e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_022bbbedd26f41f89a3522f2ba199e0d",
            "placeholder": "​",
            "style": "IPY_MODEL_4d35973ab754422b91ff53f5b9e00227",
            "value": " 53.0/53.0 [00:00&lt;00:00, 3.17kB/s]"
          }
        },
        "b6965b7ae85f45dcb8d19895bfdf8ed1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6857b85763e44d45a0c168a62322eab1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35dfba8a70a94758a039abfc4d33b2e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e890736d66e4322a4fda3c1ed5aad55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23f0af3e680c4b59a5192293e69261b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "022bbbedd26f41f89a3522f2ba199e0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d35973ab754422b91ff53f5b9e00227": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d3116d917894b83bd9c59cce3f5cda5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f974700f8bc74543b5603e8c0ac38acc",
              "IPY_MODEL_244922f1250b47e5be98f1c70a546836",
              "IPY_MODEL_f84cdce3f1934fc787486d66180be556"
            ],
            "layout": "IPY_MODEL_498f745219c34870900c7cbc88c051e7"
          }
        },
        "f974700f8bc74543b5603e8c0ac38acc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8dbc0ebd01d473d82d8a5427bfb277e",
            "placeholder": "​",
            "style": "IPY_MODEL_b6bda19a81704b848e511a0b98445966",
            "value": "config.json: 100%"
          }
        },
        "244922f1250b47e5be98f1c70a546836": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98924f38cc8142e0904e1a44e9e10c17",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e428df2930bd4a918250503d37778c7c",
            "value": 571
          }
        },
        "f84cdce3f1934fc787486d66180be556": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98f021043ade45ab9980ddf2fdccf3bd",
            "placeholder": "​",
            "style": "IPY_MODEL_5ff64c98d29b496e92861ded18f94663",
            "value": " 571/571 [00:00&lt;00:00, 39.5kB/s]"
          }
        },
        "498f745219c34870900c7cbc88c051e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8dbc0ebd01d473d82d8a5427bfb277e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6bda19a81704b848e511a0b98445966": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98924f38cc8142e0904e1a44e9e10c17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e428df2930bd4a918250503d37778c7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "98f021043ade45ab9980ddf2fdccf3bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ff64c98d29b496e92861ded18f94663": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee622074d4634bbb926f55e00558d0f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c3e02137dd2448eda4fc23196ec8bbf6",
              "IPY_MODEL_c0d027fe768d4ac3990ed5af08bc0160",
              "IPY_MODEL_fa29168ff173456cb9a21d013fa1b3bf"
            ],
            "layout": "IPY_MODEL_b432a313be15460d8949cc37d3ecac61"
          }
        },
        "c3e02137dd2448eda4fc23196ec8bbf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cab1aef199b4fc6a27b3f7379741513",
            "placeholder": "​",
            "style": "IPY_MODEL_c24168590a3040b095bb5adefaf4d2bf",
            "value": "model.safetensors: 100%"
          }
        },
        "c0d027fe768d4ac3990ed5af08bc0160": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae5ba49973f24810bc548be311242d4e",
            "max": 437971872,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bbb21c4d64e446f19f3f5e6d3a3b8550",
            "value": 437971872
          }
        },
        "fa29168ff173456cb9a21d013fa1b3bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6d39077f9bd47bf876f336522be0369",
            "placeholder": "​",
            "style": "IPY_MODEL_2fe80462f33a4ceda1d7b7cc12afd9fd",
            "value": " 438M/438M [00:03&lt;00:00, 255MB/s]"
          }
        },
        "b432a313be15460d8949cc37d3ecac61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cab1aef199b4fc6a27b3f7379741513": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c24168590a3040b095bb5adefaf4d2bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae5ba49973f24810bc548be311242d4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbb21c4d64e446f19f3f5e6d3a3b8550": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f6d39077f9bd47bf876f336522be0369": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fe80462f33a4ceda1d7b7cc12afd9fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6aa78b3f1bea4b3a925474a731256049": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64e49d8c3a3a4da6b310d62bd8db28c1",
              "IPY_MODEL_6ee7706931dd47faa9d252944becef2a",
              "IPY_MODEL_2b63f667cf4a454daf0389600fc93fc6"
            ],
            "layout": "IPY_MODEL_5fd77545ebd5478c90985bdbe46da01c"
          }
        },
        "64e49d8c3a3a4da6b310d62bd8db28c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe9e47fc6ed041e996f750f6e835f48e",
            "placeholder": "​",
            "style": "IPY_MODEL_af62c272943343c39feb615c989f1e14",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "6ee7706931dd47faa9d252944becef2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9456ab3322f409982377adeb25a9028",
            "max": 363,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a28770cf8a274e8cbe1ec8bfd9af9e84",
            "value": 363
          }
        },
        "2b63f667cf4a454daf0389600fc93fc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1917f7f85ca496d97ab5b4afb815a99",
            "placeholder": "​",
            "style": "IPY_MODEL_25b9b68892744a13820297c41fd6436a",
            "value": " 363/363 [00:00&lt;00:00, 28.8kB/s]"
          }
        },
        "5fd77545ebd5478c90985bdbe46da01c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe9e47fc6ed041e996f750f6e835f48e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af62c272943343c39feb615c989f1e14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9456ab3322f409982377adeb25a9028": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a28770cf8a274e8cbe1ec8bfd9af9e84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d1917f7f85ca496d97ab5b4afb815a99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25b9b68892744a13820297c41fd6436a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92059ca01e474967803ae64a483c5341": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_86b4edf4b8fb4b6690f2718965760ed6",
              "IPY_MODEL_51d9aea70102424eba4c92269192b10d",
              "IPY_MODEL_d804d60457bd40b6b734bb8e35e8af97"
            ],
            "layout": "IPY_MODEL_46b090b2a94a4db6a33fa7e544f62742"
          }
        },
        "86b4edf4b8fb4b6690f2718965760ed6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_817a36118802462baef3ecbdaf6f119e",
            "placeholder": "​",
            "style": "IPY_MODEL_09cd724762414066bf153436ae3bbb4c",
            "value": "vocab.txt: "
          }
        },
        "51d9aea70102424eba4c92269192b10d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed48f9e6b04f4c90afe80530d350ff91",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6152081dfa3f432b95e9ec9051d37020",
            "value": 1
          }
        },
        "d804d60457bd40b6b734bb8e35e8af97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b56bfb0c09634de4ae49fc45b4acb36c",
            "placeholder": "​",
            "style": "IPY_MODEL_ae281965225c4ed5bdf443b696405133",
            "value": " 232k/? [00:00&lt;00:00, 8.83MB/s]"
          }
        },
        "46b090b2a94a4db6a33fa7e544f62742": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "817a36118802462baef3ecbdaf6f119e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09cd724762414066bf153436ae3bbb4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed48f9e6b04f4c90afe80530d350ff91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "6152081dfa3f432b95e9ec9051d37020": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b56bfb0c09634de4ae49fc45b4acb36c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae281965225c4ed5bdf443b696405133": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b658bfc05b22476682cc81e118a4fd06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db69b0c2578c494b91794dd66a6c66b2",
              "IPY_MODEL_1bf1fa3349254493871a689401e0e97f",
              "IPY_MODEL_264c8ebc479a427fa739072c451cd73a"
            ],
            "layout": "IPY_MODEL_519c3923af4f4ec7a0fe8b7fee184311"
          }
        },
        "db69b0c2578c494b91794dd66a6c66b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e35d78417df4dd195091d199ea69060",
            "placeholder": "​",
            "style": "IPY_MODEL_45dc3aae71444943a5f329831b5a8216",
            "value": "tokenizer.json: "
          }
        },
        "1bf1fa3349254493871a689401e0e97f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9de27cf516c4ebc8895fe27fe94720c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_78b217ee23f148cc9b1da661561fea82",
            "value": 1
          }
        },
        "264c8ebc479a427fa739072c451cd73a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d8795befd2349ebafed1e2af9052480",
            "placeholder": "​",
            "style": "IPY_MODEL_196cf0c891ea485db29aafd221c7b76a",
            "value": " 466k/? [00:00&lt;00:00, 11.9MB/s]"
          }
        },
        "519c3923af4f4ec7a0fe8b7fee184311": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e35d78417df4dd195091d199ea69060": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45dc3aae71444943a5f329831b5a8216": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9de27cf516c4ebc8895fe27fe94720c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "78b217ee23f148cc9b1da661561fea82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3d8795befd2349ebafed1e2af9052480": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "196cf0c891ea485db29aafd221c7b76a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2a53ef4495446498099ca959722fa77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d6cf2d44cb24695916e0a310fd01b80",
              "IPY_MODEL_713f525f93bd494f83803a4ad6a40978",
              "IPY_MODEL_cb1c2f328ccb4339983371c4f907ae8a"
            ],
            "layout": "IPY_MODEL_c854e644fab74662b47d56aa44eae4f6"
          }
        },
        "2d6cf2d44cb24695916e0a310fd01b80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_820674ccf03f412f90cd808c349d8113",
            "placeholder": "​",
            "style": "IPY_MODEL_e12059a36ab949b3bb6cf07efe6cd27d",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "713f525f93bd494f83803a4ad6a40978": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_005cb05f389045adaacb6d2ec9f3b362",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2f70258509cd4087bf87418b7b4c5d5a",
            "value": 239
          }
        },
        "cb1c2f328ccb4339983371c4f907ae8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9049d24628734b2daaa2b613272656e3",
            "placeholder": "​",
            "style": "IPY_MODEL_800e52d107ca492c9748c85bad9a1b6c",
            "value": " 239/239 [00:00&lt;00:00, 17.1kB/s]"
          }
        },
        "c854e644fab74662b47d56aa44eae4f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "820674ccf03f412f90cd808c349d8113": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e12059a36ab949b3bb6cf07efe6cd27d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "005cb05f389045adaacb6d2ec9f3b362": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f70258509cd4087bf87418b7b4c5d5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9049d24628734b2daaa2b613272656e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "800e52d107ca492c9748c85bad9a1b6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8af00b33062d4f4189b5bd6cef79a84a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_33fc8cc5a4d048cda28b55760c4c7afc",
              "IPY_MODEL_97f135b3e680442aa0c9f8e754e3162d",
              "IPY_MODEL_7400dffc4b294c1d9b4fa12b7dcfc57e"
            ],
            "layout": "IPY_MODEL_f805ab45967848629752e0c62e41e120"
          }
        },
        "33fc8cc5a4d048cda28b55760c4c7afc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72468ebb38a24a478eccff576725b9cf",
            "placeholder": "​",
            "style": "IPY_MODEL_8ca41f7dcec74ceeba80da41c1595caf",
            "value": "config.json: 100%"
          }
        },
        "97f135b3e680442aa0c9f8e754e3162d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf1869bee565493a9ad118f2b07bf20b",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4fcf9510bc8d482bb09ced54fab8f737",
            "value": 190
          }
        },
        "7400dffc4b294c1d9b4fa12b7dcfc57e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63c300bae8114585ad14b8585b472bca",
            "placeholder": "​",
            "style": "IPY_MODEL_b76cef54bc6e437ba0526a3ef0478c8b",
            "value": " 190/190 [00:00&lt;00:00, 16.7kB/s]"
          }
        },
        "f805ab45967848629752e0c62e41e120": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72468ebb38a24a478eccff576725b9cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ca41f7dcec74ceeba80da41c1595caf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf1869bee565493a9ad118f2b07bf20b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fcf9510bc8d482bb09ced54fab8f737": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "63c300bae8114585ad14b8585b472bca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b76cef54bc6e437ba0526a3ef0478c8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "installing the packages(modules)"
      ],
      "metadata": {
        "id": "B_2A3vviY8aj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e08-hWhFYkvh",
        "outputId": "248f7a00-5906-4dc8-9317-4e15e462bfbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/603.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m593.9/603.7 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m603.7/603.7 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.0/329.0 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m978.2/978.2 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q weaviate-client langchain tiktoken pypdf rapidocr-onnxruntime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can access this weaviate in 2 ways\n",
        "\n",
        "\n",
        "1.you can you it as a in memory database\n",
        "\n",
        "\n",
        "2.and you can use it as a cloud service aswell."
      ],
      "metadata": {
        "id": "XwCCFZ-xanOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we are here using it as a cloud service, we can store the vector embedding in cloud services, here i am  storing over the cloud itself"
      ],
      "metadata": {
        "id": "3t4C25f7bHZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-weaviate\n"
      ],
      "metadata": {
        "id": "K5Awc72tcqwO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "import weaviate\n",
        "from weaviate.auth import AuthApiKey\n",
        "from langchain_weaviate import WeaviateVectorStore\n"
      ],
      "metadata": {
        "id": "Os-4RjCiZkLL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"WEAVIATE_API_KEY\"] = userdata.get(\"WEAVIATE_API_KEY\")\n",
        "os.environ[\"WEAVIATE_URL\"] = userdata.get(\"WEAVIATE_CLUSTER\")\n"
      ],
      "metadata": {
        "id": "dA5HSCAMZGgz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = weaviate.connect_to_wcs(\n",
        "    cluster_url=os.environ[\"WEAVIATE_URL\"],\n",
        "    auth_credentials=AuthApiKey(os.environ[\"WEAVIATE_API_KEY\"])\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeaaUq5sZse7",
        "outputId": "88b17a17-504f-40ec-de72-394a1bf4f2de"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2021824886.py:1: DeprecatedWarning: connect_to_wcs is deprecated as of 4.6.2. \n",
            "This method is deprecated and will be removed in a future release. Use :func:`connect_to_weaviate_cloud` instead.\n",
            "\n",
            "  client = weaviate.connect_to_wcs(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = WeaviateVectorStore(\n",
        "    client=client,\n",
        "    index_name=\"RAG_knowledgebase\",\n",
        "    text_key=\"text\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "3iAkrTnrZu1r"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU  langchain langchain-huggingface sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOlirXpnk5EG",
        "outputId": "a4acecda-dd9d-47b0-d2ac-875daa4fa63b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/106.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.4/106.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#specifying embedding model(huggingface sentence transformer)\n",
        "#from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
        "embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386,
          "referenced_widgets": [
            "5f697262aa90471bb0b4ca72e86d31f0",
            "095ab6795be54ff4a518e7655ef5354c",
            "67f0762286eb46069288e4824b3ba56c",
            "93b542d18ec34aef975a551eb459b357",
            "08c35bb20afa443f8770697b18676849",
            "2dc8c7d9adb84fed808ea734525a4e83",
            "393bbee2c3b94a8e9213c24685762b33",
            "f9b800855e7f425a91437e7017052cc3",
            "f1fa05a10b5c4a84b988f5dba0f82522",
            "7278c5e2b1544de9b86c848551000d94",
            "6420b6abcc7e43a5811bfd98091c093b",
            "cd87423a2bbd40e4a90789b6ed475877",
            "e213fe90dd0448a3b0b93856df94eca3",
            "b1b5a5e1a91e4de4a0c70d45e4a36517",
            "88ef375594454a68a38671a3a2543f95",
            "e9d48a6bb7df4cb98f6e3704516d7f58",
            "a070996e45524469910e35bb23097364",
            "6a8beb7bb0154940bd3e4c19acf2ddf4",
            "403acc930d53415b9ea874989077c4a4",
            "75312eec023743d2a0a05e3a42827940",
            "5b2c408f362540c7b7cfa8d01f1e09a5",
            "af2bff1c867d42c3bf792db543eb2073",
            "f38f671a961d4b3f86405ee1c55bf859",
            "b58f99cdf7bf445ca60bb825ad09b47f",
            "4a8cc66eacad4effbe74a46d8bf41d32",
            "3cc6c1b702be46b38dc174d8c95b0839",
            "3436b7041a934abd9a46294d7ffc5784",
            "236f09f37c204b66b0e1319c315ce807",
            "e4f5e303d1a948bfaa44dc320d3e8655",
            "26c2481afe2e434b80b93f559795bdca",
            "21577fd5d08b48aaaa2fd2caae90e0c6",
            "0c5f5c62e052458493b32dee017aa711",
            "dac750ce30594de9b5537eb2ffde97f1",
            "e27e2193ca354c1591b9d09c8211b9ba",
            "bdf1493a68804219888f6df488cfc65d",
            "3f28c8ad2e30449b9c7fdff95b104dba",
            "c8a9ea3cee8b47b99385457797b8a53e",
            "b6965b7ae85f45dcb8d19895bfdf8ed1",
            "6857b85763e44d45a0c168a62322eab1",
            "35dfba8a70a94758a039abfc4d33b2e3",
            "8e890736d66e4322a4fda3c1ed5aad55",
            "23f0af3e680c4b59a5192293e69261b5",
            "022bbbedd26f41f89a3522f2ba199e0d",
            "4d35973ab754422b91ff53f5b9e00227",
            "1d3116d917894b83bd9c59cce3f5cda5",
            "f974700f8bc74543b5603e8c0ac38acc",
            "244922f1250b47e5be98f1c70a546836",
            "f84cdce3f1934fc787486d66180be556",
            "498f745219c34870900c7cbc88c051e7",
            "a8dbc0ebd01d473d82d8a5427bfb277e",
            "b6bda19a81704b848e511a0b98445966",
            "98924f38cc8142e0904e1a44e9e10c17",
            "e428df2930bd4a918250503d37778c7c",
            "98f021043ade45ab9980ddf2fdccf3bd",
            "5ff64c98d29b496e92861ded18f94663",
            "ee622074d4634bbb926f55e00558d0f7",
            "c3e02137dd2448eda4fc23196ec8bbf6",
            "c0d027fe768d4ac3990ed5af08bc0160",
            "fa29168ff173456cb9a21d013fa1b3bf",
            "b432a313be15460d8949cc37d3ecac61",
            "0cab1aef199b4fc6a27b3f7379741513",
            "c24168590a3040b095bb5adefaf4d2bf",
            "ae5ba49973f24810bc548be311242d4e",
            "bbb21c4d64e446f19f3f5e6d3a3b8550",
            "f6d39077f9bd47bf876f336522be0369",
            "2fe80462f33a4ceda1d7b7cc12afd9fd",
            "6aa78b3f1bea4b3a925474a731256049",
            "64e49d8c3a3a4da6b310d62bd8db28c1",
            "6ee7706931dd47faa9d252944becef2a",
            "2b63f667cf4a454daf0389600fc93fc6",
            "5fd77545ebd5478c90985bdbe46da01c",
            "fe9e47fc6ed041e996f750f6e835f48e",
            "af62c272943343c39feb615c989f1e14",
            "d9456ab3322f409982377adeb25a9028",
            "a28770cf8a274e8cbe1ec8bfd9af9e84",
            "d1917f7f85ca496d97ab5b4afb815a99",
            "25b9b68892744a13820297c41fd6436a",
            "92059ca01e474967803ae64a483c5341",
            "86b4edf4b8fb4b6690f2718965760ed6",
            "51d9aea70102424eba4c92269192b10d",
            "d804d60457bd40b6b734bb8e35e8af97",
            "46b090b2a94a4db6a33fa7e544f62742",
            "817a36118802462baef3ecbdaf6f119e",
            "09cd724762414066bf153436ae3bbb4c",
            "ed48f9e6b04f4c90afe80530d350ff91",
            "6152081dfa3f432b95e9ec9051d37020",
            "b56bfb0c09634de4ae49fc45b4acb36c",
            "ae281965225c4ed5bdf443b696405133",
            "b658bfc05b22476682cc81e118a4fd06",
            "db69b0c2578c494b91794dd66a6c66b2",
            "1bf1fa3349254493871a689401e0e97f",
            "264c8ebc479a427fa739072c451cd73a",
            "519c3923af4f4ec7a0fe8b7fee184311",
            "9e35d78417df4dd195091d199ea69060",
            "45dc3aae71444943a5f329831b5a8216",
            "e9de27cf516c4ebc8895fe27fe94720c",
            "78b217ee23f148cc9b1da661561fea82",
            "3d8795befd2349ebafed1e2af9052480",
            "196cf0c891ea485db29aafd221c7b76a",
            "f2a53ef4495446498099ca959722fa77",
            "2d6cf2d44cb24695916e0a310fd01b80",
            "713f525f93bd494f83803a4ad6a40978",
            "cb1c2f328ccb4339983371c4f907ae8a",
            "c854e644fab74662b47d56aa44eae4f6",
            "820674ccf03f412f90cd808c349d8113",
            "e12059a36ab949b3bb6cf07efe6cd27d",
            "005cb05f389045adaacb6d2ec9f3b362",
            "2f70258509cd4087bf87418b7b4c5d5a",
            "9049d24628734b2daaa2b613272656e3",
            "800e52d107ca492c9748c85bad9a1b6c",
            "8af00b33062d4f4189b5bd6cef79a84a",
            "33fc8cc5a4d048cda28b55760c4c7afc",
            "97f135b3e680442aa0c9f8e754e3162d",
            "7400dffc4b294c1d9b4fa12b7dcfc57e",
            "f805ab45967848629752e0c62e41e120",
            "72468ebb38a24a478eccff576725b9cf",
            "8ca41f7dcec74ceeba80da41c1595caf",
            "bf1869bee565493a9ad118f2b07bf20b",
            "4fcf9510bc8d482bb09ced54fab8f737",
            "63c300bae8114585ad14b8585b472bca",
            "b76cef54bc6e437ba0526a3ef0478c8b"
          ]
        },
        "id": "uPBqaawJcfT0",
        "outputId": "7e5fa90f-2c12-40fb-9426-80e4ef95e407"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f697262aa90471bb0b4ca72e86d31f0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd87423a2bbd40e4a90789b6ed475877"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f38f671a961d4b3f86405ee1c55bf859"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e27e2193ca354c1591b9d09c8211b9ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d3116d917894b83bd9c59cce3f5cda5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee622074d4634bbb926f55e00558d0f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6aa78b3f1bea4b3a925474a731256049"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92059ca01e474967803ae64a483c5341"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b658bfc05b22476682cc81e118a4fd06"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2a53ef4495446498099ca959722fa77"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8af00b33062d4f4189b5bd6cef79a84a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-community pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMFm0e5rhI0A",
        "outputId": "0ac6ff95-655f-4c3a-ea70-3c160ae86a81"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m1.8/2.5 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m490.2/490.2 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "loader=PyPDFLoader(\"/content/Multimodal_RAG.pdf\",extract_images=True)\n",
        "pages=loader.load()"
      ],
      "metadata": {
        "id": "3u-mqrByg_7y"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pages)"
      ],
      "metadata": {
        "id": "rqzX9KBElkX2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26bae708-5923-406f-8ec0-4e4cf697bd0b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Mohammad Mahdi Abootorabi; Amirhosein Zobeiri; Mahdi Dehghani; Mohammadali Mohammadkhani; Bardia Mohammadi; Omid Ghahroodi; Mahdieh Soleymani Baghshah; Ehsaneddin Asgari', 'doi': 'https://doi.org/10.48550/arXiv.2502.08826', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2502.08826v3', 'source': '/content/Multimodal_RAG.pdf', 'total_pages': 34, 'page': 0, 'page_label': '1'}, page_content='arXiv:2502.08826v3  [cs.CL]  2 Jun 2025\\nAsk in Any Modality\\nA Comprehensive Survey on Multimodal Retrieval-Augmented Generation\\nMohammad Mahdi Abootorabi†, Amirhosein Zobeiri⋄, Mahdi Dehghani¶, Mohammadali Mohammadkhani§,\\nBardia Mohammadi§, Omid Ghahroodi†, Mahdieh Soleymani Baghshah§, *, Ehsaneddin Asgari†, *\\n§Computer Engineering Department, Sharif University of Technology, Tehran, Iran,\\n⋄College of Interdisciplinary Science and Technology, University of Tehran, Tehran, Iran,\\n¶Computer Engineering Department, K.N. Toosi University of Technology, Tehran, Iran,\\n†Qatar Computing Research Institute, Doha, Qatar\\nCorrespondence: soleymani@sharif.edu and easgari@hbku.edu.qa\\nAbstract\\nLarge Language Models (LLMs) suffer from\\nhallucinations and outdated knowledge due to\\ntheir reliance on static training data. Retrieval-\\nAugmented Generation (RAG) mitigates these\\nissues by integrating external dynamic infor-\\nmation for improved factual grounding. With\\nadvances in multimodal learning, Multimodal\\nRAG extends this approach by incorporating\\nmultiple modalities such as text, images, au-\\ndio, and video to enhance the generated outputs.\\nHowever, cross-modal alignment and reasoning\\nintroduce unique challenges beyond those in\\nunimodal RAG. This survey offers a structured\\nand comprehensive analysis of Multimodal RAG\\nsystems, covering datasets, benchmarks, metrics,\\nevaluation, methodologies, and innovations in\\nretrieval, fusion, augmentation, and generation.\\nWe review training strategies, robustness en-\\nhancements, loss functions, and agent-based\\napproaches, while also exploring the diverse\\nMultimodal RAG scenarios. In addition, we\\noutline open challenges and future directions to\\nguide research in this evolving field. This survey\\nlays the foundation for developing more capable\\nand reliable AI systems that effectively leverage\\nmultimodal dynamic external knowledge bases.\\nAll resources are publicly available 1.\\n1 Introduction & Background\\nRecent advancements in transformer architectures\\n(Vaswani et al., 2017), coupled with increased com-\\nputational resources and the availability of large-\\nscale training datasets (Naveed et al., 2024), have\\nsignificantly accelerated progress in the development\\nof language models. The emergence of foundational\\nLarge Language Models (LLMs) (Ouyang et al.,\\n2022; Grattafiori et al., 2024; Touvron et al., 2023;\\nQwen et al., 2025; Anil et al., 2023), has revolution-\\nized natural language processing (NLP), excelling in\\n1https://github.com/llm-lab-org/\\nMultimodal-RAG-Survey\\n*These authors contributed equally.\\ntasks such as instruction following (Qin et al., 2024),\\nreasoning (Wei et al., 2024b), in-context learning\\n(Brown et al., 2020), and multilingual translation\\n(Zhu et al., 2024a). Despite these achievements,\\nLLMs face challenges such as hallucinations, out-\\ndated knowledge, and a lack of verifiable reason-\\ning (Huang et al., 2024; Xu et al., 2024b). Their\\nreliance on parametric memory limits access to up-\\nto-date information, reducing their effectiveness in\\nknowledge-intensive tasks.\\nRetrieval-Augmented Generation (RAG) RAG\\n(Lewis et al., 2020) addresses these limitations by\\nenabling LLMs to retrieve and incorporate exter-\\nnal knowledge, improving factual accuracy and\\nreducing hallucinations (Shuster et al., 2021; Ding\\net al., 2024a). By dynamically accessing exter-\\nnal knowledge sources, RAG enhances knowledge-\\nintensive tasks while grounding responses in verifi-\\nable sources (Gao et al., 2023). In practice, RAG\\nsystems follow a retriever-generator pipeline: the\\nretriever uses embedding models (Chen et al., 2024a;\\nRau et al., 2024) to identify relevant passages from\\nexternal knowledge bases and may apply re-ranking\\ntechniques to improve precision (Dong et al., 2024a).\\nThe retrieved passages are then provided to the gen-\\nerator, which leverages this contextual information\\nto produce more informed and coherent responses.\\nRecent advancements in RAG frameworks, such as\\nplanning-guided retrieval (Lee et al., 2024), agentic\\nRAG (An et al., 2024), and feedback-driven iterative\\nrefinement (Liu et al., 2024c; Asai et al., 2023), have\\nfurther improved both the retrieval and generation\\ncomponents of these systems.\\nMultimodal Learning In parallel with advances\\nin language modeling, multimodal learning has\\nemerged as a transformative area in artificial intelli-\\ngence, enabling systems to integrate and reason over\\nheterogeneous data sources for more comprehensive\\nrepresentations. A pivotal breakthrough was the\\nintroduction of CLIP model (Radford et al., 2021),\\n1'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Mohammad Mahdi Abootorabi; Amirhosein Zobeiri; Mahdi Dehghani; Mohammadali Mohammadkhani; Bardia Mohammadi; Omid Ghahroodi; Mahdieh Soleymani Baghshah; Ehsaneddin Asgari', 'doi': 'https://doi.org/10.48550/arXiv.2502.08826', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2502.08826v3', 'source': '/content/Multimodal_RAG.pdf', 'total_pages': 34, 'page': 1, 'page_label': '2'}, page_content='Figure 1: Overview of the multimodal RAG pipeline, illustrating key techniques and recent advancements.\\nwhich aligned visual and textual modalities through\\ncontrastive learning and inspired a wave of subse-\\nquent models (Alayrac et al., 2024; Wang et al.,\\n2023a; Pramanick et al., 2023). These developments\\nhave catalyzed progress across diverse domains,\\nincluding sentiment analysis (Das and Singh, 2023)\\nand biomedical applications (Hemker et al., 2024),\\nhighlighting the effectiveness of multimodal ap-\\nproaches. By facilitating the joint processing of\\ntext, images, audio, and video, multimodal learning\\nis increasingly recognized as a critical enabler of\\nprogress toward artificial general intelligence (AGI)\\n(Song et al., 2025).\\nMultimodal RAG The extension of large language\\nmodels to multimodal LLMs (MLLMs) has signifi-\\ncantly broadened their capabilities, enabling reason-\\ning and generation across multiple data modalities\\n(Liu et al., 2023a; Team et al., 2024; Li et al., 2023b).\\nNotably, GPT-4 (OpenAI et al., 2024) demonstrates\\nhuman-level performance by jointly processing text\\nand images, marking a milestone in multimodal un-\\nderstanding. Building on this progress, multimodal\\nRAG incorporates diverse sources, such as images,\\naudio, and structured data, to enrich contextual\\ngrounding and enhance generation quality (Hu et al.,\\n2023; Chen et al., 2022a). This approach improves\\nreasoning by leveraging cross-modal cues, but also\\nintroduces challenges, including modality selection,\\neffective fusion, and managing cross-modal rele-\\nvance (Zhao et al., 2023a). Figure 1 illustrates the\\ngeneral pipeline of these systems.\\nMultimodal RAG Formulation We present a math-\\nematical formulation of multimodal RAG. These sys-\\ntems aim to generate a multimodal response r given\\na multimodal query q. Let D = {d1, d2, . . . , dn}\\ndenote a multimodal corpus. For clarity, we assume\\neach document di ∈ D is associated with a single\\nmodality Mdi. In practice, however, documents\\noften span multiple modalities—for example, a sci-\\nentific article containing both text and images. Such\\ncases are typically addressed by either decomposing\\nthe document into modality-specific sub-documents\\nor employing universal encoders capable of jointly\\nprocessing multiple modalities.\\nEach document di is encoded using its corre-\\nsponding modality-specific encoder, yielding zi =\\nEncMdi\\n(di). The collection of all encoded represen-\\ntations is denoted as Z = {z1, z2, . . . , zn}. These\\nmodality-specific encoders project diverse input\\nmodalities into a shared semantic space, enabling\\ncross-modal alignment.\\nA retrieval model R computes a relevance score\\ns(eq, zi) between the encoded query representation\\neq (obtained by encoding q using the appropriate\\nencoders) and each document representation zi. The\\nretrieval-augmented multimodal context X is con-\\nstructed by selecting documents whose relevance\\nscores exceed a modality-specific threshold:\\nX = {di ∈ D | s(eq, zi) ≥ τMdi\\n},\\nwhere τMdi\\nis the relevance threshold for the modal-\\nity Mdi, and s is the scoring function that measures\\nsemantic relevance. Finally, the generative model\\nG produces the response conditioned on the origi-\\nnal query q and the retrieved context X, formally\\ndefined as r = G(q, X).\\nRelated Works Multimodal RAG is a rapidly\\nemerging field, yet a comprehensive survey dedi-\\ncated to its recent advancements remains lacking.\\nWhile over ten surveys discuss RAG-related topics\\nsuch as Agentic RAG (Singh et al., 2025), none\\nspecifically focus on the multimodal setting. To\\nour knowledge, the only relevant work (Zhao et al.,\\n2023a) categorizes multimodal RAGs by applica-\\ntion and modality. In contrast, our survey adopts\\na more innovation-driven perspective, offering a\\ndetailed taxonomy and addressing recent trends and\\nopen challenges. We review over 100 recent papers,\\nprimarily from the ACL Anthology, reflecting the\\n2'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Mohammad Mahdi Abootorabi; Amirhosein Zobeiri; Mahdi Dehghani; Mohammadali Mohammadkhani; Bardia Mohammadi; Omid Ghahroodi; Mahdieh Soleymani Baghshah; Ehsaneddin Asgari', 'doi': 'https://doi.org/10.48550/arXiv.2502.08826', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2502.08826v3', 'source': '/content/Multimodal_RAG.pdf', 'total_pages': 34, 'page': 2, 'page_label': '3'}, page_content='growing interest and progress in this domain.\\nContributions In this work, (i) we present a com-\\nprehensive review of multimodal RAG, covering\\ntask formulation, datasets, benchmarks, applica-\\ntions, and key innovations across retrieval, fusion,\\naugmentation, generation, training strategies, loss\\nfunctions, and agent frameworks. (ii) We propose\\na structured taxonomy (Figure 2) that categorizes\\nstate-of-the-art models by their core contributions,\\nhighlighting methodological advances and emerging\\ntrends. (iii) We provide open-access resources, in-\\ncluding datasets, benchmarks, and implementation\\ndetails, to facilitate future research. (iv) Finally, we\\nidentify research gaps and offer insights to guide\\nfuture directions in this rapidly evolving field.\\n2 Datasets, Evaluation, and Applications\\nWe review diverse datasets and benchmarks support-\\ning tasks such as multimodal summarization, visual\\nQA, video understanding, and more. For full details,\\nsee Appendix (§B) and Tables 1 and 2. Multimodal\\nRAG has been applied across various domains, in-\\ncluding healthcare, software engineering, fashion,\\nentertainment, and emerging fields. An overview of\\ntasks and applications are detailed in Appendix (§E)\\nand Figure 3. Evaluating these systems requires\\nmultiple metrics, covering retrieval performance,\\ngeneration quality, and modality alignment. The\\ncomplete evaluation methods, metrics, and their\\ndefinitions and formulations are in Appendix (§C).\\n3 Key Innovations and Methodologies\\n3.1 Retrieval Strategy\\nEfficient Search and Similarity Retrieval Mod-\\nern multimodal RAG systems encode diverse input\\nmodalities into a unified embedding space to en-\\nable direct cross-modal retrieval. Early CLIP-based\\n(Radford et al., 2021) methods often struggled to\\nbalance retrieval precision and computational cost.\\nBLIP-inspired (Li et al., 2022) approaches addressed\\nsome of these trade-offs by integrating cross-modal\\nattention during training, yielding richer alignments\\nbetween visual and textual features. To reconcile\\nhigh accuracy with efficiency, contrastive retrieval\\nframeworks such as MARVEL (Zhou et al., 2024c)\\nand Uni-IR (Wei et al., 2024a) improved inter-modal\\ndiscrimination through hard-negative mining and bal-\\nanced sampling strategies (Zhang et al., 2024i; Lan\\net al., 2025). Despite these representational gains,\\ndirect search over millions of embeddings demands\\nfast similarity computation. Maximum inner product\\nsearch (MIPS) variants offer sublinear lookup by\\napproximating top-k inner products (Tiwari et al.,\\n2024; Wang et al., 2024c; Zhao et al., 2023b). How-\\never, coarse quantization can degrade recall. To\\nmitigate this, adaptive quantization methods (Zhang\\net al., 2023a; Li et al., 2024a) dynamically allocate\\nbits where the embedding distribution is dense, re-\\nsulting in recall improvements over static schemes.\\nHybrid sparse–dense retrieval (Nguyen et al., 2024;\\nZhang et al., 2024a) further complements dense\\nembeddings with sparse lexical signals. Systems\\nsuch as MuRAG (Chen et al., 2022a) and RA-CM3\\n(Yasunaga et al., 2023) employ approximate MIPS\\nfor efficient top-k candidate retrieval from large\\ncollections of image–text embeddings. Large-scale\\nimplementations leverage distributed MIPS tech-\\nniques, such as TPU-KNN (Chern et al., 2022),\\nfor high-speed retrieval. Other efficient similar-\\nity computation methods include ScaNN (Scalable\\nNearest Neighbors) (Guo et al., 2020), MAXSIM\\nscore (Chan and Ng, 2008; Cho et al., 2024), and\\napproximate KNN methods (Caffagni et al., 2024).\\nEmerging approaches explore learned index struc-\\ntures (Zhai et al., 2023; Basnet et al., 2024), which\\nembed the search tree itself in neural parameters,\\naiming to adapt retrieval paths to the data distribu-\\ntion and reduce both latency and storage overhead.\\nModality-Based Retrieval Modality-aware re-\\ntrieval techniques optimize efficiency by leveraging\\nthe unique characteristics of each modality. (i)\\nText-centric retrieval remains foundational in mul-\\ntimodal RAG systems, with both traditional meth-\\nods like BM25 (Robertson and Zaragoza, 2009)\\nand dense retrievers such as MiniLM (Wang et al.,\\n2020a) and BGE-M3 (Chen et al., 2024b) domi-\\nnating text-based evidence retrieval (Chen et al.,\\n2022b; Suri et al., 2024; Nan et al., 2024b). Novel\\napproaches also address the need for fine-grained\\nsemantic matching and domain specificity: For in-\\nstance, ColBERT (Khattab and Zaharia, 2020) and\\nPreFLMR (Lin et al., 2024b) employ token-level\\ninteraction mechanisms that preserve nuanced tex-\\ntual details to improve precision for multimodal\\nqueries, while RAFT (Zhang et al., 2024h) and\\nCRAG (Yan et al., 2024) enhance retrieval by ensur-\\ning accurate citation of text spans.(ii) Vision-centric\\nretrieval leverages image representations for knowl-\\nedge extraction (Kumar and Marttinen, 2024; Yuan\\net al., 2023). Systems such as EchoSight (Yan and\\nXie, 2024) and ImgRet (Shohan et al., 2024) re-\\ntrieve visually similar content by using reference\\nimages as queries. In addition, composed image\\n3'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Mohammad Mahdi Abootorabi; Amirhosein Zobeiri; Mahdi Dehghani; Mohammadali Mohammadkhani; Bardia Mohammadi; Omid Ghahroodi; Mahdieh Soleymani Baghshah; Ehsaneddin Asgari', 'doi': 'https://doi.org/10.48550/arXiv.2502.08826', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2502.08826v3', 'source': '/content/Multimodal_RAG.pdf', 'total_pages': 34, 'page': 3, 'page_label': '4'}, page_content='Multimodal RAG\\nRetrievalStrategies(§3.1)\\nEfficient-Search andSimilarity Retrieval(§3.1)\\nMaximum InnerProduct Search(MIPS)\\nTPU-KNN (Chern et al., 2022), ScaNN (Guo et al., 2020), MAXSIM score (Chan and Ng, 2008), ADQ (Li et al., 2024a),Zhang et al. (2023a), BanditMIPS (Tiwari et al., 2024), MUST (Wang et al., 2024c), FARGO (Zhao et al., 2023b),MuRAG (Chen et al., 2022a), RA-CM3 (Yasunaga et al., 2023), Nguyen et al. (2024), Graph-based ANNs (Zhang et al., 2024a),Zhai et al. (2023), Deeperimpact (Basnet et al., 2024), RetrievalAttention (Liu et al., 2024a), FactMM-RAG (Sun et al., 2024b)\\nMultimodalEncoders\\nCLIP (Radford et al., 2021), BLIP (Li et al., 2022), MARVEL (Zhou et al., 2024c), ALIGN (Jia et al., 2021), FLA V A (Singhet al., 2022), UniVL-DR (Liu et al., 2023b), UniIR (Wei et al., 2024a), GME (Zhang et al., 2024i), VISTA (Zhou et al., 2024b),ColPali (Faysse et al., 2024), InternVideo (Wang et al., 2022), Ovis (Lu et al., 2024), LLaVE (Lan et al., 2025) Mi-RAG (Adjaliet al., 2024)\\nModality-CentricRetrieval (§3.1)\\nText-CentricContriever (Izacard et al., 2022), GTE (Li et al., 2023c), Re-Imagen (Chen et al., 2022b), BM25 (Robertson and Zaragoza,2009), MiniLM (Wang et al., 2020a), BGE-M3 (Chen et al., 2024b), CapRet (Shohan et al., 2024), OMG-QA (Nan et al.,2024b), ColBERT (Khattab and Zaharia, 2020), PreFLMR (Lin et al., 2024b), RAFT (Zhang et al., 2024h), CRAG (Yan et al.,2024), M2-RAG (Ma et al., 2024d)\\nVision-CentricVQA4CIR (Feng et al., 2023), Unifashion (Zhao et al., 2024), Jang et al. (Jang et al., 2024), Pic2word (Saito et al., 2023),eClip (Kumar and Marttinen, 2024), RAMM (Yuan et al., 2023), Joshi et al. (Joshi et al., 2024), VISA (Ma et al., 2024b),ImgRet (Shohan et al., 2024), EchoSight (Yan and Xie, 2024), Xue et al. (Xue et al., 2024b)\\nVideo-CentriciRAG (Arefeen et al., 2024), VideoRAG (Ren et al., 2025), VideoRAG (Jeong et al., 2025), T-Mass (Wang et al., 2024b),MV-Adapter (Jin et al., 2024), OmAgent (Zhang et al., 2024e), CM2 (Kim et al., 2024), Video-RAG (Luo et al., 2024b),CTCH (Shen et al., 2024), RTime (Du et al., 2024), VideoMAE (Tong et al., 2022), DrVideo (Ma et al., 2024e)\\nAudio-CentricCA-CLAP (Xue et al., 2024a), Recap (Ghosh et al., 2024), SpeechRAG (Min et al., 2025), WavRAG (Chen et al., 2025b),SEAL (Sun et al., 2025), Audiobox TTA-RAG (Yang et al., 2024a), DRCap (Li et al., 2025c), P2PCAP (Changin et al., 2024),LA-RAG (Li et al., 2024b), Xiao et al. (2025)\\nDocumentRetrievalColPali (Faysse et al., 2024), ColQwen2 (Wang et al., 2024d), M3DocVQA (Cho et al., 2024), ViTLP (Mao et al., 2024),DocLLM (Wang et al., 2024a), CREAM (Zhang et al., 2024b), mPLUG-DocOwl 1.5 (Hu et al., 2024a), mPLUG-DocOwl 2 (Huet al., 2024b), VisDom (Suri et al., 2024), DSE (Ma et al., 2024a) , SV-RAG (Chen et al., 2025a)\\nRe-rankingStrategies (§3.1)\\nOptimizedExampleSelection\\nMSIER (Luo et al., 2024a), Hybrid RAG (Su et al., 2024a), RULE (Xia et al., 2024b), RAMM (Yuan et al., 2023),M2RAAP (Dong et al., 2024b)\\nRelevance ScoreEvaluation\\nRAG-Check (Mortaheb et al., 2025a,b), UniRaG (Zhi Lim et al., 2024), MR2AG (Zhang et al., 2024g), LDRE (Yang et al.,2024b), BM25 (Robertson and Zaragoza, 2009), RAGTrans (Cheng et al., 2024), OMG-QA (Nan et al., 2024b), EchoSight (Yanand Xie, 2024), EgoInstructor (Xu et al., 2024a), VR-RAG (Khan et al., 2025)\\nFilteringMechanismsMAIN-RAG (Chang et al., 2024), MM-Embed (Lin et al., 2024a), GME (Zhang et al., 2024i), MuRAR (Zhu et al., 2025)RAFT (Zhang et al., 2024h)\\nFusionMechanisms(§3.2)\\nScore Fusionand Alignment (§3.2)\\nM3 (Cai et al., 2025) Zhi Lim et al. (2024), Sharifymoghaddam et al. (2024), REVEAL (Hu et al., 2023), RAG-Driver (Yuan et al., 2024),C3Net (Zhang et al., 2024c), LLM-RA (Jian et al., 2024), Riedler and Langer (2024), VISA (Ma et al., 2024b), MA-LMM (He et al., 2024), Xueet al. (2024b), RA-BLIP (Ding et al., 2024b), Re-IMAGEN (Chen et al., 2022b), MegaPairs (Zhou et al., 2024a), Wiki-LLaV A (Caffagni et al., 2024),VISRAG (Yu et al., 2024)\\nAttention-BasedMechanisms (§3.2)RAMM (Yuan et al., 2023), EMERGE (Zhu et al., 2024b), MORE (Cui et al., 2024), RAGTrans (Cheng et al., 2024), AlzheimerRAG (Lahiri and Hu,2024), MV-Adapter (Jin et al., 2024), Xu et al. (2024a), Kim et al. (2024), M2-RAAP (Dong et al., 2024b), Mu-RAG (Chen et al., 2022a), Ou etal. (Ou et al., 2025), CADMR (Khalafaoui et al., 2024)\\nUnified Frameworksand Projections (§3.2)Hybrid-RAG (Su et al., 2024a), Dense2Sparse (Nguyen et al., 2024), IRAMIG (Liu et al., 2024b), M3DocRAG (Cho et al., 2024), DQU-CIR (Wenet al., 2024), PDF-MVQA (Ding et al., 2024d), SAM-RAG (Zhai, 2024), UFineBench (Zuo et al., 2024), Li et al. (2022)\\nAugmentationTechniques(§3.3)\\nContext-Enrichment (§3.3)EMERGE (Zhu et al., 2024b), MiRAG (Adjali et al., 2024), Wiki-LLaV A (Caffagni et al., 2024), Video-RAG (Luo et al., 2024b), Img2Loc (Zhouet al., 2024e), Xue et al. (2024b)\\nAdaptive and IterativeRetrieval (§3.3)SKURG (Yang et al., 2023), IRAMIG (Liu et al., 2024b), OMG-QA (Nan et al., 2024b), SAM-RAG (Zhai, 2024), MMed-RAG (Xia et al., 2024a),OmniSearch (Li et al., 2024d), mR2AG (Zhang et al., 2024f), RAGAR (Khaliq et al., 2024), UniversalRAG (Yeo et al., 2025), OMGM (Yang et al.,2025)\\nGenerationTechniques(§3.4)\\nIn-Context Learning(§3.4) RMR (Tan et al., 2024), Sharifymoghaddam et al. (2024), RA-CM3 (Yasunaga et al., 2023), RAG-Driver (Yuan et al., 2024), MSIER (Luo et al.,2024a), Raven (Rao et al., 2024)\\nReasoning (§3.4)RAGAR (Khaliq et al., 2024), VisDoM (Suri et al., 2024), SAM-RAG (Zhai, 2024), LDRE (Yang et al., 2024b)\\nInstruction Tuning(§3.4) RA-BLIP (Ding et al., 2024b), RAGPT (Lang et al., 2025), mR2AG (Zhang et al., 2024f), RagVL (Chen et al., 2024e), Jang et al. (2024), MMed-RAG (Xia et al., 2024a), MegaPairs (Zhou et al., 2024a), Surf (Sun et al., 2024a), Rule (Xia et al., 2024b)\\nSource Attribution(§3.4) MuRAR (Zhu et al., 2025), VISA (Ma et al., 2024b), OMG-QA (Nan et al., 2024b)\\nAgentic Generation andInteraction (§3.4)AppAgent v2 (Li et al., 2024c), USER-LLM R1 (Rahimi et al., 2025), MMAD (Jiang et al., 2025), Yi et al. (2025), CollEX (Schneider et al., 2025),HM-RAG (Liu et al., 2025a), CogPlanner (Yu et al., 2025)\\nTrainingStrategies(§3.5)\\nAlignment (§3.5)VISRAG (Yu et al., 2024), MegaPairs (Zhou et al., 2024a), SAM-RAG (Zhai, 2024), EchoSight (Yan and Xie, 2024), HACL (Jiang et al., 2024),Zhi Lim et al. (2024), Kumar and Marttinen (2024), Dense2Sparse (Nguyen et al., 2024)\\nRobustness (§D.1)Buettner and Kovashka (2024), MORE (Cui et al., 2024), AlzheimerRAG (Lahiri and Hu, 2024), RAGTrans (Cheng et al., 2024), RA-BLIP (Dinget al., 2024b), RagVL (Chen et al., 2024e), RA-CM3 (Yasunaga et al., 2023)\\nFigure 2: Taxonomy of recent advances in Multimodal RAG. Refer to Appendix (§A) for further details.\\nretrieval methods (Feng et al., 2023; Zhao et al.,\\n2024; Jang et al., 2024; Saito et al., 2023) inte-\\ngrate multiple image features into unified query\\nrepresentations, enabling zero-shot image retrieval.\\n(iii) Video-centric retrieval extends vision-based\\ntechniques by incorporating temporal dynamics and\\nlarge video-language models. For instance, iRAG\\n(Arefeen et al., 2024) enables incremental retrieval\\nfor sequential video understanding, addressing the\\nneed for temporal coherence, while T-Mass (Wang\\net al., 2024b) uses stochastic text embeddings to im-\\nprove robustness in text-video alignment. Tackling\\nlong-context processing, Video-RAG (Luo et al.,\\n2024b) avoids reliance on proprietary models by\\nusing auxiliary OCR/ASR texts, whereas VideoRAG\\n(Ren et al., 2025) employs dual-channel architec-\\ntures and graph-based knowledge grounding for\\nextreme-length videos. To capture temporal reason-\\ning, CTCH (Shen et al., 2024) applies contrastive\\ntransformer hashing for long-term dependencies,\\nwhich RTime (Du et al., 2024) further refines by\\nintroducing reversed-video hard negatives for more\\nrobust causality benchmarking. Finally, OmAgent\\n(Zhang et al., 2024e) addresses the challenge of\\ncomplex video understanding with a divide-and-\\nconquer framework, while DRVideo (Ma et al.,\\n2024e) takes a complementary document-centric\\napproach to enhance narrative preservation. (iv)\\nAudio-centric retrieval aims to bypass traditional\\nASR pipelines while improving contextual align-\\nment and real-time processing (Xue et al., 2024a;\\nGhosh et al., 2024; Min et al., 2025). Pioneering\\nframeworks like WavRAG (Chen et al., 2025b) and\\nSEAL (Sun et al., 2025) introduce unified embed-\\n4'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Mohammad Mahdi Abootorabi; Amirhosein Zobeiri; Mahdi Dehghani; Mohammadali Mohammadkhani; Bardia Mohammadi; Omid Ghahroodi; Mahdieh Soleymani Baghshah; Ehsaneddin Asgari', 'doi': 'https://doi.org/10.48550/arXiv.2502.08826', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2502.08826v3', 'source': '/content/Multimodal_RAG.pdf', 'total_pages': 34, 'page': 4, 'page_label': '5'}, page_content='ding architectures, directly mapping raw audio into\\na shared latent space to enable retrieval from hy-\\nbrid knowledge bases. Audiobox TTA-RAG (Yang\\net al., 2024a) conditions text-to-audio synthesis\\non retrieved acoustic samples, thereby enhancing\\nzero-shot performance by enriching prompts with un-\\nlabeled audio context. For audio captioning, DRCap\\n(Li et al., 2025c) bridges the audio-text latent space\\nof CLAP (Wu et al., 2023) via text-only training for\\ndomain-adaptable descriptions without paired data.\\nIn parallel, P2PCAP (Changin et al., 2024) improves\\nretrieval precision by regenerating captions as dy-\\nnamic queries. Further innovations address error\\ncorrection and efficiency. LA-RAG (Li et al., 2024b)\\nutilizes fine-grained speech-to-speech retrieval and\\nforced alignment to enhance ASR accuracy through\\nLLM in-context learning. Meanwhile, hybrid sys-\\ntems, such as Xiao et al. (2025), integrate LLMs to\\ncorrect errors in noisy environments using retrieved\\ntext/audio context.\\nDocument Retrieval and Layout Understanding\\nRecent research has moved beyond traditional uni-\\nmodal retrieval, developing models that process\\nentire documents by integrating textual, visual, and\\nlayout information. ColPali (Faysse et al., 2024)\\npioneers end-to-end document image retrieval by\\nembedding page patches with a vision-language\\nbackbone, bypassing OCR entirely. Models like\\nColQwen2 (Wang et al., 2024d; Faysse et al., 2024)\\nand M3DocVQA (Cho et al., 2024) extend this\\nparadigm with dynamic resolution handling and\\nholistic multi-page reasoning. Newer frameworks\\nrefine efficiency and layout understanding: ViTLP\\n(Mao et al., 2024) and DocLLM (Wang et al., 2024a)\\npre-train generative models to align spatial layouts\\nwith text, while CREAM (Zhang et al., 2024b)\\nemploys coarse-to-fine retrieval with multimodal\\nefficient tuning to balance accuracy and computa-\\ntional costs. Finally, mPLUG-DocOwl 1.5 (Hu et al.,\\n2024a) and 2 (Hu et al., 2024b) unify structure learn-\\ning across formats (e.g., invoices, forms) without\\nOCR dependencies, while SV-RAG (Chen et al.,\\n2025a) leverages MLLMs’ intrinsic retrieval capa-\\nbilities via dual LoRA adapters: one for evidence\\npage retrieval and the other for question answering.\\nRe-ranking and Selection Strategies Effective re-\\ntrieval in multimodal RAG systems requires not only\\nidentifying relevant information but also prioritizing\\nretrieved candidates. Re-ranking and selection strate-\\ngies improve retrieval quality through optimized\\nexample selection, refined relevance scoring, and fil-\\ntering mechanisms. (i) Optimized example selection\\ntechniques often employ multi-step retrieval, inte-\\ngrating both supervised and unsupervised selection\\napproaches (Luo et al., 2024a; Yuan et al., 2023).\\nSupervised methods like Su et al. (2024a) enhance\\nmultimodal inputs using probabilistic control key-\\nwords, whereas RULE (Xia et al., 2024b) calibrates\\nretrieved context via statistical methods like the\\nBonferroni correction (Haynes, 2013) to mitigate\\nfactuality risks. Clustering-based key-frame selec-\\ntion ensures diversity in video-based retrieval (Dong\\net al., 2024b). Advanced (ii) scoring mechanisms\\nare employed by several methods to improve re-\\ntrieval relevance (Mortaheb et al., 2025b,a; Zhi Lim\\net al., 2024). Multimodal similarity measures, in-\\ncluding structural similarity index measure (SSIM)\\n(Wang et al., 2020b), normalized cross-correlation\\n(NCC), and BERTScore (Zhang et al., 2020), aid in\\nre-ranking documents. Some frameworks combine\\nsimilarity scores derived from various modalities\\nfor more robust re-ranking. For example, VR-RAG\\n(Khan et al., 2025) proposes a visual re-ranking\\nframework that combines cross-modal text-image\\nsimilarity with intra-modal visual similarity using\\nDINOv2 (Oquab et al., 2023), demonstrating sig-\\nnificant improvements in open-vocabulary recogni-\\ntion tasks. Hierarchical post-processing integrates\\npassage-level and answer confidence scores for im-\\nproved ranking (Zhang et al., 2024g; Yan and Xie,\\n2024; Xu et al., 2024a). LDRE (Yang et al., 2024b)\\nemploys semantic ensemble methods to adaptively\\nweigh multiple caption features, while RAGTrans\\n(Cheng et al., 2024) and OMG-QA (Nan et al.,\\n2024b) incorporate traditional ranking functions\\nlike BM25 (Robertson and Zaragoza, 2009). (iii)\\nFiltering methods ensure high-quality retrieval by\\neliminating irrelevant data. Hard negative mining, as\\nused in GME (Zhang et al., 2024i) and MM-Embed\\n(Lin et al., 2024a), mitigates modality bias through\\nmodality-aware sampling and synthesized negatives.\\nSimilarly, consensus-based filtering, seen in Mu-\\nRAR (Zhu et al., 2025) and ColPali (Faysse et al.,\\n2024), employs source attribution and multi-vector\\nmapping to filter out low-similarity candidates. Dy-\\nnamic modality filtering methods, such as RAFT\\n(Zhang et al., 2024h) and MAIN-RAG (Chang et al.,\\n2024), train retrievers to disregard confusing data,\\nimproving multimodal retrieval robustness.\\n3.2 Fusion Mechanisms\\nScore Fusion and Alignment Models in this cate-\\ngory utilize distinct strategies to align multimodal\\n5'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Mohammad Mahdi Abootorabi; Amirhosein Zobeiri; Mahdi Dehghani; Mohammadali Mohammadkhani; Bardia Mohammadi; Omid Ghahroodi; Mahdieh Soleymani Baghshah; Ehsaneddin Asgari', 'doi': 'https://doi.org/10.48550/arXiv.2502.08826', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2502.08826v3', 'source': '/content/Multimodal_RAG.pdf', 'total_pages': 34, 'page': 5, 'page_label': '6'}, page_content='representations. Zhi Lim et al. (2024) convert text,\\ntables, and images into a single textual format using\\na cross-encoder trained for relevance scoring. Shari-\\nfymoghaddam et al. (2024) introduce interleaved\\nimage–text pairs that vertically merge multiple few-\\nshot images (as in LLaV A (Liu et al., 2023a)), while\\naligning modalities via CLIP score fusion (Hes-\\nsel et al., 2021) and BLIP feature fusion (Li et al.,\\n2022). Wiki-LLaV A (Caffagni et al., 2024), C3Net\\n(Zhang et al., 2024c), Riedler and Langer (2024),\\nand MegaPairs (Zhou et al., 2024a) embed images\\nand queries into a shared CLIP space. In particular,\\nMegaPairs (Zhou et al., 2024a) scales this approach\\nby integrating both CLIP-based and MLLM-based\\nretrieval, fusing their scores to leverage complemen-\\ntary strengths, but at the cost of increased inference\\ncomplexity. VISA (Ma et al., 2024b) employs the\\nDocument Screenshot Embedding (DSE) model to\\nalign textual queries with visual document represen-\\ntations by encoding both into a shared embedding\\nspace. REVEAL (Hu et al., 2023) injects retrieval\\nscores into attention layers to minimize L2-norm\\ndifferences between query and knowledge embed-\\ndings, and MA-LMM (He et al., 2024) aligns video-\\ntext embeddings via a BLIP-inspired Query Trans-\\nformer (Li et al., 2022). LLM-RA (Jian et al., 2024)\\nconcatenates text and visual embeddings into joint\\nqueries to reduce retrieval noise, while RA-BLIP\\n(Ding et al., 2024b) employs a 3-layer BERT-based\\nadaptive fusion module to unify visual–textual se-\\nmantics. Xue et al. (2024b) use a prototype-based\\nembedding network (Zheng et al., 2023) to map\\nobject-predicate pairs into a shared semantic space,\\naligning visual features with textual prototypes. Re-\\nIMAGEN (Chen et al., 2022b) balances creativity\\nand entity fidelity in text-to-image synthesis via\\ninterleaved classifier-free guidance during diffusion\\nsampling. To improve multimodal alignment, VIS-\\nRAG (Yu et al., 2024) applies position-weighted\\nmean pooling over VLM hidden states, giving higher\\nrelevance to later tokens. In contrast, RAG-Driver\\n(Yuan et al., 2024) aligns visual and language em-\\nbeddings through visual instruction tuning and an\\nMLP projector.\\nAttention-Based Mechanisms Attention-based\\nmethods dynamically modulate cross-modal inter-\\nactions to enable fine-tuned reasoning across tasks,\\nbalancing specificity and interpretability. Cross-\\nattention is frequently used to integrate hetero-\\ngeneous modalities, as in EMERGE (Zhu et al.,\\n2024b), MORE (Cui et al., 2024), and Alzheimer-\\nRAG (Lahiri and Hu, 2024), though often requiring\\ntask-specific attention heads. RAMM (Yuan et al.,\\n2023) employs a dual-stream co-attention trans-\\nformer, combining self-attention and cross-attention\\nto fuse retrieved biomedical images/texts with in-\\nput data. RAGTrans (Cheng et al., 2024) applies\\nuser-aware attention to social media features. MV-\\nAdapter (Jin et al., 2024) introduces Cross-Modality\\nTying to align video-text embeddings by sharing\\nlatent factors, improving robustness but reducing\\ngranularity of modality-specific features. M2-RAAP\\n(Dong et al., 2024b) enhances fusion through an\\nauxiliary caption-guided strategy that re-weights\\nframes and text captions based on intra-modal simi-\\nlarity, then uses a mutual-guided alignment head to\\nfilter misaligned features via dot-product similarity\\nand frame-to-token attention; however, this method\\nis computationally intensive. Xu et al. (2024a) con-\\ndition text generation on visual features using gated\\ncross-attention, optimizing controllability but re-\\nquiring aligned supervision, and Mu-RAG (Chen\\net al., 2022a) employs intermediate cross-attention\\nfor open-domain QA. Kim et al. (2024) leverage\\ncross-modal memory retrieval with pre-trained CLIP\\nViT-L/14 to map video-text pairs into a shared space,\\nenabling dense captioning through the attention-\\nbased fusion of retrieved memories.\\nUnified Frameworks and Projections Unified\\nframeworks and projection methods consolidate mul-\\ntimodal inputs into coherent representations. Su et al.\\n(2024a) employ hierarchical cross-chains and late\\nfusion for healthcare data, while IRAMIG (Liu et al.,\\n2024b) iteratively integrates multimodal results into\\nunified knowledge representations, enhancing con-\\nsistency but requiring multiple reasoning passes.\\nM3DocRAG (Cho et al., 2024) flattens multi-page\\ndocuments into a single embedding tensor, and PDF-\\nMVQA (Ding et al., 2024d) proposes a joint-grained\\nretriever that fuses coarse-grained semantic entity\\nrepresentations with their fine-grained token-level\\ntextual content, creating a richer, unified representa-\\ntion. DQU-CIR (Wen et al., 2024) unifies raw data\\nby converting images into text captions for complex\\nqueries and overlaying text onto images for sim-\\nple ones, then fusing embeddings via MLP-learned\\nweights. SAM-RAG (Zhai, 2024) aligns image-text\\nmodalities by generating captions for images, con-\\nverting the multimodal input to unimodal text for\\nsubsequent processing. UFineBench (Zuo et al.,\\n2024) uses a shared granularity decoder for ultra-fine\\ntext–person retrieval. Nguyen et al. (2024) intro-\\nduce Dense2Sparse projection, converting dense\\nembeddings from models like BLIP/ALBEF (Li\\n6'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Mohammad Mahdi Abootorabi; Amirhosein Zobeiri; Mahdi Dehghani; Mohammadali Mohammadkhani; Bardia Mohammadi; Omid Ghahroodi; Mahdieh Soleymani Baghshah; Ehsaneddin Asgari', 'doi': 'https://doi.org/10.48550/arXiv.2502.08826', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2502.08826v3', 'source': '/content/Multimodal_RAG.pdf', 'total_pages': 34, 'page': 6, 'page_label': '7'}, page_content='et al., 2022) into sparse lexical vectors using layer\\nnormalization and probabilistic expansion control to\\noptimize storage and interpretability.\\n3.3 Augmentation Techniques\\nBasic RAG systems typically retrieve content in\\na single step, directly passing it to generation, of-\\nten leading to inefficiencies and suboptimal out-\\nputs. Augmentation techniques refine retrieved data\\nbeforehand, improving multimodal interpretation,\\nstructuring, and integration (Gao et al., 2023).\\nContext Enrichment This focuses on enhancing\\nthe relevance of retrieved knowledge by refining\\nor expanding retrieved data. General approaches\\nincorporate additional contextual elements (e.g.,\\ntext chunks, image tokens, structured data) to pro-\\nvide a richer grounding for generation (Caffagni\\net al., 2024; Xue et al., 2024b). EMERGE (Zhu\\net al., 2024b) enriches context by integrating entity\\nrelationships and semantic descriptions. MiRAG\\n(Adjali et al., 2024) expands initial queries through\\nentity retrieval and reformulation, enhancing sub-\\nsequent stages for the visual question-answering.\\nVideo-RAG (Luo et al., 2024b) enhances long-video\\nunderstanding through Query Decoupling, which\\nreformulates user queries into structured retrieval\\nrequests to extract auxiliary multimodal context.\\nImg2Loc (Zhou et al., 2024e) boosts accuracy by in-\\ncluding both similar and dissimilar points in prompts,\\nhelping rule out implausible locations.\\nAdaptive and Iterative Retrieval For more com-\\nplex queries, dynamic retrieval mechanisms have\\nproven effective. Adaptive retrieval approaches\\noptimize relevance by adjusting retrieval dynami-\\ncally. For instance, UniversalRAG (Yeo et al., 2025)\\nintroduces a framework that adapts retrieval by dy-\\nnamically routing queries to the most suitable corpus\\nbased on both the required modality and granularity\\n(e.g., paragraph vs. document, clip vs. full video),\\nthereby addressing the specific knowledge type\\nand scope demanded by the query. SKURG (Yang\\net al., 2023) determines the number of retrieval\\nhops based on query complexity. SAM-RAG (Zhai,\\n2024) and mR2AG (Zhang et al., 2024f) dynami-\\ncally assess the need for external knowledge and\\nfilter irrelevant content using MLLMs to retain only\\ntask-critical information. MMed-RAG (Xia et al.,\\n2024a) further improves retrieval precision by dis-\\ncarding low-relevance results, while OmniSearch (Li\\net al., 2024d) decomposes multimodal queries into\\nstructured sub-questions, planning retrieval actions\\nin real time. Iterative approaches refine results over\\nmultiple steps by incorporating feedback from prior\\niterations. For example, OMGM (Yang et al., 2025)\\norchestrates a multi-step, coarse-to-fine retrieval pro-\\ncess for knowledge-based visual question answering,\\nstarting with a broad entity search and progressively\\nrefining the selection through multimodal reranking\\nand fine-grained textual filtering to pinpoint the most\\nrelevant knowledge, achieving superior retrieval per-\\nformance in comparison to prior methods. IRAMIG\\n(Liu et al., 2024b) improves multimodal retrieval\\nby dynamically updating queries based on retrieved\\ncontent. OMG-QA (Nan et al., 2024b) integrates\\nepisodic memory to refine retrieval across multiple\\nrounds, ensuring continuity in reasoning. RAGAR\\n(Khaliq et al., 2024) further enhances contextual\\nconsistency by iteratively adjusting retrieval based\\non prior responses and multimodal analysis.\\n3.4 Generation Techniques\\nIn-Context Learning (ICL) ICL with retrieval aug-\\nmentation enhances reasoning in multimodal RAGs\\nby leveraging retrieved content as few-shot examples\\nwithout requiring retraining. Models such as RMR\\n(Tan et al., 2024), Sharifymoghaddam et al. (2024),\\nand RA-CM3 (Yasunaga et al., 2023), extend this\\nparadigm to multimodal RAG settings. RAG-Driver\\n(Yuan et al., 2024) refines ICL by retrieving rele-\\nvant driving experiences from a memory database.\\nMSIER (Luo et al., 2024a) improves example se-\\nlection with a multimodal supervised in-context\\nexamples retrieval framework, using an MLLM\\nscorer to assess textual and visual relevance. Raven\\n(Rao et al., 2024) introduces Fusion-in-Context\\nLearning, integrating diverse in-context examples\\nfor superior performance over standard ICL.\\nReasoning Reasoning methods, like chain of\\nthought (CoT), decompose complex reasoning into\\nsequential steps, improving coherence and robust-\\nness in multimodal RAG. RAGAR (Khaliq et al.,\\n2024) refines fact-checking queries and explores\\nbranching reasoning paths by introducing Chain of\\nRAG and Tree of RAG, while VisDoM (Suri et al.,\\n2024) and SAM-RAG (Zhai, 2024) integrate CoT\\nwith evidence curation and multi-stage verification\\nto enhance accuracy and support. Notably, VisDoM\\nperforms well in scenarios where key information is\\ndistributed across modalities. LDRE (Yang et al.,\\n2024b) applies LLMs for divergent compositional\\nreasoning by refining captions using dense descrip-\\ntions and textual modifications, achieving superior\\nzero-shot composed image retrieval results.\\n7'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Mohammad Mahdi Abootorabi; Amirhosein Zobeiri; Mahdi Dehghani; Mohammadali Mohammadkhani; Bardia Mohammadi; Omid Ghahroodi; Mahdieh Soleymani Baghshah; Ehsaneddin Asgari', 'doi': 'https://doi.org/10.48550/arXiv.2502.08826', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2502.08826v3', 'source': '/content/Multimodal_RAG.pdf', 'total_pages': 34, 'page': 7, 'page_label': '8'}, page_content='Instruction Tuning Several works have fine-tuned\\nor instruct-tuned generation components for spe-\\ncific applications. RA-BLIP (Ding et al., 2024b)\\nleverages the Q-Former architecture from Instruct-\\nBLIP (Dai et al., 2023) to extract visual features\\nbased on question instructions, while RAGPT (Lang\\net al., 2025) employs a context-aware prompter to\\ngenerate dynamic prompts from relevant instances.\\nMR2AG (Zhang et al., 2024f) and RagVL (Chen\\net al., 2024e) train MLLMs to invoke retrieval adap-\\ntively, identify relevant evidence, and enhance rank-\\ning capabilities for improved response accuracy.\\nJang et al. (2024) focus on distinguishing image\\ndifferences to generate descriptive textual responses.\\nMMed-RAG (Xia et al., 2024a) applies preference\\nfine-tuning to help models balance retrieved knowl-\\nedge with internal reasoning. To improve generation\\nquality, MegaPairs (Zhou et al., 2024a) and Surf\\n(Sun et al., 2024a) construct multimodal instruction-\\ntuning datasets from prior LLM errors, while Rule\\n(Xia et al., 2024b) refines a medical large vision lan-\\nguage model through direct preference optimization\\nto mitigate overreliance on retrieved contexts.\\nSource Attribution and Evidence Transparency\\nEnsuring source attribution in multimodal RAG sys-\\ntems is a significant research focus. OMG-QA (Nan\\net al., 2024b) prompts LLMs for explicit evidence\\ncitation in generated responses. MuRAR (Zhu et al.,\\n2025) refines an LLM’s initial response by integrat-\\ning multimodal information from a source-based\\nretriever to improve informativeness. However, its\\nrecall is constrained, as the retriever may miss evi-\\ndence spanning different sections or web documents.\\nSimilarly, VISA (Ma et al., 2024b) employs vision-\\nlanguage models to generate answers with visual\\nsource attribution by highlighting evidence in re-\\ntrieved screenshots. Nevertheless, its attribution\\naccuracy degrades when evidence spans multiple\\nsections or requires cross-modal integration.\\nAgentic Generation and Interaction Agent-driven\\nmultimodal RAG uses versatile autonomous/semi-\\nautonomous systems across diverse interaction\\nparadigms and specialized domains, often generat-\\ning complex outputs. For user interaction, AppAgent\\nv2 (Li et al., 2024c) enables mobile GUI naviga-\\ntion while USER-LLM R1 (Rahimi et al., 2025)\\ncreates personalized conversational agents via dy-\\nnamic profiling, particularly for elderly users. In\\nspecialized applications, MMAD (Jiang et al., 2025)\\naddresses industrial anomaly detection with training-\\nfree enhancement strategies, Yi et al. (2025) improve\\nclinical report generation while reducing hallucina-\\ntion, and CollEX (Schneider et al., 2025) facilitates\\nscientific collection exploration for researchers and\\nlearners. For complex reasoning, HM-RAG (Liu\\net al., 2025a) coordinates hierarchical multi-agent\\ncollaboration across multimodal data streams, while\\nCogPlanner (Yu et al., 2025) introduces a cognitively\\ninspired planning framework that iteratively refines\\nqueries and selects retrieval strategies adaptively.\\n3.5 Training Strategies\\nTraining multimodal RAG models follows a multi-\\nstage process to effectively capture cross-modal\\ninteractions (Chen et al., 2022a). Pretraining on\\nlarge paired datasets establishes cross-modal rela-\\ntionships, while fine-tuning adapts models to task-\\nspecific objectives by aligning outputs with task\\nrequirements (Ye et al., 2019). For example, RE-\\nVEAL (Hu et al., 2023) integrates multiple training\\nobjectives. Its pretraining phase optimizes Prefix\\nLanguage Modeling Loss ( LPrefixLM), where text\\nis predicted from a given prefix and an associated\\nimage. Supporting losses include Contrastive Loss\\n(Lcontra) which aligns queries with pseudo-ground-\\ntruth knowledge, Disentangled Regularization Loss\\n(Ldecor) to enhance embedding expressiveness, and\\nAlignment Regularization Loss ( Lalign) to refine\\nquery-knowledge alignment. Fine-tuning employs a\\ncross-entropy objective for downstream tasks like\\nvisual question answering or image captioning. De-\\ntails on robustness advancements and loss formula-\\ntions are in Appendix (§D).\\nAlignment Contrastive learning improves repre-\\nsentation quality by pulling positive pairs closer\\nand pushing negative pairs apart in the embedding\\nspace. The InfoNCE loss (van den Oord et al., 2019)\\nis widely employed in multimodal RAG models,\\nincluding VISRAG (Yu et al., 2024), MegaPairs\\n(Zhou et al., 2024a), and SAM-RAG (Zhai, 2024),\\nto improve retrieval-augmented generation. Sev-\\neral models introduce refinements to contrastive\\ntraining. EchoSight (Yan and Xie, 2024) enhances\\nretrieval accuracy by selecting visually similar yet\\nsemantically distinct negatives, while HACL (Jiang\\net al., 2024) mitigates hallucinations by incorporat-\\ning adversarial captions as distractors. Similarly,\\nUniRaG (Zhi Lim et al., 2024) improves retrieval\\nrobustness by leveraging hard negative documents\\nto help the model discriminate between relevant\\nand irrelevant contexts. The eCLIP loss (Kumar\\nand Marttinen, 2024) extends contrastive learning\\nby integrating expert-annotated data and an auxil-\\niary MSE loss to refine embedding quality. Mixup\\n8'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Mohammad Mahdi Abootorabi; Amirhosein Zobeiri; Mahdi Dehghani; Mohammadali Mohammadkhani; Bardia Mohammadi; Omid Ghahroodi; Mahdieh Soleymani Baghshah; Ehsaneddin Asgari', 'doi': 'https://doi.org/10.48550/arXiv.2502.08826', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2502.08826v3', 'source': '/content/Multimodal_RAG.pdf', 'total_pages': 34, 'page': 8, 'page_label': '9'}, page_content='strategies further improve generalization by generat-\\ning synthetic positive pairs (Kumar and Marttinen,\\n2024). Dense2Sparse (Nguyen et al., 2024) employs\\nimage-to-caption ℓ(I → C) and caption-to-image\\nℓ(C → I) losses, while enforcing sparsity through\\nℓ1 regularization, optimizing retrieval precision by\\nbalancing dense and sparse representations.\\n4 Open Problems and Future Directions\\nAdditional challenges and future directions about\\nlong-context processing, scalability, efficiency, and\\npersonalization are discussed in Appendix (§F).\\nGeneralization, Explainability, and Robustness\\nMultimodal RAG systems often struggle with do-\\nmain adaptation and exhibit modality biases, fre-\\nquently over-relying on text for both retrieval and\\ngeneration (Winterbottom et al., 2020). Explainabil-\\nity remains a major challenge, as these systems often\\nattribute responses to broad sources, citing entire\\ndocuments or large visual regions instead of pinpoint-\\ning exact contributing elements across modalities\\n(Ma et al., 2024b; Hu et al., 2023). Moreover, the\\ninterplay between modalities affects the outcome\\nquality; for example, answers derived solely from\\ntext sources may differ in quality compared to those\\nrequiring a combination of text and image inputs\\n(Baltrusaitis et al., 2019). They are also vulnera-\\nble to adversarial perturbations, such as misleading\\nimages influencing textual outputs, and their per-\\nformance degrades when relying on low-quality\\nor outdated sources (Chen et al., 2022b). MM-\\nPoisonRAG (Ha et al., 2025) and Poisoned-MRAG\\n(Liu et al., 2025b) demonstrate that even a few adver-\\nsarial knowledge injections can hijack cross-modal\\nretrieval and derail generation, underscoring the\\nimperative for robust defense mechanisms against\\nknowledge poisoning in multimodal RAG systems.\\nWhile the trustworthiness of unimodal RAGs has\\nbeen studied (Zhou et al., 2024d), ensuring robust-\\nness in multimodal RAGs remains an open challenge\\nand a crucial research direction.\\nReasoning, Alignment, and Retrieval Enhance-\\nment Multimodal RAGs struggle with compo-\\nsitional reasoning, requiring logical integration\\nof information across modalities for coherent,\\ncontext-rich outputs. While cross-modal techniques\\nlike Multimodal-CoT (Zhang et al., 2023b) have\\nemerged, further advancements are needed to en-\\nhance coherence and contextual relevance. Improv-\\ning modality alignment and entity-aware retrieval is\\ncrucial. Moreover, despite the potential of knowl-\\nedge graphs to enrich cross-modal reasoning, they\\nremain underexplored in multimodal RAGs com-\\npared to text-based RAGs (Zhang et al., 2024f;\\nProcko and Ochoa, 2024). Retrieval biases such\\nas position sensitivity (Hu et al., 2024c), redun-\\ndancy (Nan et al., 2024b), and biases from training\\ndata or retrieved content (Zhai, 2024), pose signifi-\\ncant challenges. A promising direction is a unified\\nembedding space for all modalities, enabling di-\\nrect multimodal search without intermediary models\\n(e.g., ASRs). Despite progress, mapping multimodal\\nknowledge into a unified space remains an open\\nchallenge with substantial potential.\\nAgent-Based and Self-Guided Systems Recent\\ntrends indicate a shift towards agent-based multi-\\nmodal RAGs that integrate retrieval, reasoning, and\\ngeneration across diverse domains. Unlike static\\nRAGs, future systems should incorporate interac-\\ntive feedback and self-guided decision-making to\\niteratively refine outputs. Existing feedback mecha-\\nnisms often fail to determine whether errors stem\\nfrom retrieval, generation, or other stages (Dong\\net al., 2024b). The incorporation of reinforcement\\nlearning and end-to-end human-aligned feedback\\nremains largely overlooked but holds significant\\npotential for assessing whether retrieval is necessary,\\nevaluating the relevance of retrieved content, and\\ndynamically determining the most suitable modal-\\nities for response generation. Robust support for\\nany-to-any modality is crucial for open-ended tasks\\n(Wu et al., 2024b). Future multimodal RAGs should\\nincorporate data from diverse real-world sources,\\nsuch as environmental sensors, alongside traditional\\nmodalities to enhance situational awareness. This\\nprogression aligns with the trend toward embodied\\nAI, where models integrate knowledge with physical\\ninteraction, enabling applications in robotics, navi-\\ngation, and physics-informed reasoning. Bridging\\nretrieval-based reasoning with real-world agency\\nbrings these systems closer to AGI.\\n5 Conclusion\\nThis study provides a comprehensive review of mul-\\ntimodal RAG, categorizing key advancements in\\nretrieval, multimodal fusion, augmentation, gen-\\neration, training strategies, and agents. We also\\nexamine task-specific applications, datasets, bench-\\nmarks, and evaluation methods while highlighting\\nopen challenges and promising future directions. We\\nhope this work inspires future research, particularly\\nin enhancing cross-modal reasoning and retrieval,\\ndeveloping agent-based interactive systems, and\\nadvancing unified multimodal embedding spaces.\\n9'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Mohammad Mahdi Abootorabi; Amirhosein Zobeiri; Mahdi Dehghani; Mohammadali Mohammadkhani; Bardia Mohammadi; Omid Ghahroodi; Mahdieh Soleymani Baghshah; Ehsaneddin Asgari', 'doi': 'https://doi.org/10.48550/arXiv.2502.08826', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2502.08826v3', 'source': '/content/Multimodal_RAG.pdf', 'total_pages': 34, 'page': 9, 'page_label': '10'}, page_content='6 Limitations\\nThis study offers a comprehensive examination of\\nmultimodal RAG systems. Extended discussions,\\ndetails of datasets and benchmarks, and additional\\nrelevant work are available in the Appendices. While\\nwe have made our maximum effort; however, some\\nlimits may persist. First, due to space constraints,\\nour descriptions of individual methodologies are nec-\\nessarily concise. Second, although we curate studies\\nfrom major venues (e.g., ACL, EMNLP, NeurIPS,\\nCVPR, ICLR, ICML, ACM Multimedia) and arXiv,\\nour selection may inadvertently overlook emerging\\nor domain-specific research, with a primary focus on\\nrecent advancements. Additionally, this work does\\nnot include a comparative performance evaluation\\nof the various models, as task definitions, evaluation\\nmetrics, and implementation details vary signifi-\\ncantly across studies, and executing these models\\nrequires substantial computational resources.\\nFurthermore, multimodal RAG is a rapidly evolving\\nfield with many open questions, such as optimiz-\\ning fusion strategies for diverse modalities and ad-\\ndressing scalability challenges. As new paradigms\\nemerge, our taxonomy and conclusions will in-\\nevitably evolve. To address these gaps, we plan\\nto continuously monitor developments and update\\nthis survey and the corresponding repository to in-\\ncorporate overlooked contributions and refine our\\nperspectives.\\n7 Ethical Statement\\nThis survey provides a comprehensive review of\\nresearch on multimodal RAG systems, offering in-\\nsights that we believe will be valuable to researchers\\nin this evolving field. All the studies, datasets,\\nand benchmarks analyzed in this work are publicly\\navailable, with only a very small number of pa-\\npers requiring institutional access. Additionally,\\nthis survey does not involve personal data or user\\ninteractions, and we adhere to ethical guidelines\\nthroughout.\\nSince this work is purely a survey of existing litera-\\nture and does not introduce new models, datasets, or\\nexperimental methodologies, it presents no potential\\nrisks. However, we acknowledge that multimodal\\nRAG systems inherently raise ethical concerns, in-\\ncluding bias, misinformation, privacy, and intellec-\\ntual property issues. Bias can emerge from both\\nretrieval and generation processes, potentially lead-\\ning to skewed or unfair outputs. Additionally, these\\nmodels may hallucinate or propagate misinforma-\\ntion, particularly when retrieval mechanisms fail or\\nrely on unreliable sources. The handling of sensi-\\ntive multimodal data also poses privacy risks, while\\ncontent generation raises concerns about proper\\nattribution and copyright compliance. Addressing\\nthese challenges requires careful dataset curation,\\nbias mitigation strategies, and transparent evaluation\\nof retrieval and generation mechanisms.\\nReferences\\nMohammad Mahdi Abootorabi and Ehsaneddin Asgari.\\n2024. Clasp: Contrastive language-speech pretrain-\\ning for multilingual multimodal information retrieval.\\nPreprint, arXiv:2412.13071.\\nOmar Adjali, Olivier Ferret, Sahar Ghannay, and Hervé\\nLe Borgne. 2024. Multi-level information retrieval\\naugmented generation for knowledge-based visual\\nquestion answering. In Proceedings of the 2024 Con-\\nference on Empirical Methods in Natural Language\\nProcessing, pages 16499–16513, Miami, Florida, USA.\\nAssociation for Computational Linguistics.\\nAndrea Agostinelli, Timo I Denk, Zalán Borsos, Jesse\\nEngel, Mauro Verzetti, Antoine Caillon, Qingqing\\nHuang, Aren Jansen, Adam Roberts, Marco Tagliasac-\\nchi, et al. 2023. Musiclm: Generating music from\\ntext. arXiv preprint arXiv:2301.11325.\\nHarsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen,\\nRishabh Jain, et al. 2019. Nocaps: Novel object\\ncaptioning at scale. Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision (ICCV),\\npages 1–10.\\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\\nArthur Mensch, Katie Millicah, Malcolm Reynolds,\\nRoman Ring, Eliza Rutherford, Serkan Cabi, Tengda\\nHan, Zhitao Gong, Sina Samangooei, Marianne\\nMonteiro, Jacob Menick, Sebastian Borgeaud, An-\\ndrew Brock, Aida Nematzadeh, Sahand Sharifzadeh,\\nMikolaj Binkowski, Ricardo Barreira, Oriol Vinyals,\\nAndrew Zisserman, and Karen Simonyan. 2024.\\nFlamingo: a visual language model for few-shot learn-\\ning. In Proceedings of the 36th International Confer-\\nence on Neural Information Processing Systems, NIPS\\n’22, Red Hook, NY , USA. Curran Associates Inc.\\nZhiyu An, Xianzhong Ding, Yen-Chun Fu, Cheng-\\nChung Chu, Yan Li, and Wan Du. 2024. Golden-\\nretriever: High-fidelity agentic retrieval augmented\\ngeneration for industrial knowledge base. Preprint,\\narXiv:2408.00798.\\nPeter Anderson, Basura Fernando, Mark Johnson, and\\nStephen Gould. 2016. Spice: Semantic propositional\\nimage caption evaluation. In Computer Vision–ECCV\\n2016: 14th European Conference, Amsterdam, The\\nNetherlands, October 11-14, 2016, Proceedings, Part\\nV 14, pages 382–398. Springer.\\n10'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Mohammad Mahdi Abootorabi; Amirhosein Zobeiri; Mahdi Dehghani; Mohammadali Mohammadkhani; Bardia Mohammadi; Omid Ghahroodi; Mahdieh Soleymani Baghshah; Ehsaneddin Asgari', 'doi': 'https://doi.org/10.48550/arXiv.2502.08826', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2502.08826v3', 'source': '/content/Multimodal_RAG.pdf', 'total_pages': 34, 'page': 10, 'page_label': '11'}, page_content='Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-\\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen,\\nEric Chu, Jonathan H. Clark, Laurent El Shafey, Yan-\\nping Huang, Kathy Meier-Hellstern, Gaurav Mishra,\\nErica Moreira, Mark Omernick, Kevin Robinson, Se-\\nbastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu,\\nYujing Zhang, Gustavo Hernandez Abrego, Junwhan\\nAhn, Jacob Austin, Paul Barham, Jan Botha, James\\nBradbury, Siddhartha Brahma, Kevin Brooks, Michele\\nCatasta, Yong Cheng, Colin Cherry, Christopher A.\\nChoquette-Choo, Aakanksha Chowdhery, Clément\\nCrepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev,\\nJacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad\\nFeinberg, Fangxiaoyu Feng, Vlad Fienber, Markus\\nFreitag, Xavier Garcia, Sebastian Gehrmann, Lucas\\nGonzalez, and et al. 2023. Palm 2 technical report.\\nPreprint, arXiv:2305.10403.\\nLisa Anne Hendricks, Oliver Wang, Eli Shechtman,\\nJosef Sivic, Trevor Darrell, and Bryan Russell. 2017.\\nLocalizing moments in video with natural language.\\nIn Proceedings of the IEEE International Conference\\non Computer Vision (ICCV).\\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\\ngaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and\\nDevi Parikh. 2015. Vqa: Visual question answering.\\nProceedings of the IEEE International Conference on\\nComputer Vision, pages 2425–2433.\\nMd Adnan Arefeen, Biplob Debnath, Md Yusuf Sarwar\\nUddin, and Srimat Chakradhar. 2024. irag: Advanc-\\ning rag for videos with an incremental approach. In\\nProceedings of the 33rd ACM International Confer-\\nence on Information and Knowledge Management,\\nCIKM ’24, page 4341–4348. ACM.\\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\\nHannaneh Hajishirzi. 2023. Self-RAG: Learning to\\nretrieve, generate, and critique through self-reflection.\\narXiv preprint arXiv:2310.11511.\\nAnas Awadalla, Le Xue, Oscar Lo, Manli Shu, Hannah\\nLee, Etash Guha, Sheng Shen, Mohamed Awadalla,\\nSilvio Savarese, Caiming Xiong, et al. 2024. Mint-\\n1t: Scaling open-source multimodal data by 10x: A\\nmultimodal dataset with one trillion tokens. Advances\\nin Neural Information Processing Systems, 37:36805–\\n36828.\\nAdil Bahaj and Mounir Ghogho. 2024. Asthmabot:\\nMulti-modal, multi-lingual retrieval augmented gen-\\neration for asthma patient support. arXiv preprint\\narXiv:2409.15815.\\nMax Bain, Arsha Nagrani, Gul Varol, and Andrew Zisser-\\nman. 2021. Frozen in time: A joint video and image\\nencoder for end-to-end retrieval. Proceedings of the\\nIEEE/CVF International Conference on Computer\\nVision (ICCV), pages 1–10.\\nAlberto Baldrati, Marco Bertini, and Alberto Del Bimbo.\\n2023. Zero-shot composed image retrieval with\\ntextual inversion. Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision (ICCV),\\npages 1–10.\\nTadas Baltrusaitis, Chaitanya Ahuja, and Louis-Philippe\\nMorency. 2019. Multimodal machine learning: A\\nsurvey and taxonomy. IEEE Trans. Pattern Anal.\\nMach. Intell., 41(2):423–443.\\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR:\\nAn automatic metric for MT evaluation with improved\\ncorrelation with human judgments. In Proceedings of\\nthe ACL Workshop on Intrinsic and Extrinsic Evalua-\\ntion Measures for Machine Translation and/or Sum-\\nmarization, pages 65–72, Ann Arbor, Michigan. Asso-\\nciation for Computational Linguistics.\\nSoyuj Basnet, Jerry Gou, Antonio Mallia, and Torsten\\nSuel. 2024. Deeperimpact: Optimizing sparse learned\\nindex structures. Preprint, arXiv:2405.17093.\\nMikołaj Bi´nkowski, Dougal J. Sutherland, Michael Arbel,\\nand Arthur Gretton. 2018. Demystifying MMD\\nGANs. In International Conference on Learning\\nRepresentations.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\\nGretchen Krueger, Tom Henighan, Rewon Child,\\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\\nteusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,\\nChristopher Berner, Sam McCandlish, Alec Radford,\\nIlya Sutskever, and Dario Amodei. 2020. Language\\nmodels are few-shot learners. In Advances in Neural\\nInformation Processing Systems, volume 33, pages\\n1877–1901. Curran Associates, Inc.\\nKyle Buettner and Adriana Kovashka. 2024. Quantify-\\ning the gaps between translation and native percep-\\ntion in training for multimodal, multilingual retrieval.\\nIn Proceedings of the 2024 Conference on Empiri-\\ncal Methods in Natural Language Processing, pages\\n5863–5870, Miami, Florida, USA. Association for\\nComputational Linguistics.\\nFabian Caba Heilbron, Victor Escorcia, Bernard Ghanem,\\nand Juan Carlos Niebles. 2015. Activitynet: A\\nlarge-scale video benchmark for human activity un-\\nderstanding. Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition (CVPR),\\npages 961–970.\\nDavide Caffagni, Federico Cocchi, Nicholas Moratelli,\\nSara Sarto, Marcella Cornia, Lorenzo Baraldi, and Rita\\nCucchiara. 2024. Wiki-llava: Hierarchical retrieval-\\naugmented generation for multimodal llms. In Pro-\\nceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition, pages 1818–1826.\\nMu Cai, Jianwei Yang, Jianfeng Gao, and Yong Jae\\nLee. 2025. Matryoshka multimodal models. In\\nThe Thirteenth International Conference on Learning\\nRepresentations.\\n11'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Mohammad Mahdi Abootorabi; Amirhosein Zobeiri; Mahdi Dehghani; Mohammadali Mohammadkhani; Bardia Mohammadi; Omid Ghahroodi; Mahdieh Soleymani Baghshah; Ehsaneddin Asgari', 'doi': 'https://doi.org/10.48550/arXiv.2502.08826', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2502.08826v3', 'source': '/content/Multimodal_RAG.pdf', 'total_pages': 34, 'page': 11, 'page_label': '12'}, page_content='Yee Seng Chan and Hwee Tou Ng. 2008. Maxsim: A\\nmaximum similarity metric for machine translation\\nevaluation. In Proceedings of ACL-08: HLT, pages\\n55–62.\\nAngel X Chang, Thomas Funkhouser, Leonidas Guibas,\\nPat Hanrahan, Qixing Huang, et al. 2015. Shapenet:\\nAn information-rich 3d model repository. arXiv\\npreprint arXiv:1512.03012.\\nChia-Yuan Chang, Zhimeng Jiang, Vineeth Rakesh,\\nMenghai Pan, Chin-Chia Michael Yeh, Guanchu Wang,\\nMingzhi Hu, Zhichao Xu, Yan Zheng, Mahashweta\\nDas, and Na Zou. 2024. Main-rag: Multi-agent\\nfiltering retrieval-augmented generation. Preprint,\\narXiv:2501.00332.\\nYingshan Chang, Mridu Narang, Hisami Suzuki, Gui-\\nhong Cao, Jianfeng Gao, and Yonatan Bisk. 2022.\\nWebqa: Multihop and multimodal qa. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition (CVPR), pages 16495–16504.\\nChoi Changin, Lim Sungjun, and Rhee Wonjong. 2024.\\nAudio captioning rag via generative pair-to-pair re-\\ntrieval with refined knowledge base. Preprint,\\narXiv:2410.10913.\\nDavid L Chen and William B Dolan. 2011. Collecting\\nhighly parallel data for paraphrase evaluation. Pro-\\nceedings of the 49th Annual Meeting of the Associa-\\ntion for Computational Linguistics: Human Language\\nTechnologies, pages 190–200.\\nJian Chen, Ruiyi Zhang, Yufan Zhou, Tong Yu, Franck\\nDernoncourt, Jiuxiang Gu, Ryan A. Rossi, Changyou\\nChen, and Tong Sun. 2025a. Sv-rag: Lora-\\ncontextualizing adaptation of mllms for long document\\nunderstanding. Preprint, arXiv:2411.01106.\\nJianlyu Chen, Shitao Xiao, Peitian Zhang, Kun Luo,\\nDefu Lian, and Zheng Liu. 2024a. M3-embedding:\\nMulti-linguality, multi-functionality, multi-granularity\\ntext embeddings through self-knowledge distillation.\\nIn Findings of the Association for Computational\\nLinguistics: ACL 2024, pages 2318–2335, Bangkok,\\nThailand. Association for Computational Linguistics.\\nJianlyu Chen, Shitao Xiao, Peitian Zhang, Kun Luo,\\nDefu Lian, and Zheng Liu. 2024b. M3-embedding:\\nMulti-linguality, multi-functionality, multi-granularity\\ntext embeddings through self-knowledge distillation.\\nIn Findings of the Association for Computational\\nLinguistics ACL 2024, pages 2318–2335.\\nLin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui\\nHe, Jiaqi Wang, Feng Zhao, and Dahua Lin. 2024c.\\nSharegpt4v: Improving large multi-modal models\\nwith better captions. In European Conference on\\nComputer Vision, pages 370–387. Springer.\\nRan Chen, Xueqi Yao, and Xuhui Jiang. 2024d.\\nLlm4design: An automated multi-modal system for ar-\\nchitectural and environmental design. arXiv preprint\\narXiv:2407.12025.\\nWenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and\\nWilliam Cohen. 2022a. Murag: Multimodal retrieval-\\naugmented generator for open question answering\\nover images and text. In Proceedings of the 2022\\nConference on Empirical Methods in Natural Lan-\\nguage Processing, pages 5558–5570, Abu Dhabi,\\nUnited Arab Emirates. Association for Computational\\nLinguistics.\\nWenhu Chen, Hexiang Hu, Chitwan Saharia, and\\nWilliam W. Cohen. 2022b. Re-imagen: Retrieval-\\naugmented text-to-image generator. Preprint,\\narXiv:2209.14491.\\nYang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit\\nChangpinyo, Alan Ritter, and Ming-Wei Chang. 2023.\\nCan pre-trained vision and language models answer\\nvisual information-seeking questions? In Proceedings\\nof the 2023 Conference on Empirical Methods in\\nNatural Language Processing, pages 14948–14968,\\nSingapore. Association for Computational Linguistics.\\nYifu Chen, Shengpeng Ji, Haoxiao Wang, Ziqing Wang,\\nSiyu Chen, Jinzheng He, Jin Xu, and Zhou Zhao.\\n2025b. Wavrag: Audio-integrated retrieval augmented\\ngeneration for spoken dialogue models. Preprint,\\narXiv:2502.14727.\\nZhanpeng Chen, Chengjin Xu, Yiyan Qi, and Jian Guo.\\n2024e. Mllm is a strong reranker: Advancing multi-\\nmodal retrieval-augmented generation via knowledge-\\nenhanced reranking and noise-injected training. arXiv\\npreprint arXiv:2407.21439.\\nZhangtao Cheng, Jienan Zhang, Xovee Xu, Goce Tra-\\njcevski, Ting Zhong, and Fan Zhou. 2024. Retrieval-\\naugmented hypergraph for multimodal social media\\npopularity prediction. In Proceedings of the 30th\\nACM SIGKDD Conference on Knowledge Discovery\\nand Data Mining, KDD ’24, page 445–455, New York,\\nNY , USA. Association for Computing Machinery.\\nFelix Chern, Blake Hechtman, Andy Davis, Ruiqi Guo,\\nDavid Majnemer, and Sanjiv Kumar. 2022. Tpu-knn:\\nK nearest neighbor search at peak flop/s. Advances in\\nNeural Information Processing Systems, 35:15489–\\n15501.\\nJaemin Cho, Debanjan Mahata, Ozan Irsoy, Yujie He,\\nand Mohit Bansal. 2024. M3docrag: Multi-modal re-\\ntrieval is what you need for multi-page multi-document\\nunderstanding. Preprint, arXiv:2411.04952.\\nKyoyun Choi, Byungmu Yoon, Soobum Kim, and\\nJonggwon Park. 2025. Leveraging llms for mul-\\ntimodal retrieval-augmented radiology report gen-\\neration via key phrase extraction. arXiv preprint\\narXiv:2504.07415.\\nYunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo\\nHa, Sunghun Kim, and Jaegul Choo. 2021. Viton-hd:\\nHigh-resolution virtual try-on via image translation.\\nProceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition (CVPR), pages\\n14131–14140.\\n12'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Mohammad Mahdi Abootorabi; Amirhosein Zobeiri; Mahdi Dehghani; Mohammadali Mohammadkhani; Bardia Mohammadi; Omid Ghahroodi; Mahdieh Soleymani Baghshah; Ehsaneddin Asgari', 'doi': 'https://doi.org/10.48550/arXiv.2502.08826', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2502.08826v3', 'source': '/content/Multimodal_RAG.pdf', 'total_pages': 34, 'page': 12, 'page_label': '13'}, page_content='Wanqing Cui, Keping Bi, Jiafeng Guo, and Xueqi\\nCheng. 2024. More: Multi-modal retrieval aug-\\nmented generative commonsense reasoning. Preprint,\\narXiv:2402.13625.\\nWenliang Dai, Junnan Li, Dongxu Li, Anthony\\nMeng Huat Tiong, Junqi Zhao, Weisheng Wang,\\nBoyang Li, Pascale Fung, and Steven Hoi. 2023.\\nInstructblip: towards general-purpose vision-language\\nmodels with instruction tuning. In Proceedings of the\\n37th International Conference on Neural Information\\nProcessing Systems, NIPS ’23, Red Hook, NY , USA.\\nCurran Associates Inc.\\nDima Damen, Hazel Doughty, Giovanni Maria Farinella,\\nAntonino Furnari, Evangelos Kazakos, Jian Ma, Da-\\nvide Moltisanti, Jonathan Munro, Toby Perrett, Will\\nPrice, and Michael Wray. 2022. Rescaling egocentric\\nvision: Collection, pipeline and challenges for epic-\\nkitchens-100. Int. J. Comput. Vision, 130(1):33–55.\\nQuang-Vinh Dang. 2024. Multi-modal retrieval aug-\\nmented generation for product query. Library of\\nProgress-Library Science, Information Technology &\\nComputer, 44(3).\\nRingki Das and Thoudam Doren Singh. 2023. Mul-\\ntimodal sentiment analysis: A survey of methods,\\ntrends, and challenges. ACM Comput. Surv., 55(13s).\\nHanxing Ding, Liang Pang, Zihao Wei, Huawei Shen,\\nand Xueqi Cheng. 2024a. Retrieve only when it\\nneeds: Adaptive retrieval augmentation for hallucina-\\ntion mitigation in large language models. Preprint,\\narXiv:2402.10612.\\nMuhe Ding, Yang Ma, Pengda Qin, Jianlong Wu, Yuhong\\nLi, and Liqiang Nie. 2024b. Ra-blip: Multimodal\\nadaptive retrieval-augmented bootstrapping language-\\nimage pre-training. Preprint, arXiv:2410.14154.\\nYihao Ding, Kaixuan Ren, Jiabin Huang, Siwen Luo, and\\nSoyeon Caren Han. 2024c. Mmvqa: A comprehen-\\nsive dataset for investigating multipage multimodal\\ninformation retrieval in pdf-based visual question\\nanswering. In Proceedings of the Thirty-Third Inter-\\nnational Joint Conference on Artificial Intelligence,\\nIJCAI, pages 3–9.\\nYihao Ding, Kaixuan Ren, Jiabin Huang, Siwen Luo, and\\nSoyeon Caren Han. 2024d. Pdf-mvqa: A dataset for\\nmultimodal information retrieval in pdf-based visual\\nquestion answering. Preprint, arXiv:2404.12720.\\nJialin Dong, Bahare Fatemi, Bryan Perozzi, Lin F. Yang,\\nand Anton Tsitsulin. 2024a. Don’t forget to connect!\\nimproving rag with graph-based reranking. Preprint,\\narXiv:2405.18414.\\nXingning Dong, Zipeng Feng, Chunluan Zhou, Xuzheng\\nYu, Ming Yang, and Qingpei Guo. 2024b. M2-raap: A\\nmulti-modal recipe for advancing adaptation-based pre-\\ntraining towards effective and efficient zero-shot video-\\ntext retrieval. In Proceedings of the 47th International\\nACM SIGIR Conference on Research and Development\\nin Information Retrieval, SIGIR ’24, page 2156–2166,\\nNew York, NY , USA. Association for Computing\\nMachinery.\\nKonstantinos Drossos, Samuel Lipping, and Tuomas\\nVirtanen. 2020. Clotho: An audio captioning dataset.\\nIn ICASSP 2020-2020 IEEE International Conference\\non Acoustics, Speech and Signal Processing (ICASSP),\\npages 736–740. IEEE.\\nYang Du, Yuqi Liu, and Qin Jin. 2024. Reversed in\\ntime: A novel temporal-emphasized benchmark for\\ncross-modal video-text retrieval. In Proceedings of the\\n32nd ACM International Conference on Multimedia,\\nMM ’24, page 5260–5269, New York, NY , USA.\\nAssociation for Computing Machinery.\\nDesmond Elliott, Stella Frank, Khalil Sima’an, and Lucia\\nSpecia. 2016. Multi30k: Multilingual english-german\\nimage descriptions. Proceedings of the 5th Workshop\\non Vision and Language, pages 70–74.\\nAngela Fan, Claire Gardent, Chloé Braud, and Antoine\\nBordes. 2019. Eli5: Long form question answer-\\ning. Proceedings of the 57th Annual Meeting of\\nthe Association for Computational Linguistics, pages\\n3558–3567.\\nManuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani,\\nGautier Viaud, Céline Hudelot, and Pierre Colombo.\\n2024. Colpali: Efficient document retrieval with\\nvision language models. Preprint, arXiv:2407.01449.\\nChun-Mei Feng, Yang Bai, Tao Luo, Zhen Li, Salman\\nKhan, Wangmeng Zuo, Xinxing Xu, Rick Siow Mong\\nGoh, and Yong Liu. 2023. Vqa4cir: Boosting com-\\nposed image retrieval with visual question answering.\\nPreprint, arXiv:2312.12273.\\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,\\nJinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo,\\nMeng Wang, and Haofen Wang. 2023. Retrieval-\\naugmented generation for large language models: A\\nsurvey. ArXiv, abs/2312.10997.\\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman,\\nAren Jansen, Wade Lawrence, et al. 2017. Audioset:\\nAn ontology and human-labeled dataset for audio\\nevents. Proceedings of the IEEE International Con-\\nference on Acoustics, Speech and Signal Processing\\n(ICASSP), pages 776–780.\\nSreyan Ghosh, Sonal Kumar, Chandra Kiran\\nReddy Evuru, Ramani Duraiswami, and Dinesh\\nManocha. 2024. Recap: Retrieval-augmented\\naudio captioning. In ICASSP 2024 - 2024 IEEE\\nInternational Conference on Acoustics, Speech and\\nSignal Processing (ICASSP), pages 1161–1165.\\nSilvio Giancola, Mohieddine Amine, Tarek Dghaily, and\\nBernard Ghanem. 2018. Soccernet: A scalable dataset\\nfor action spotting in soccer videos. In Proceedings of\\nthe IEEE conference on computer vision and pattern\\nrecognition workshops, pages 1711–1721.\\n13'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Mohammad Mahdi Abootorabi; Amirhosein Zobeiri; Mahdi Dehghani; Mohammadali Mohammadkhani; Bardia Mohammadi; Omid Ghahroodi; Mahdieh Soleymani Baghshah; Ehsaneddin Asgari', 'doi': 'https://doi.org/10.48550/arXiv.2502.08826', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2502.08826v3', 'source': '/content/Multimodal_RAG.pdf', 'total_pages': 34, 'page': 13, 'page_label': '14'}, page_content='Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\\nBatra, and Devi Parikh. 2017a. Making the V in VQA\\nmatter: Elevating the role of image understanding\\nin Visual Question Answering. In Conference on\\nComputer Vision and Pattern Recognition (CVPR).\\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\\nBatra, and Devi Parikh. 2017b. Making the v in vqa\\nmatter: Elevating the role of image understanding\\nin visual question answering. In Proceedings of\\nthe IEEE conference on computer vision and pattern\\nrecognition, pages 6904–6913.\\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,\\nAbhinav Pandey, Abhishek Kadian, Ahmad Al-\\nDahle, Aiesha Letman, Akhil Mathur, Alan Schelten,\\nAlex Vaughan, Amy Yang, Angela Fan, Anirudh\\nGoyal, Anthony Hartshorn, Aobo Yang, Archi Mi-\\ntra, Archie Sravankumar, Artem Korenev, Arthur\\nHinsvark, Arun Rao, Aston Zhang, Aurelien Ro-\\ndriguez, Austen Gregerson, Ava Spataru, Baptiste\\nRoziere, Bethany Biron, Binh Tang, Bobbie Chern,\\nCharlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris\\nMarra, Chris McConnell, Christian Keller, Christophe\\nTouret, Chunyang Wu, Corinne Wong, Cristian Canton\\nFerrer, Cyrus Nikolaidis, Damien Allonsius, Daniel\\nSong, Danielle Pintz, Danny Livshits, Danny Wy-\\natt, David Esiobu, Dhruv Choudhary, Dhruv Maha-\\njan, Diego Garcia-Olano, Diego Perino, Dieuwke\\nHupkes, Egor Lakomkin, Ehab AlBadawy, Elina\\nLobanova, Emily Dinan, Eric Michael Smith, and\\net al. 2024. The llama 3 herd of models. Preprint,\\narXiv:2407.21783.\\nKristen Grauman, Andrew Westbury, Eugene Byrne,\\nZachary Chavis, Antonino Furnari, Rohit Girdhar,\\nJackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu,\\net al. 2022. Ego4d: Around the world in 3,000 hours\\nof egocentric video. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recogni-\\ntion, pages 18995–19012.\\nRuiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David\\nSimcha, Felix Chern, and Sanjiv Kumar. 2020. Accel-\\nerating large-scale inference with anisotropic vector\\nquantization. In International Conference on Machine\\nLearning, pages 3887–3896. PMLR.\\nHyeonjeong Ha, Qiusi Zhan, Jeonghwan Kim, Dimitrios\\nBralios, Saikrishna Sanniboina, Nanyun Peng, Kai-\\nWei Chang, Daniel Kang, and Heng Ji. 2025. Mm-\\npoisonrag: Disrupting multimodal rag with local and\\nglobal poisoning attacks. Preprint, arXiv:2502.17832.\\nWinston Haynes. 2013. Bonferroni Correction, pages\\n154–154. Springer New York, New York, NY .\\nBo He, Hengduo Li, Young Kyun Jang, Menglin Jia,\\nXuefei Cao, Ashish Shah, Abhinav Shrivastava, and\\nSer-Nam Lim. 2024. Ma-lmm: Memory-augmented\\nlarge multimodal model for long-term video under-\\nstanding. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition,\\npages 13504–13514.\\nKonstantin Hemker, Nikola Simidjievski, and Mateja\\nJamnik. 2024. HEALNet: Multimodal fusion for\\nheterogeneous biomedical data. In The Thirty-eighth\\nAnnual Conference on Neural Information Processing\\nSystems.\\nJack Hessel, Ari Holtzman, Maxwell Forbes, Ronan\\nLe Bras, and Yejin Choi. 2021. CLIPScore: A\\nreference-free evaluation metric for image captioning.\\nIn Proceedings of the 2021 Conference on Empirical\\nMethods in Natural Language Processing, pages 7514–\\n7528, Online and Punta Cana, Dominican Republic.\\nAssociation for Computational Linguistics.\\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner,\\nBernhard Nessler, and Sepp Hochreiter. 2017. Gans\\ntrained by a two time-scale update rule converge\\nto a local nash equilibrium. Advances in neural\\ninformation processing systems, 30.\\nAnwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang\\nZhang, Bo Zhang, Ji Zhang, Qin Jin, Fei Huang, and\\nJingren Zhou. 2024a. mPLUG-DocOwl 1.5: Unified\\nstructure learning for OCR-free document understand-\\ning. In Findings of the Association for Computational\\nLinguistics: EMNLP 2024 , pages 3096–3120, Mi-\\nami, Florida, USA. Association for Computational\\nLinguistics.\\nAnwen Hu, Haiyang Xu, Liang Zhang, Jiabo Ye, Ming\\nYan, Ji Zhang, Qin Jin, Fei Huang, and Jingren Zhou.\\n2024b. mplug-docowl2: High-resolution compress-\\ning for ocr-free multi-page document understanding.\\nPreprint, arXiv:2409.03420.\\nWenbo Hu, Jia-Chen Gu, Zi-Yi Dou, Mohsen Fayyaz,\\nPan Lu, Kai-Wei Chang, and Nanyun Peng. 2024c.\\nMrag-bench: Vision-centric evaluation for retrieval-\\naugmented multimodal models. arXiv preprint\\narXiv:2410.08182.\\nZiniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei\\nChang, Yizhou Sun, Cordelia Schmid, David A. Ross,\\nand Alireza Fathi. 2023. Reveal: Retrieval-augmented\\nvisual-language pre-training with multi-source mul-\\ntimodal knowledge memory. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pat-\\ntern Recognition (CVPR), pages 23369–23379.\\nLei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,\\nZhangyin Feng, Haotian Wang, Qianglong Chen, Wei-\\nhua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu.\\n2024. A survey on hallucination in large language\\nmodels: Principles, taxonomy, challenges, and open\\nquestions. ACM Trans. Inf. Syst. Just Accepted.\\nLiting Huang, Zhihao Zhang, Yiran Zhang, Xiyue Zhou,\\nand Shoujin Wang. 2025. Ru-ai: A large multimodal\\ndataset for machine-generated content detection. In\\nCompanion Proceedings of the ACM on Web Confer-\\nence 2025, pages 733–736.\\nWisdom Oluchi Ikezogwo, Mehmet Saygin Sey-\\nfioglu, Fatemeh Ghezloo, Dylan Stefan Chan Geva,\\nFatwir Sheikh Mohammed, Pavan Kumar Anand, Ran-\\njay Krishna, and Linda Shapiro. 2023. Quilt-1m: One\\n14'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Mohammad Mahdi Abootorabi; Amirhosein Zobeiri; Mahdi Dehghani; Mohammadali Mohammadkhani; Bardia Mohammadi; Omid Ghahroodi; Mahdieh Soleymani Baghshah; Ehsaneddin Asgari', 'doi': 'https://doi.org/10.48550/arXiv.2502.08826', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2502.08826v3', 'source': '/content/Multimodal_RAG.pdf', 'total_pages': 34, 'page': 14, 'page_label': '15'}, page_content='million image-text pairs for histopathology. arXiv\\npreprint arXiv:2306.11207.\\nJeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu,\\nSilviana Ciurea-Ilcus, Chris Chute, Henrik Marklund,\\nBehzad Haghgoo, Robyn Ball, Katie Shpanskaya,\\net al. 2019. Chexpert: A large chest radiograph\\ndataset with uncertainty labels and expert comparison.\\nIn Proceedings of the AAAI conference on artificial\\nintelligence, volume 33, pages 590–597.\\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebas-\\ntian Riedel, Piotr Bojanowski, Armand Joulin, and\\nEdouard Grave. 2022. Unsupervised dense infor-\\nmation retrieval with contrastive learning. Preprint,\\narXiv:2112.09118.\\nYoung Kyun Jang, Donghyun Kim, Zihang Meng, Dat\\nHuynh, and Ser-Nam Lim. 2024. Visual delta genera-\\ntor with large multi-modal models for semi-supervised\\ncomposed image retrieval. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pat-\\ntern Recognition, pages 16805–16814.\\nSoyeong Jeong, Kangsan Kim, Jinheon Baek, and\\nSung Ju Hwang. 2025. Videorag: Retrieval-\\naugmented generation over video corpus. Preprint,\\narXiv:2501.05874.\\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\\nParekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen\\nLi, and Tom Duerig. 2021. Scaling up visual and\\nvision-language representation learning with noisy text\\nsupervision. In Proceedings of the 38th International\\nConference on Machine Learning , volume 139 of\\nProceedings of Machine Learning Research, pages\\n4904–4916. PMLR.\\nMenglin Jia, Mengyun Shi, Mikhail Sirotenko, Yin Cui,\\nClaire Cardie, Bharath Hariharan, Hartwig Adam,\\nand Serge Belongie. 2020. Fashionpedia: Ontology,\\nsegmentation, and an attribute localization dataset.\\nPreprint, arXiv:2004.12276.\\nPu Jian, Donglei Yu, and Jiajun Zhang. 2024. Large lan-\\nguage models know what is key visual entity: An LLM-\\nassisted multimodal retrieval for VQA. In Proceed-\\nings of the 2024 Conference on Empirical Methods in\\nNatural Language Processing, pages 10939–10956,\\nMiami, Florida, USA. Association for Computational\\nLinguistics.\\nChaoya Jiang, Haiyang Xu, Mengfan Dong, Jiaxing Chen,\\nWei Ye, Ming Yan, Qinghao Ye, Ji Zhang, Fei Huang,\\nand Shikun Zhang. 2024. Hallucination augmented\\ncontrastive learning for multimodal large language\\nmodel. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition, pages\\n27036–27046.\\nXi Jiang, Jian Li, Hanqiu Deng, Yong Liu, Bin-Bin Gao,\\nYifeng Zhou, Jialin Li, Chengjie Wang, and Feng\\nZheng. 2025. Mmad: A comprehensive benchmark\\nfor multimodal large language models in industrial\\nanomaly detection. Preprint, arXiv:2410.09453.\\nXiaojie Jin, Bowen Zhang, Weibo Gong, Kai Xu, Xue-\\nqing Deng, Peng Wang, Zhao Zhang, Xiaohui Shen,\\nand Jiashi Feng. 2024. Mv-adapter: Multimodal\\nvideo transfer learning for video text retrieval. In Pro-\\nceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition, pages 27144–27153.\\nAlistair EW Johnson, Tom J Pollard, Nathaniel R Green-\\nbaum, Matthew P Lungren, Chih-ying Deng, Yifan\\nPeng, et al. 2019. Mimic-cxr-jpg, a large publicly\\navailable database of labeled chest radiographs. arXiv\\npreprint arXiv:1901.07042.\\nAlistair EW Johnson, Tom J Pollard, Lu Shen, H Lehman\\nLi-wei, Mengling Feng, et al. 2016. Mimic-iii, a\\nfreely accessible critical care database. Scientific\\nData, 3:160035.\\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\\nZettlemoyer. 2017. TriviaQA: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In Proceedings of the 55th Annual Meeting of\\nthe Association for Computational Linguistics (Vol-\\nume 1: Long Papers), pages 1601–1611, Vancouver,\\nCanada. Association for Computational Linguistics.\\nPankaj Joshi, Aditya Gupta, Pankaj Kumar, and Manas\\nSisodia. 2024. Robust multi model rag pipeline for\\ndocuments containing text, table & images. In 2024\\n3rd International Conference on Applied Artificial\\nIntelligence and Computing (ICAAIC), pages 993–999.\\nIEEE.\\nKushal Kafle, Brian Price, Scott Cohen, and Christopher\\nKanan. 2018. Dvqa: Understanding data visual-\\nizations via question answering. In Proceedings of\\nthe IEEE conference on computer vision and pattern\\nrecognition, pages 5648–5656.\\nMahesh Kandhare and Thibault Gisselbrecht. 2024.\\nAn empirical comparison of video frame sampling\\nmethods for multi-modal rag retrieval. Preprint,\\narXiv:2408.03340.\\nYasser Khalafaoui, Martino Lovisetto, Basarab Matei,\\nand Nistor Grozavu. 2024. Cadmr: Cross-attention\\nand disentangled learning for multimodal recom-\\nmender systems. Preprint, arXiv:2412.02295.\\nMohammed Abdul Khaliq, Paul Yu-Chun Chang,\\nMingyang Ma, Bernhard Pflugfelder, and Filip Mileti´c.\\n2024. RAGAR, your falsehood radar: RAG-\\naugmented reasoning for political fact-checking using\\nmultimodal large language models. In Proceedings of\\nthe Seventh Fact Extraction and VERification Work-\\nshop (FEVER), pages 280–296, Miami, Florida, USA.\\nAssociation for Computational Linguistics.\\nFaizan Farooq Khan, Jun Chen, Youssef Mohamed, Chun-\\nMei Feng, and Mohamed Elhoseiny. 2025. Vr-\\nrag: Open-vocabulary species recognition with rag-\\nassisted large multi-modal models. arXiv preprint\\narXiv:2505.05635.\\nOmar Khattab and Matei Zaharia. 2020. Colbert: Effi-\\ncient and effective passage search via contextualized\\n15'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Mohammad Mahdi Abootorabi; Amirhosein Zobeiri; Mahdi Dehghani; Mohammadali Mohammadkhani; Bardia Mohammadi; Omid Ghahroodi; Mahdieh Soleymani Baghshah; Ehsaneddin Asgari', 'doi': 'https://doi.org/10.48550/arXiv.2502.08826', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2502.08826v3', 'source': '/content/Multimodal_RAG.pdf', 'total_pages': 34, 'page': 15, 'page_label': '16'}, page_content='late interaction over bert. In Proceedings of the 43rd\\nInternational ACM SIGIR Conference on Research\\nand Development in Information Retrieval, SIGIR ’20,\\npage 39–48, New York, NY , USA. Association for\\nComputing Machinery.\\nKevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and\\nMatthew Sharifi. 2019. Fréchet audio distance: A\\nmetric for evaluating music enhancement algorithms.\\nPreprint, arXiv:1812.08466.\\nChris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,\\nand Gunhee Kim. 2019. Audiocaps: Generating\\ncaptions for audios in the wild. In Proceedings of the\\n2019 Conference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies, Volume 1 (Long and Short\\nPapers), pages 119–132, Minneapolis, Minnesota.\\nAssociation for Computational Linguistics.\\nJinkyu Kim, Anna Rohrbach, Trevor Darrell, John Canny,\\nand Zeynep Akata. 2018. Textual explanations for\\nself-driving vehicles. Proceedings of the European\\nConference on Computer Vision (ECCV).\\nMinkuk Kim, Hyeon Bae Kim, Jinyoung Moon, Jinwoo\\nChoi, and Seong Tae Kim. 2024. Do you remember?\\ndense video captioning with cross-modal memory\\nretrieval. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition, pages\\n13894–13904.\\nRanjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and\\nJuan Carlos Niebles. 2017. Dense-captioning events\\nin videos. In Proceedings of the IEEE International\\nConference on Computer Vision (ICCV).\\nYogesh Kumar and Pekka Marttinen. 2024. Improving\\nmedical multi-modal contrastive learning with expert\\nannotations. In Computer Vision – ECCV 2024:\\n18th European Conference, Milan, Italy, September\\n29–October 4, 2024, Proceedings, Part XX , page\\n468–486, Berlin, Heidelberg. Springer-Verlag.\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield,\\nMichael Collins, Ankur Parikh, Chris Alberti, Danielle\\nEpstein, Illia Polosukhin, Jacob Devlin, Kenton Lee,\\net al. 2019. Natural questions: a benchmark for\\nquestion answering research. Transactions of the\\nAssociation for Computational Linguistics, 7:453–466.\\nAritra Kumar Lahiri and Qinmin Vivian Hu. 2024.\\nAlzheimerrag: Multimodal retrieval augmented genera-\\ntion for pubmed articles. Preprint, arXiv:2412.16701.\\nZhibin Lan, Liqiang Niu, Fandong Meng, Jie Zhou, and\\nJinsong Su. 2025. Llave: Large language and vision\\nembedding models with hardness-weighted contrastive\\nlearning. arXiv preprint arXiv:2503.04812.\\nJian Lang, Zhangtao Cheng, Ting Zhong, and Fan Zhou.\\n2025. Retrieval-augmented dynamic prompt tuning\\nfor incomplete multimodal learning. arXiv preprint\\narXiv:2501.01120.\\nMyeonghwa Lee, Seonho An, and Min-Soo Kim. 2024.\\nPlanRAG: A plan-then-retrieval augmented genera-\\ntion for generative large language models as decision\\nmakers. In Proceedings of the 2024 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Human Language Tech-\\nnologies (Volume 1: Long Papers), pages 6537–6555,\\nMexico City, Mexico. Association for Computational\\nLinguistics.\\nPaul Lerner, Olivier Ferret, Camille Guinaudeau, Hervé\\nLe Borgne, Romaric Besançon, Jose G Moreno, and\\nJesús Lovón Melgarejo. 2022. ViQuAE, a dataset\\nfor knowledge-based visual question answering about\\nnamed entities. In Proceedings of The 45th Inter-\\nnational ACM SIGIR Conference on Research and\\nDevelopment in Information Retrieval, SIGIR’22, New\\nYork, NY , USA. Association for Computing Machin-\\nery.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Heinrich\\nKüttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel,\\net al. 2020. Retrieval-augmented generation for\\nknowledge-intensive nlp tasks. Advances in Neural\\nInformation Processing Systems, 33:9459–9474.\\nPatrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale Min-\\nervini, Heinrich Küttler, Aleksandra Piktus, Pontus\\nStenetorp, and Sebastian Riedel. 2021. Paq: 65\\nmillion probably-asked questions and what you can\\ndo with them. Transactions of the Association for\\nComputational Linguistics, 9:1098–1115.\\nBo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\\nFanyi Pu, Joshua Adrian Cahyono, Jingkang Yang,\\nChunyuan Li, and Ziwei Liu. 2025a. Otter: A\\nmulti-modal model with in-context instruction tuning.\\nIEEE Transactions on Pattern Analysis and Machine\\nIntelligence.\\nBohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao\\nGe, and Ying Shan. 2023a. Seed-bench: Benchmark-\\ning multimodal llms with generative comprehension.\\nPreprint, arXiv:2307.16125.\\nChia-Hsuan Li, Szu-Lin Wu, Chi-Liang Liu, and Hung-yi\\nLee. 2018. Spoken squad: A study of mitigating\\nthe impact of speech recognition errors on listening\\ncomprehension. arXiv preprint arXiv:1804.00320.\\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven\\nHoi. 2023b. Blip-2: bootstrapping language-image\\npre-training with frozen image encoders and large\\nlanguage models. In Proceedings of the 40th Interna-\\ntional Conference on Machine Learning, ICML’23.\\nJMLR.org.\\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven\\nHoi. 2022. Blip: Bootstrapping language-image pre-\\ntraining for unified vision-language understanding and\\ngeneration. In International conference on machine\\nlearning, pages 12888–12900. PMLR.\\nMuquan Li, Dongyang Zhang, Qiang Dong, Xiurui Xie,\\nand Ke Qin. 2024a. Adaptive dataset quantization.\\nPreprint, arXiv:2412.16895.\\n16'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Mohammad Mahdi Abootorabi; Amirhosein Zobeiri; Mahdi Dehghani; Mohammadali Mohammadkhani; Bardia Mohammadi; Omid Ghahroodi; Mahdieh Soleymani Baghshah; Ehsaneddin Asgari', 'doi': 'https://doi.org/10.48550/arXiv.2502.08826', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2502.08826v3', 'source': '/content/Multimodal_RAG.pdf', 'total_pages': 34, 'page': 16, 'page_label': '17'}, page_content='Qingyun Li, Zhe Chen, Weiyun Wang, Wenhai Wang,\\nShenglong Ye, Zhenjiang Jin, Guanzhou Chen, Yinan\\nHe, Zhangwei Gao, Erfei Cui, et al. 2025b. Omnicor-\\npus: A unified multimodal corpus of 10 billion-level\\nimages interleaved with text. In ICLR.\\nShaojun Li, Hengchao Shang, Daimeng Wei, Jiaxin\\nGuo, Zongyao Li, Xianghui He, Min Zhang, and Hao\\nYang. 2024b. La-rag:enhancing llm-based asr accu-\\nracy with retrieval-augmented generation. Preprint,\\narXiv:2409.08597.\\nXiquan Li, Wenxi Chen, Ziyang Ma, Xuenan Xu, Yuzhe\\nLiang, Zhisheng Zheng, Qiuqiang Kong, and Xie Chen.\\n2025c. Drcap: Decoding clap latents with retrieval-\\naugmented generation for zero-shot audio captioning.\\nIn ICASSP 2025 - 2025 IEEE International Conference\\non Acoustics, Speech and Signal Processing (ICASSP),\\npages 1–5.\\nXirong Li, Chaoxi Xu, Xiaoxu Wang, Weiyu Lan,\\nZhengxiong Jia, Gang Yang, and Jieping Xu. 2019.\\nCoco-cn for cross-lingual image tagging, captioning\\nand retrieval.\\nYanda Li, Chi Zhang, Wanqi Yang, Bin Fu, Pei Cheng,\\nXin Chen, Ling Chen, and Yunchao Wei. 2024c.\\nAppagent v2: Advanced agent for flexible mobile\\ninteractions. Preprint, arXiv:2408.11824.\\nYangning Li, Yinghui Li, Xingyu Wang, Yong Jiang,\\nZhen Zhang, Xinran Zheng, Hui Wang, Hai-Tao\\nZheng, Philip S Yu, Fei Huang, et al. 2024d. Bench-\\nmarking multimodal retrieval augmented generation\\nwith dynamic vqa dataset and self-adaptive planning\\nagent. arXiv preprint arXiv:2411.02937.\\nZehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long,\\nPengjun Xie, and Meishan Zhang. 2023c. Towards\\ngeneral text embeddings with multi-stage contrastive\\nlearning. Preprint, arXiv:2308.03281.\\nChin-Yew Lin. 2004. ROUGE: A package for automatic\\nevaluation of summaries. In Text Summarization\\nBranches Out, pages 74–81, Barcelona, Spain. Associ-\\nation for Computational Linguistics.\\nSheng-Chieh Lin, Chankyu Lee, Mohammad Shoeybi,\\nJimmy Lin, Bryan Catanzaro, and Wei Ping. 2024a.\\nMm-embed: Universal multimodal retrieval with mul-\\ntimodal llms. Preprint, arXiv:2411.02571.\\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\\nand C Lawrence Zitnick. 2014. Microsoft coco:\\nCommon objects in context. European Conference on\\nComputer Vision, pages 740–755.\\nWeizhe Lin, Jingbiao Mei, Jinghong Chen, and Bill Byrne.\\n2024b. PreFLMR: Scaling up fine-grained late-\\ninteraction multi-modal retrievers. In Proceedings\\nof the 62nd Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers),\\npages 5294–5316, Bangkok, Thailand. Association\\nfor Computational Linguistics.\\nDi Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhen-\\nhua Han, Qianxi Zhang, Qi Chen, Chengruidong\\nZhang, Bailu Ding, Kai Zhang, Chen Chen, Fan Yang,\\nYuqing Yang, and Lili Qiu. 2024a. Retrievalattention:\\nAccelerating long-context llm inference via vector\\nretrieval. Preprint, arXiv:2409.10516.\\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\\nLee. 2023a. Visual instruction tuning. Advances\\nin neural information processing systems, 36:34892–\\n34916.\\nPei Liu, Xin Liu, Ruoyu Yao, Junming Liu, Siyuan Meng,\\nDing Wang, and Jun Ma. 2025a. Hm-rag: Hierar-\\nchical multi-agent multimodal retrieval augmented\\ngeneration. Preprint, arXiv:2504.12330.\\nSiqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama, and\\nKevin Murphy. 2017. Improved image captioning via\\npolicy gradient optimization of spider. In Proceedings\\nof the IEEE international conference on computer\\nvision, pages 873–881.\\nXingzu Liu, Mingbang Wang, Songhang Deng, Xinyue\\nPeng, Yanming Liu, Ruilin Nong, David Williams,\\nand Jiyuan Li. 2024b. Iterative retrieval augmentation\\nfor multi-modal knowledge integration and generation.\\nYanming Liu, Xinyue Peng, Xuhong Zhang, Weihao\\nLiu, Jianwei Yin, Jiannan Cao, and Tianyu Du. 2024c.\\nRA-ISF: Learning to answer and understand from\\nretrieval augmentation via iterative self-feedback. In\\nFindings of the Association for Computational Linguis-\\ntics: ACL 2024, pages 4730–4749, Bangkok, Thailand.\\nAssociation for Computational Linguistics.\\nYinuo Liu, Zenghui Yuan, Guiyao Tie, Jiawen Shi, Pan\\nZhou, Lichao Sun, and Neil Zhenqiang Gong. 2025b.\\nPoisoned-mrag: Knowledge poisoning attacks to multi-\\nmodal retrieval augmented generation. arXiv preprint\\narXiv:2503.06254.\\nYuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li,\\nSongyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi\\nWang, Conghui He, Ziwei Liu, et al. 2025c. Mm-\\nbench: Is your multi-modal model an all-around\\nplayer? In European conference on computer vi-\\nsion, pages 216–233. Springer.\\nZhenghao Liu, Chenyan Xiong, Yuanhuiyi Lv, Zhiyuan\\nLiu, and Ge Yu. 2023b. Universal vision-language\\ndense retrieval: Learning a unified representation\\nspace for multi-modal retrieval. In Proceedings of\\nICLR.\\nZhenghao Liu, Xingsheng Zhu, Tianshuo Zhou, Xinyi\\nZhang, Xiaoyuan Yi, Yukun Yan, Yu Gu, Ge Yu,\\nand Maosong Sun. 2025d. Benchmarking retrieval-\\naugmented generation in multi-modal contexts. arXiv\\npreprint arXiv:2502.17297.\\nZheyuan Liu, Cristian Rodriguez-Opazo, Damien Teney,\\nand Stephen Gould. 2021. Image retrieval on real-life\\nimages with pre-trained vision-and-language models.\\nIn Proceedings of the IEEE/CVF International Confer-\\nence on Computer Vision (ICCV), pages 2125–2134.\\n17'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Mohammad Mahdi Abootorabi; Amirhosein Zobeiri; Mahdi Dehghani; Mohammadali Mohammadkhani; Bardia Mohammadi; Omid Ghahroodi; Mahdieh Soleymani Baghshah; Ehsaneddin Asgari', 'doi': 'https://doi.org/10.48550/arXiv.2502.08826', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2502.08826v3', 'source': '/content/Multimodal_RAG.pdf', 'total_pages': 34, 'page': 17, 'page_label': '18'}, page_content='Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and\\nXiaoou Tang. 2016. Deepfashion: Powering robust\\nclothes recognition and retrieval with rich annotations.\\nIn Proceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition (CVPR).\\nPan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-\\nWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\\nClark, and Ashwin Kalyan. 2022. Learn to explain:\\nMultimodal reasoning via thought chains for science\\nquestion answering. In The 36th Conference on\\nNeural Information Processing Systems (NeurIPS).\\nShiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua\\nLuo, Kaifu Zhang, and Han-Jia Ye. 2024. Ovis:\\nStructural embedding alignment for multimodal large\\nlanguage model. Preprint, arXiv:2405.20797.\\nYang Luo, Zangwei Zheng, Zirui Zhu, and Yang You.\\n2024a. How does the textual information affect the re-\\ntrieval of multimodal in-context learning? In Proceed-\\nings of the 2024 Conference on Empirical Methods\\nin Natural Language Processing, pages 5321–5335,\\nMiami, Florida, USA. Association for Computational\\nLinguistics.\\nYongdong Luo, Xiawu Zheng, Xiao Yang, Guilin Li,\\nHaojia Lin, Jinfa Huang, Jiayi Ji, Fei Chao, Jiebo\\nLuo, and Rongrong Ji. 2024b. Video-rag: Visually-\\naligned retrieval-augmented long video comprehen-\\nsion. Preprint, arXiv:2411.13093.\\nXueguang Ma, Sheng-Chieh Lin, Minghan Li, Wenhu\\nChen, and Jimmy Lin. 2024a. Unifying multimodal re-\\ntrieval via document screenshot embedding. Preprint,\\narXiv:2406.11251.\\nXueguang Ma, Shengyao Zhuang, Bevan Koopman,\\nGuido Zuccon, Wenhu Chen, and Jimmy Lin. 2024b.\\nVisa: Retrieval augmented generation with visual\\nsource attribution. Preprint, arXiv:2412.14457.\\nYubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen,\\nYizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan\\nMa, Xiaoyi Dong, Pan Zhang, Liangming Pan, Yu-\\nGang Jiang, Jiaqi Wang, Yixin Cao, and Aixin Sun.\\n2024c. Mmlongbench-doc: Benchmarking long-\\ncontext document understanding with visualizations.\\nPreprint, arXiv:2407.01523.\\nZi-Ao Ma, Tian Lan, Rong-Cheng Tu, Yong Hu, Heyan\\nHuang, and Xian-Ling Mao. 2024d. Multi-modal\\nretrieval augmented multi-modal generation: A bench-\\nmark, evaluate metrics and strong baselines. Preprint,\\narXiv:2411.16365.\\nZiyu Ma, Chenhui Gou, Hengcan Shi, Bin Sun, Shutao Li,\\nHamid Rezatofighi, and Jianfei Cai. 2024e. Drvideo:\\nDocument retrieval based long video understanding.\\nPreprint, arXiv:2406.12846.\\nZhiming Mao, Haoli Bai, Lu Hou, Lifeng Shang, Xin\\nJiang, Qun Liu, and Kam-Fai Wong. 2024. Visu-\\nally guided generative text-layout pre-training for\\ndocument intelligence. In Proceedings of the 2024\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies (Volume 1: Long Papers) ,\\npages 4713–4730, Mexico City, Mexico. Association\\nfor Computational Linguistics.\\nKenneth Marino, Xinlei Chen, Abhinav Gupta, Marcus\\nRohrbach, and Devi Parikh. 2019. Ok-vqa: A visual\\nquestion answering benchmark requiring external\\nknowledge. Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition (CVPR),\\npages 3195–3204.\\nAhmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty,\\nand Enamul Hoque. 2022. Chartqa: A benchmark\\nfor question answering about charts with visual and\\nlogical reasoning. arXiv preprint arXiv:2203.10244.\\nMinesh Mathew, Dimosthenis Karatzas, and CV Jawahar.\\n2021. Docvqa: A dataset for vqa on document images.\\nIn Proceedings of the IEEE/CVF winter conference on\\napplications of computer vision, pages 2200–2209.\\nXinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang Kong,\\nTom Ko, Chengqi Zhao, Mark D Plumbley, Yuexian\\nZou, and Wenwu Wang. 2024. Wavcaps: A chatgpt-\\nassisted weakly-labelled audio captioning dataset for\\naudio-language multimodal research. IEEE/ACM\\nTransactions on Audio, Speech, and Language Pro-\\ncessing.\\nAntoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,\\nMakarand Tapaswi, Ivan Laptev, and Josef Sivic. 2019.\\nHowto100m: Learning a text-video embedding by\\nwatching hundred million narrated video clips. In Pro-\\nceedings of the IEEE/CVF International Conference\\non Computer Vision (ICCV).\\nDo June Min, Karel Mundnich, Andy Lapastora, Erfan\\nSoltanmohammadi, Srikanth Ronanki, and Kyu Han.\\n2025. Speech retrieval-augmented generation without\\nautomatic speech recognition. In ICASSP 2025 - 2025\\nIEEE International Conference on Acoustics, Speech\\nand Signal Processing (ICASSP), pages 1–5.\\nMatin Mortaheb, Mohammad A. Amir Khojastepour, Sri-\\nmat T. Chakradhar, and Sennur Ulukus. 2025a. Rag-\\ncheck: Evaluating multimodal retrieval augmented\\ngeneration performance. Preprint, arXiv:2501.03995.\\nMatin Mortaheb, Mohammad A. Amir Khojastepour,\\nSrimat T. Chakradhar, and Sennur Ulukus. 2025b. Re-\\nranking the context for multimodal retrieval augmented\\ngeneration. Preprint, arXiv:2501.04695.\\nKepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhen-\\nheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and\\nYing Tai. 2024a. Openvid-1m: A large-scale high-\\nquality dataset for text-to-video generation. arXiv\\npreprint arXiv:2407.02371.\\nLinyong Nan, Weining Fang, Aylin Rasteh, Pouya Lahabi,\\nWeijin Zou, Yilun Zhao, and Arman Cohan. 2024b.\\nOMG-QA: Building open-domain multi-modal gener-\\native question answering systems. In Proceedings of\\nthe 2024 Conference on Empirical Methods in Nat-\\nural Language Processing: Industry Track , pages\\n18'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Mohammad Mahdi Abootorabi; Amirhosein Zobeiri; Mahdi Dehghani; Mohammadali Mohammadkhani; Bardia Mohammadi; Omid Ghahroodi; Mahdieh Soleymani Baghshah; Ehsaneddin Asgari', 'doi': 'https://doi.org/10.48550/arXiv.2502.08826', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2502.08826v3', 'source': '/content/Multimodal_RAG.pdf', 'total_pages': 34, 'page': 18, 'page_label': '19'}, page_content='1001–1015, Miami, Florida, US. Association for\\nComputational Linguistics.\\nNoor Nashid, Mifta Sintaha, and Ali Mesbah. 2023.\\nRetrieval-based prompt selection for code-related few-\\nshot learning. In 2023 IEEE/ACM 45th International\\nConference on Software Engineering (ICSE), pages\\n2450–2462.\\nHumza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad\\nSaqib, Saeed Anwar, Muhammad Usman, Naveed\\nAkhtar, Nick Barnes, and Ajmal Mian. 2024. A\\ncomprehensive overview of large language models.\\nPreprint, arXiv:2307.06435.\\nAhmad M Nazar, Abdulkadir Celik, Mohamed Y Selim,\\nAsmaa Abdallah, Daji Qiao, and Ahmed M Eltawil.\\n2024. Enwar: A rag-empowered multi-modal llm\\nframework for wireless environment perception. arXiv\\npreprint arXiv:2410.18104.\\nThong Nguyen, Mariya Hendriksen, Andrew Yates, and\\nMaarten de Rijke. 2024. Multimodal learned sparse\\nretrieval with probabilistic expansion control. In\\nAdvances in Information Retrieval, pages 448–464,\\nCham. Springer Nature Switzerland.\\nYulei Niu, Kaihua Tang, Hanwang Zhang, Zhiwu Lu,\\nXian-Sheng Hua, and Ji-Rong Wen. 2021. Counter-\\nfactual vqa: A cause-effect look at language bias. In\\nProceedings of the IEEE/CVF conference on computer\\nvision and pattern recognition, pages 12700–12710.\\nOpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,\\nLama Ahmad, Ilge Akkaya, Florencia Leoni Ale-\\nman, Diogo Almeida, Janko Altenschmidt, Sam Alt-\\nman, Shyamal Anadkat, Red Avila, Igor Babuschkin,\\nSuchir Balaji, Valerie Balcom, Paul Baltescu, Haiming\\nBao, Mohammad Bavarian, Jeff Belgum, Irwan Bello,\\nJake Berdine, Gabriel Bernadett-Shapiro, Christo-\\npher Berner, Lenny Bogdonoff, Oleg Boiko, Made-\\nlaine Boyd, Anna-Luisa Brakman, Greg Brockman,\\nTim Brooks, Miles Brundage, Kevin Button, Trevor\\nCai, Rosie Campbell, Andrew Cann, Brittany Carey,\\nChelsea Carlson, Rory Carmichael, Brooke Chan,\\nChe Chang, Fotis Chantzis, Derek Chen, Sully Chen,\\nRuby Chen, Jason Chen, Mark Chen, Ben Chess,\\nChester Cho, Casey Chu, Hyung Won Chung, Dave\\nCummings, Jeremiah Currier, Yunxing Dai, Cory De-\\ncareaux, Thomas Degry, and et al. 2024. Gpt-4\\ntechnical report. Preprint, arXiv:2303.08774.\\nMaxime Oquab, Timothée Darcet, Theo Moutakanni,\\nHuy V . V o, Marc Szafraniec, Vasil Khalidov, Pierre\\nFernandez, Daniel Haziza, Francisco Massa, Alaaeldin\\nEl-Nouby, Russell Howes, Po-Yao Huang, Hu Xu,\\nVasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike\\nRabbat, Mido Assran, Nicolas Ballas, Gabriel Syn-\\nnaeve, Ishan Misra, Herve Jegou, Julien Mairal,\\nPatrick Labatut, Armand Joulin, and Piotr Bojanowski.\\n2023. Dinov2: Learning robust visual features without\\nsupervision.\\nWeihua Ou, Yingjie Chen, Linqing Liang, Jianping\\nGou, Jiahao Xiong, Jiacheng Zhang, Lingge Lai,\\nand Lei Zhang. 2025. Cross-modal retrieval of\\nchest x-ray images and diagnostic reports based on\\nreport entity graph and dual attention: Cross-modal\\nretrieval of chest x-ray images and diagnostic reports...\\nMultimedia Syst., 31(1).\\nLinke Ouyang, Yuan Qu, Hongbin Zhou, Jiawei Zhu,\\nRui Zhang, Qunshu Lin, Bin Wang, Zhiyuan Zhao,\\nMan Jiang, Xiaomeng Zhao, Jin Shi, Fan Wu, Pei Chu,\\nMinghao Liu, Zhenxiang Li, Chao Xu, Bo Zhang,\\nBotian Shi, Zhongying Tu, and Conghui He. 2024.\\nOmnidocbench: Benchmarking diverse pdf document\\nparsing with comprehensive annotations. Preprint,\\narXiv:2412.07626.\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\\nSandhini Agarwal, Katarina Slama, Alex Ray, John\\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\\nMaddie Simens, Amanda Askell, Peter Welinder,\\nPaul F Christiano, Jan Leike, and Ryan Lowe. 2022.\\nTraining language models to follow instructions with\\nhuman feedback. In Advances in Neural Information\\nProcessing Systems, volume 35, pages 27730–27744.\\nCurran Associates, Inc.\\nArnold Overwijk, Chenyan Xiong, and Jamie Callan.\\n2022. Clueweb22: 10 billion web documents with rich\\ninformation. In Proceedings of the 45th International\\nACM SIGIR Conference on Research and Development\\nin Information Retrieval, SIGIR ’22, page 3360–3362,\\nNew York, NY , USA. Association for Computing\\nMachinery.\\nVassil Panayotov, Guoguo Chen, Daniel Povey, and San-\\njeev Khudanpur. 2015. Librispeech: An asr corpus\\nbased on public domain audio books. Proceedings\\nof the IEEE International Conference on Acoustics,\\nSpeech and Signal Processing (ICASSP), pages 5206–\\n5210.\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\\nJing Zhu. 2002. Bleu: a method for automatic\\nevaluation of machine translation. In Proceedings of\\nthe 40th Annual Meeting of the Association for Com-\\nputational Linguistics, pages 311–318, Philadelphia,\\nPennsylvania, USA. Association for Computational\\nLinguistics.\\nMd Rizwan Parvez, Wasi Uddin Ahmad, Saikat\\nChakraborty, Baishakhi Ray, and Kai-Wei Chang.\\n2021. Retrieval augmented code generation and\\nsummarization. In EMNLP-Findings.\\nJohn Pavlopoulos, Vasiliki Kougia, and Ion Androut-\\nsopoulos. 2019. A survey on biomedical image\\ncaptioning. In Proceedings of the Second Workshop\\non Shortcomings in Vision and Language, pages 26–36,\\nMinneapolis, Minnesota. Association for Computa-\\ntional Linguistics.\\nAbhirama Subramanyam Penamakuri, Manish Gupta,\\nMithun Das Gupta, and Anand Mishra. 2023. Answer\\nmining from a pool of images: Towards retrieval-based\\nvisual question answering. In IJCAI. ijcai.org.\\n19'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Mohammad Mahdi Abootorabi; Amirhosein Zobeiri; Mahdi Dehghani; Mohammadali Mohammadkhani; Bardia Mohammadi; Omid Ghahroodi; Mahdieh Soleymani Baghshah; Ehsaneddin Asgari', 'doi': 'https://doi.org/10.48550/arXiv.2502.08826', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2502.08826v3', 'source': '/content/Multimodal_RAG.pdf', 'total_pages': 34, 'page': 19, 'page_label': '20'}, page_content='Shraman Pramanick, Li Jing, Sayan Nag, Jiachen Zhu,\\nHardik Shah, Yann LeCun, and Rama Chellappa.\\n2023. V olta: Vision-language transformer with\\nweakly-supervised local-feature alignment. TMLR.\\nTyler Thomas Procko and Omar Ochoa. 2024. Graph\\nretrieval-augmented generation for large language\\nmodels: A survey. In 2024 Conference on AI, Science,\\nEngineering, and Technology (AIxSET), pages 166–\\n169.\\nYiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao,\\nSangwoo Cho, Xiaoyang Wang, Xuansheng Wu, Fei\\nLiu, Pengfei Liu, and Dong Yu. 2024. InFoBench:\\nEvaluating instruction following ability in large lan-\\nguage models. In Findings of the Association for\\nComputational Linguistics: ACL 2024, pages 13025–\\n13048, Bangkok, Thailand. Association for Computa-\\ntional Linguistics.\\nQwen, :, An Yang, Baosong Yang, Beichen Zhang,\\nBinyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li,\\nDayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian\\nYang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi\\nYang, Jingren Zhou, Junyang Lin, Kai Dang, Keming\\nLu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng\\nXue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tian-\\nhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren,\\nXuancheng Ren, Yang Fan, Yang Su, Yichang Zhang,\\nYu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and\\nZihan Qiu. 2025. Qwen2.5 technical report. Preprint,\\narXiv:2412.15115.\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\\ntry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.\\n2021. Learning transferable visual models from natu-\\nral language supervision. In International conference\\non machine learning, pages 8748–8763. PMLR.\\nHamed Rahimi, Jeanne Cattoni, Meriem Beghili, Mouad\\nAbrini, Mahdi Khoramshahi, Maribel Pino, and Mo-\\nhamed Chetouani. 2025. Reasoning llms for user-\\naware multimodal conversational agents. Preprint,\\narXiv:2504.01700.\\nVikram V . Ramaswamy, Sing Yu Lin, Dora Zhao,\\nAaron B. Adcock, Laurens van der Maaten, Deepti\\nGhadiyaram, and Olga Russakovsky. 2023. Geode: a\\ngeographically diverse evaluation dataset for object\\nrecognition. Preprint, arXiv:2301.02560.\\nVarun Nagaraj Rao, Siddharth Choudhary, Aditya Desh-\\npande, Ravi Kumar Satzoda, and Srikar Appalaraju.\\n2024. Raven: Multitask retrieval augmented vision-\\nlanguage learning. Preprint, arXiv:2406.19150.\\nDavid Rau, Shuai Wang, Hervé Déjean, and Stéphane\\nClinchant. 2024. Context embeddings for efficient\\nanswer generation in rag. Preprint, arXiv:2407.09252.\\nXubin Ren, Lingrui Xu, Long Xia, Shuaiqiang Wang,\\nDawei Yin, and Chao Huang. 2025. Videorag:\\nRetrieval-augmented generation with extreme long-\\ncontext videos. Preprint, arXiv:2502.01549.\\nMonica Riedler and Stefan Langer. 2024. Beyond text:\\nOptimizing rag with multimodal inputs for industrial\\napplications. Preprint, arXiv:2410.21943.\\nStephen Robertson and Hugo Zaragoza. 2009. The\\nprobabilistic relevance framework: Bm25 and beyond.\\nFoundations and Trends® in Information Retrieval,\\n3(4):333–389.\\nAnna Rohrbach, Marcus Rohrbach, Nihar Tandon, and\\nBernt Schiele. 2015. A dataset for movie description.\\nProceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition (CVPR), pages 1–10.\\nNegar Rostamzadeh, Seyedarian Hosseini, Thomas Bo-\\nquet, Wojciech Stokowiec, Ying Zhang, Christian\\nJauvin, and Chris Pal. 2018. Fashion-gen: The gener-\\native fashion dataset and challenge. arXiv preprint\\narXiv:1806.08317.\\nKuniaki Saito, Kihyuk Sohn, Xiang Zhang, Chun-Liang\\nLi, Chen-Yu Lee, Kate Saenko, and Tomas Pfister.\\n2023. Pic2word: Mapping pictures to words for\\nzero-shot composed image retrieval. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition, pages 19305–19314.\\nFulvio Sanguigni, Davide Morelli, Marcella Cornia, and\\nRita Cucchiara. 2025. Fashion-rag: Multimodal fash-\\nion image editing via retrieval-augmented generation.\\narXiv preprint arXiv:2504.14011.\\nFlorian Schneider, Narges Baba Ahmadi, Niloufar Baba\\nAhmadi, Iris V ogel, Martin Semmann, and Chris\\nBiemann. 2025. Collex – a multimodal agentic rag\\nsystem enabling interactive exploration of scientific\\ncollections. Preprint, arXiv:2504.07643.\\nChristoph Schuhmann, Romain Beaumont, Richard\\nVencu, Cade Gordon, Ross Wightman, Mehdi Cherti,\\nTheo Coombes, Aarush Katta, Clayton Mullis,\\nMitchell Wortsman, et al. 2022. Laion-5b: An\\nopen large-scale dataset for training next generation\\nimage-text models. Advances in Neural Information\\nProcessing Systems, 35:25278–25294.\\nChristoph Schuhmann, Romain Vencu, Richard Beau-\\nmont, Robert Kaczmarczyk, Jenia Jitsev, Atsushi\\nKomatsuzaki, et al. 2021. Laion-400m: Open dataset\\nof clip-filtered 400 million image-text pairs. arXiv\\npreprint arXiv:2111.02114.\\nDustin Schwenk, Apoorv Khandelwal, Christopher Clark,\\nKenneth Marino, and Roozbeh Mottaghi. 2022. A-\\nokvqa: A benchmark for visual question answering\\nusing world knowledge. In European Conference on\\nComputer Vision, pages 146–162.\\nSahel Sharifymoghaddam, Shivani Upadhyay, Wenhu\\nChen, and Jimmy Lin. 2024. Unirag: Universal\\nretrieval augmentation for multi-modal large language\\nmodels. ArXiv, abs/2405.10311.\\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\\nRadu Soricut. 2018. Conceptual captions: A cleaned,\\n20'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Mohammad Mahdi Abootorabi; Amirhosein Zobeiri; Mahdi Dehghani; Mohammadali Mohammadkhani; Bardia Mohammadi; Omid Ghahroodi; Mahdieh Soleymani Baghshah; Ehsaneddin Asgari', 'doi': 'https://doi.org/10.48550/arXiv.2502.08826', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2502.08826v3', 'source': '/content/Multimodal_RAG.pdf', 'total_pages': 34, 'page': 20, 'page_label': '21'}, page_content='hypernymed, image alt-text dataset for automatic im-\\nage captioning. In Annual Meeting of the Association\\nfor Computational Linguistics.\\nXiaobo Shen, Qianxin Huang, Long Lan, and Yuhui\\nZheng. 2024. Contrastive transformer cross-modal\\nhashing for video-text retrieval. In Proceedings of\\nthe Thirty-Third International Joint Conference on\\nArtificial Intelligence, IJCAI-24, pages 1227–1235. In-\\nternational Joint Conferences on Artificial Intelligence\\nOrganization. Main Track.\\nEnsheng Shi, Yanlin Wang, Wei Tao, Lun Du, Hongyu\\nZhang, Shi Han, Dongmei Zhang, and Hongbin Sun.\\n2022. RACE: Retrieval-augmented commit message\\ngeneration. In Proceedings of the 2022 Conference on\\nEmpirical Methods in Natural Language Processing,\\npages 5520–5530, Abu Dhabi, United Arab Emirates.\\nAssociation for Computational Linguistics.\\nFaisal Tareque Shohan, Mir Tafseer Nayeem, Samsul\\nIslam, Abu Ubaida Akash, and Shafiq Joty. 2024.\\nXL-HeadTags: Leveraging multimodal retrieval aug-\\nmentation for the multilingual generation of news\\nheadlines and tags. In Findings of the Association\\nfor Computational Linguistics: ACL 2024 , pages\\n12991–13024, Bangkok, Thailand. Association for\\nComputational Linguistics.\\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\\nand Jason Weston. 2021. Retrieval augmentation re-\\nduces hallucination in conversation. In Findings of the\\nAssociation for Computational Linguistics: EMNLP\\n2021, pages 3784–3803, Punta Cana, Dominican Re-\\npublic. Association for Computational Linguistics.\\nGunnar A Sigurdsson, Gul Varol, Giovanni Maria\\nFarinella, et al. 2018. Charadesego: A dataset\\nfor egocentric video understanding. arXiv preprint\\narXiv:1804.09626.\\nAditi Singh, Abul Ehtesham, Saket Kumar, and Tala Ta-\\nlaei Khoei. 2025. Agentic retrieval-augmented\\ngeneration: A survey on agentic rag. Preprint,\\narXiv:2501.09136.\\nAmanpreet Singh, Ronghang Hu, Vedanuj Goswami,\\nGuillaume Couairon, Wojciech Galuba, Marcus\\nRohrbach, and Douwe Kiela. 2022. Flava: A founda-\\ntional language and vision alignment model. In Pro-\\nceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition, pages 15638–15650.\\nShezheng Song, Xiaopeng Li, Shasha Li, Shan Zhao,\\nJie Yu, Jun Ma, Xiaoguang Mao, Weimin Zhang, and\\nMeng Wang. 2025. How to bridge the gap between\\nmodalities: Survey on multimodal large language\\nmodel. IEEE Transactions on Knowledge and Data\\nEngineering.\\nAleksander Theo Strand, Sushant Gautam, Cise Midoglu,\\nand Pål Halvorsen. 2024. Soccerrag: Multimodal soc-\\ncer information retrieval via natural queries. Preprint,\\narXiv:2406.01273.\\nCheng Su, Jinbo Wen, Jiawen Kang, Yonghua Wang,\\nYuanjia Su, Hudan Pan, Zishao Zhong, and M Shamim\\nHossain. 2024a. Hybrid rag-empowered multi-modal\\nllm for secure data management in internet of medical\\nthings: A diffusion-based contract approach. IEEE\\nInternet of Things Journal.\\nXin Su, Man Luo, Kris W Pan, Tien Pei Chou, Va-\\nsudev Lal, and Phillip Howard. 2024b. Sk-vqa:\\nSynthetic knowledge generation at scale for training\\ncontext-augmented multimodal llms. arXiv preprint\\narXiv:2406.19593.\\nChunyu Sun, Bingyu Liu, Zhichao Cui, Anbin Qi, Tian\\nhao Zhang, Dinghao Zhou, and Lewei Lu. 2025. Seal:\\nSpeech embedding alignment learning for speech large\\nlanguage model with retrieval-augmented generation.\\nPreprint, arXiv:2502.02603.\\nJiashuo Sun, Jihai Zhang, Yucheng Zhou, Zhaochen Su,\\nXiaoye Qu, and Yu Cheng. 2024a. Surf: Teaching\\nlarge vision-language models to selectively utilize\\nretrieved information. In Proceedings of the 2024\\nConference on Empirical Methods in Natural Lan-\\nguage Processing, pages 7611–7629.\\nLiwen Sun, James Zhao, Megan Han, and Chenyan\\nXiong. 2024b. Fact-aware multimodal retrieval\\naugmentation for accurate medical radiology report\\ngeneration. Preprint, arXiv:2407.15268.\\nManan Suri, Puneet Mathur, Franck Dernoncourt, Kanika\\nGoswami, Ryan A Rossi, and Dinesh Manocha. 2024.\\nVisdom: Multi-document qa with visually rich ele-\\nments using multimodal retrieval-augmented genera-\\ntion. arXiv preprint arXiv:2412.10704.\\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\\nJon Shlens, and Zbigniew Wojna. 2016. Rethinking\\nthe inception architecture for computer vision. In\\nProceedings of the IEEE conference on computer\\nvision and pattern recognition, pages 2818–2826.\\nAlon Talmor, Ori Yoran, Amnon Catav, Dan Lahav,\\nYizhong Wang, Akari Asai, Gabriel Ilharco, Han-\\nnaneh Hajishirzi, and Jonathan Berant. 2021. Multi-\\nmodal{qa}: complex question answering over text,\\ntables and images. In International Conference on\\nLearning Representations.\\nCheng Tan, Jingxuan Wei, Linzhuang Sun, Zhangyang\\nGao, Siyuan Li, Bihui Yu, Ruifeng Guo, and Stan Z. Li.\\n2024. Retrieval meets reasoning: Even high-school\\ntextbook knowledge benefits multimodal reasoning.\\nPreprint, arXiv:2405.20834.\\nYansong Tang, Xiaohan Wang, Jingdong Wang, et al.\\n2019. Coin: A large-scale dataset for comprehensive\\ninstructional video analysis. Proceedings of the\\nIEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 1–10.\\nGemini Team, Rohan Anil, Sebastian Borgeaud, Jean-\\nBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan\\nSchalkwyk, Andrew M. Dai, Anja Hauth, Katie\\nMillican, David Silver, Melvin Johnson, Ioannis\\n21'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Mohammad Mahdi Abootorabi; Amirhosein Zobeiri; Mahdi Dehghani; Mohammadali Mohammadkhani; Bardia Mohammadi; Omid Ghahroodi; Mahdieh Soleymani Baghshah; Ehsaneddin Asgari', 'doi': 'https://doi.org/10.48550/arXiv.2502.08826', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2502.08826v3', 'source': '/content/Multimodal_RAG.pdf', 'total_pages': 34, 'page': 21, 'page_label': '22'}, page_content='Antonoglou, Julian Schrittwieser, Amelia Glaese,\\nJilin Chen, Emily Pitler, Timothy Lillicrap, Ange-\\nliki Lazaridou, Orhan Firat, James Molloy, Michael\\nIsard, Paul R. Barham, Tom Hennigan, Benjamin Lee,\\nFabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan\\nDoherty, Eli Collins, Clemens Meyer, Eliza Ruther-\\nford, Erica Moreira, Kareem Ayoub, Megha Goel,\\nJack Krawczyk, Cosmo Du, Ed Chi, Heng-Tze Cheng,\\nEric Ni, Purvi Shah, Patrick Kane, Betty Chan, Manaal\\nFaruqui, Aliaksei Severyn, Hanzhao Lin, YaGuang\\nLi, Yong Cheng, Abe Ittycheriah, Mahdis Mahdieh,\\nMia Chen, Pei Sun, Dustin Tran, Sumit Bagri, Balaji\\nLakshminarayanan, and et al. 2024. Gemini: A fam-\\nily of highly capable multimodal models. Preprint,\\narXiv:2312.11805.\\nMo Tiwari, Ryan Kang, Jaeyong Lee, Donghyun Lee,\\nChristopher J Piech, Sebastian Thrun, Ilan Shomorony,\\nand Martin Jinye Zhang. 2024. Faster maximum\\ninner product search in high dimensions. In Forty-first\\nInternational Conference on Machine Learning.\\nZhan Tong, Yibing Song, Jue Wang, and Limin Wang.\\n2022. VideoMAE: Masked autoencoders are data-\\nefficient learners for self-supervised video pre-training.\\nIn Advances in Neural Information Processing Sys-\\ntems.\\nSM Tonmoy, SM Zaman, Vinija Jain, Anku Rani, Vipula\\nRawte, Aman Chadha, and Amitava Das. 2024. A\\ncomprehensive survey of hallucination mitigation\\ntechniques in large language models. arXiv preprint\\narXiv:2401.01313.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\\nAmjad Almahairi, Yasmine Babaei, Nikolay Bash-\\nlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\\nale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer,\\nMoya Chen, Guillem Cucurull, David Esiobu, Jude\\nFernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cyn-\\nthia Gao, Vedanuj Goswami, Naman Goyal, Anthony\\nHartshorn, Saghar Hosseini, Rui Hou, Hakan Inan,\\nMarcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel\\nKloumann, Artem Korenev, Punit Singh Koura, Marie-\\nAnne Lachaux, Thibaut Lavril, Jenya Lee, Diana\\nLiskovich, Yinghai Lu, Yuning Mao, Xavier Martinet,\\nTodor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin\\nNie, and et al. 2023. Llama 2: Open foundation and\\nfine-tuned chat models. Preprint, arXiv:2307.09288.\\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. 2019.\\nRepresentation learning with contrastive predictive\\ncoding. Preprint, arXiv:1807.03748.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In Proceedings of the 31st International\\nConference on Neural Information Processing Systems,\\nNIPS’17, page 6000–6010, Red Hook, NY , USA.\\nCurran Associates Inc.\\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi\\nParikh. 2015. Cider: Consensus-based image de-\\nscription evaluation. In Proceedings of the IEEE\\nconference on computer vision and pattern recogni-\\ntion, pages 4566–4575.\\nDongsheng Wang, Natraj Raman, Mathieu Sibue,\\nZhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei,\\nArmineh Nourbakhsh, and Xiaomo Liu. 2024a. Do-\\ncLLM: A layout-aware generative language model for\\nmultimodal document understanding. In Proceedings\\nof the 62nd Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers),\\npages 8529–8548, Bangkok, Thailand. Association\\nfor Computational Linguistics.\\nJiamian Wang, Guohao Sun, Pichao Wang, Dongfang\\nLiu, Sohail Dianat, Majid Rabbani, Raghuveer Rao,\\nand Zhiqiang Tao. 2024b. Text is mass: Modeling\\nas stochastic embedding for text-video retrieval. In\\nProceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition (CVPR), pages\\n16551–16560.\\nMengzhao Wang, Xiangyu Ke, Xiaoliang Xu, Lu Chen,\\nYunjun Gao, Pinpin Huang, and Runkai Zhu. 2024c.\\nMust: An effective and scalable framework for mul-\\ntimodal search of target modality. In 2024 IEEE\\n40th International Conference on Data Engineering\\n(ICDE), pages 4747–4759. IEEE.\\nPeng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhi-\\nhao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin\\nWang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei\\nDu, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang\\nZhou, Jingren Zhou, and Junyang Lin. 2024d. Qwen2-\\nvl: Enhancing vision-language model’s perception\\nof the world at any resolution. arXiv preprint\\narXiv:2409.12191.\\nPeng Wang, Shijie Wang, Junyang Lin, Shuai Bai, Xi-\\naohuan Zhou, Jingren Zhou, Xinggang Wang, and\\nChang Zhou. 2023a. One-peace: Exploring one gen-\\neral representation model toward unlimited modalities.\\narXiv preprint arXiv:2305.11172.\\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\\nYang, and Ming Zhou. 2020a. Minilm: Deep self-\\nattention distillation for task-agnostic compression\\nof pre-trained transformers. Advances in Neural\\nInformation Processing Systems, 33:5776–5788.\\nXin Wang, Jiawei Wu, Junkun Chen, et al. 2019. Vatex:\\nA large-scale, high-quality multilingual dataset for\\nvideo-and-language research. Proceedings of the\\nIEEE International Conference on Computer Vision\\n(ICCV), pages 1–10.\\nYi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo\\nYu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen,\\nYaohui Wang, et al. 2023b. Internvid: A large-scale\\nvideo-text dataset for multimodal understanding and\\ngeneration. In The Twelfth International Conference\\non Learning Representations.\\nYi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun\\nHuang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu,\\nZun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo\\nYu, Yali Wang, Limin Wang, and Yu Qiao. 2022.\\n22'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Mohammad Mahdi Abootorabi; Amirhosein Zobeiri; Mahdi Dehghani; Mohammadali Mohammadkhani; Bardia Mohammadi; Omid Ghahroodi; Mahdieh Soleymani Baghshah; Ehsaneddin Asgari', 'doi': 'https://doi.org/10.48550/arXiv.2502.08826', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2502.08826v3', 'source': '/content/Multimodal_RAG.pdf', 'total_pages': 34, 'page': 22, 'page_label': '23'}, page_content='Internvideo: General video foundation models via\\ngenerative and discriminative learning. Preprint,\\narXiv:2212.03191.\\nZhihao Wang, Jian Chen, and Steven C. H. Hoi. 2020b.\\nDeep learning for image super-resolution: A survey.\\nPreprint, arXiv:1902.06068.\\nCong Wei, Yang Chen, Haonan Chen, Hexiang Hu,\\nGe Zhang, Jie Fu, Alan Ritter, and Wenhu Chen.\\n2024a. Uniir: Training and benchmarking universal\\nmultimodal information retrievers. In European Con-\\nference on Computer Vision, pages 387–404. Springer.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V .\\nLe, and Denny Zhou. 2024b. Chain-of-thought\\nprompting elicits reasoning in large language models.\\nIn Proceedings of the 36th International Conference\\non Neural Information Processing Systems, NIPS ’22,\\nRed Hook, NY , USA. Curran Associates Inc.\\nHaokun Wen, Xuemeng Song, Xiaolin Chen, Yinwei\\nWei, Liqiang Nie, and Tat-Seng Chua. 2024. Simple\\nbut effective raw-data level multimodal fusion for\\ncomposed image retrieval. In Proceedings of the 47th\\nInternational ACM SIGIR Conference on Research\\nand Development in Information Retrieval , SIGIR\\n2024, page 229–239. ACM.\\nThomas Winterbottom, Sarah Xiao, Alistair McLean,\\nand Noura Al Moubayed. 2020. On modality bias in\\nthe tvqa dataset. Preprint, arXiv:2012.10210.\\nHui Wu, Yupeng Gao, Xiaoxiao Guo, Ziad Al-Halah,\\nSteven Rennie, Kristen Grauman, and Rogerio Feris.\\n2021. The fashion iq dataset: Retrieving images\\nby combining side information and relative natural\\nlanguage feedback. CVPR.\\nIan Wu, Sravan Jayanthi, Vijay Viswanathan, Simon\\nRosenberg, Sina Khoshfetrat Pakazad, Tongshuang\\nWu, and Graham Neubig. 2024a. Synthetic mul-\\ntimodal question generation. In Findings of the\\nAssociation for Computational Linguistics: EMNLP\\n2024, pages 12960–12993, Miami, Florida, USA.\\nAssociation for Computational Linguistics.\\nShengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-\\nSeng Chua. 2024b. Next-gpt: Any-to-any multimodal\\nllm. In Proceedings of the International Conference\\non Machine Learning, pages 53366–53397.\\nYusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor\\nBerg-Kirkpatrick, and Shlomo Dubnov. 2023. Large-\\nscale contrastive language-audio pretraining with fea-\\nture fusion and keyword-to-caption augmentation. In\\nICASSP 2023 - 2023 IEEE International Conference\\non Acoustics, Speech and Signal Processing (ICASSP),\\npages 1–5.\\nPeng Xia, Kangyu Zhu, Haoran Li, Tianze Wang, Weijia\\nShi, Sheng Wang, Linjun Zhang, James Zou, and\\nHuaxiu Yao. 2024a. Mmed-rag: Versatile multi-\\nmodal rag system for medical vision language models.\\nPreprint, arXiv:2410.13085.\\nPeng Xia, Kangyu Zhu, Haoran Li, Hongtu Zhu, Yun\\nLi, Gang Li, Linjun Zhang, and Huaxiu Yao. 2024b.\\nRULE: Reliable multimodal RAG for factuality in\\nmedical vision language models. In Proceedings\\nof the 2024 Conference on Empirical Methods in\\nNatural Language Processing, pages 1081–1093, Mi-\\nami, Florida, USA. Association for Computational\\nLinguistics.\\nCihan Xiao, Zejiang Hou, Daniel Garcia-Romero, and\\nKyu J Han. 2025. Contextual asr with retrieval\\naugmented large language model. In ICASSP 2025\\n- 2025 IEEE International Conference on Acoustics,\\nSpeech and Signal Processing (ICASSP), pages 1–5.\\nD. Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang,\\nXiangnan He, and Yueting Zhuang. 2017. Video\\nquestion answering via gradually refined attention\\nover appearance and motion. Proceedings of the 25th\\nACM international conference on Multimedia.\\nHaiyang Xu, Qinghao Ye, Xuan Wu, Ming Yan, Yuan\\nMiao, Jiabo Ye, Guohai Xu, Anwen Hu, Yaya Shi,\\nGuangwei Xu, Chenliang Li, Qi Qian, Maofei Que,\\nJi Zhang, Xiao Zeng, and Fei Huang. 2023. Youku-\\nmplug: A 10 million large-scale chinese video-\\nlanguage dataset for pre-training and benchmarks.\\nPreprint, arXiv:2306.04362.\\nJilan Xu, Yifei Huang, Junlin Hou, Guo Chen, Yuejie\\nZhang, Rui Feng, and Weidi Xie. 2024a. Retrieval-\\naugmented egocentric video captioning. In Proceed-\\nings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition, pages 13525–13536.\\nZiwei Xu, Sanjay Jain, and Mohan Kankanhalli. 2024b.\\nHallucination is inevitable: An innate limitation of\\nlarge language models. Preprint, arXiv:2401.11817.\\nJinlong Xue, Yayue Deng, Yingming Gao, and Ya Li.\\n2024a. Retrieval augmented generation in prompt-\\nbased text-to-speech synthesis with context-aware\\ncontrastive language-audio pretraining. Preprint,\\narXiv:2406.03714.\\nJunxiao Xue, Quan Deng, Fei Yu, Yanhao Wang, Jun\\nWang, and Yuehua Li. 2024b. Enhanced multi-\\nmodal rag-llm for accurate visual question answering.\\nPreprint, arXiv:2412.20927.\\nShi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling.\\n2024. Corrective retrieval augmented generation.\\nYibin Yan and Weidi Xie. 2024. Echosight: Advancing\\nvisual-language models with wiki knowledge. In\\nFindings of the Association for Computational Linguis-\\ntics: EMNLP 2024, pages 1538–1551, Miami, Florida,\\nUSA. Association for Computational Linguistics.\\nMu Yang, Bowen Shi, Matthew Le, Wei-Ning Hsu, and\\nAndros Tjandra. 2024a. Audiobox tta-rag: Improving\\nzero-shot and few-shot text-to-audio with retrieval-\\naugmented generation. Preprint, arXiv:2411.05141.\\n23'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Mohammad Mahdi Abootorabi; Amirhosein Zobeiri; Mahdi Dehghani; Mohammadali Mohammadkhani; Bardia Mohammadi; Omid Ghahroodi; Mahdieh Soleymani Baghshah; Ehsaneddin Asgari', 'doi': 'https://doi.org/10.48550/arXiv.2502.08826', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2502.08826v3', 'source': '/content/Multimodal_RAG.pdf', 'total_pages': 34, 'page': 23, 'page_label': '24'}, page_content='Qian Yang, Qian Chen, Wen Wang, Baotian Hu, and\\nMin Zhang. 2023. Enhancing multi-modal multi-\\nhop question answering via structured knowledge and\\nunified retrieval-generation. In Proceedings of the\\n31st ACM International Conference on Multimedia,\\nMM ’23, page 5223–5234, New York, NY , USA.\\nAssociation for Computing Machinery.\\nWei Yang, Jingjing Fu, Rui Wang, Jinyu Wang, Lei Song,\\nand Jiang Bian. 2025. Omgm: Orchestrate multiple\\ngranularities and modalities for efficient multimodal\\nretrieval. arXiv preprint arXiv:2505.07879.\\nZhenyu Yang, Dizhan Xue, Shengsheng Qian, Weiming\\nDong, and Changsheng Xu. 2024b. Ldre: Llm-\\nbased divergent reasoning and ensemble for zero-shot\\ncomposed image retrieval. In Proceedings of the 47th\\nInternational ACM SIGIR Conference on Research\\nand Development in Information Retrieval, SIGIR ’24,\\npage 80–90, New York, NY , USA. Association for\\nComputing Machinery.\\nBarry Menglong Yao, Aditya Shah, Lichao Sun, Jin-Hee\\nCho, and Lifu Huang. 2023. End-to-end multimodal\\nfact-checking and explanation generation: A challeng-\\ning dataset and models. In Proceedings of the 46th\\nInternational ACM SIGIR Conference on Research\\nand Development in Information Retrieval , pages\\n2733–2743.\\nMichihiro Yasunaga, Armen Aghajanyan, Weijia Shi,\\nRichard James, Jure Leskovec, Percy Liang, Mike\\nLewis, Luke Zettlemoyer, and Wen-Tau Yih. 2023.\\nRetrieval-augmented multimodal language modeling.\\nIn International Conference on Machine Learning,\\npages 39755–39769. PMLR.\\nLinwei Ye, Mrigank Rochan, Zhi Liu, and Yang Wang.\\n2019. Cross-modal self-attention network for re-\\nferring image segmentation. In Proceedings of the\\nIEEE/CVF conference on computer vision and pattern\\nrecognition, pages 10502–10511.\\nWoongyeong Yeo, Kangsan Kim, Soyeong Jeong, Jin-\\nheon Baek, and Sung Ju Hwang. 2025. Universalrag:\\nRetrieval-augmented generation over multiple cor-\\npora with diverse modalities and granularities. arXiv\\npreprint arXiv:2504.20734.\\nZiruo Yi, Ting Xiao, and Mark V . Albert. 2025. A mul-\\ntimodal multi-agent framework for radiology report\\ngeneration. Preprint, arXiv:2505.09787.\\nPeter Young, Alice Lai, Micah Hodosh, and Julia Hock-\\nenmaier. 2014. From image descriptions to visual\\ndenotations: New similarity metrics for semantic in-\\nference over event descriptions. Transactions of the\\nAssociation for Computational Linguistics, 2:67–78.\\nShi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao\\nRan, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han,\\nZhiyuan Liu, and Maosong Sun. 2024. Visrag:\\nVision-based retrieval-augmented generation on multi-\\nmodality documents. Preprint, arXiv:2410.10594.\\nXiaohan Yu, Zhihan Yang, and Chong Chen. 2025.\\nUnveiling the potential of multimodal retrieval aug-\\nmented generation with planning. Preprint,\\narXiv:2501.15470.\\nZhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao,\\nYueting Zhuang, and Dacheng Tao. 2019. Activitynet-\\nqa: A dataset for understanding complex web videos\\nvia question answering. In Proceedings of the AAAI\\nConference on Artificial Intelligence, volume 33, pages\\n9127–9134.\\nJianhao Yuan, Shuyang Sun, Daniel Omeiza, Bo Zhao,\\nPaul Newman, Lars Kunze, and Matthew Gadd.\\n2024. Rag-driver: Generalisable driving explana-\\ntions with retrieval-augmented in-context learning\\nin multi-modal large language model. Preprint,\\narXiv:2402.10828.\\nZheng Yuan, Qiao Jin, Chuanqi Tan, Zhengyun Zhao,\\nHongyi Yuan, Fei Huang, and Songfang Huang. 2023.\\nRamm: Retrieval-augmented biomedical visual ques-\\ntion answering with multi-modal pre-training. In\\nProceedings of the 31st ACM International Confer-\\nence on Multimedia, pages 547–556.\\nJiaqi Zhai, Zhaojie Gong, Yueming Wang, Xiao Sun,\\nZheng Yan, Fu Li, and Xing Liu. 2023. Revisiting\\nneural retrieval on accelerators. In Proceedings of the\\n29th ACM SIGKDD Conference on Knowledge Dis-\\ncovery and Data Mining, KDD ’23, page 5520–5531,\\nNew York, NY , USA. Association for Computing\\nMachinery.\\nWenjia Zhai. 2024. Self-adaptive multimodal retrieval-\\naugmented generation. Preprint, arXiv:2410.11321.\\nHaoyu Zhang, Jun Liu, Zhenhua Zhu, Shulin Zeng,\\nMaojia Sheng, Tao Yang, Guohao Dai, and Yu Wang.\\n2024a. Efficient and effective retrieval of dense-sparse\\nhybrid vectors using graph-based approximate nearest\\nneighbor search. Preprint, arXiv:2410.20381.\\nJin Zhang, Defu Lian, Haodi Zhang, Baoyun Wang, and\\nEnhong Chen. 2023a. Query-aware quantization\\nfor maximum inner product search. Proceedings\\nof the AAAI Conference on Artificial Intelligence ,\\n37(4):4875–4883.\\nJinxu Zhang, Yongqi Yu, and Yu Zhang. 2024b. Cream:\\nCoarse-to-fine retrieval and multi-modal efficient tun-\\ning for document vqa. In Proceedings of the 32nd\\nACM International Conference on Multimedia, MM\\n’24, page 925–934, New York, NY , USA. Association\\nfor Computing Machinery.\\nJuntao Zhang, Yuehuai Liu, Yu-Wing Tai, and Chi-Keung\\nTang. 2024c. C3net: Compound conditioned control-\\nnet for multimodal content generation. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition, pages 26886–26895.\\nJunyuan Zhang, Qintong Zhang, Bin Wang, Linke\\nOuyang, Zichen Wen, Ying Li, Ka-Ho Chow, Con-\\nghui He, and Wentao Zhang. 2024d. Ocr hin-\\nders rag: Evaluating the cascading impact of ocr\\n24'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Mohammad Mahdi Abootorabi; Amirhosein Zobeiri; Mahdi Dehghani; Mohammadali Mohammadkhani; Bardia Mohammadi; Omid Ghahroodi; Mahdieh Soleymani Baghshah; Ehsaneddin Asgari', 'doi': 'https://doi.org/10.48550/arXiv.2502.08826', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2502.08826v3', 'source': '/content/Multimodal_RAG.pdf', 'total_pages': 34, 'page': 24, 'page_label': '25'}, page_content='on retrieval-augmented generation. arXiv preprint\\narXiv:2412.02592.\\nLu Zhang, Tiancheng Zhao, Heting Ying, Yibo Ma, and\\nKyusong Lee. 2024e. OmAgent: A multi-modal\\nagent framework for complex video understanding\\nwith task divide-and-conquer. In Proceedings of the\\n2024 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 10031–10045, Miami,\\nFlorida, USA. Association for Computational Linguis-\\ntics.\\nTao Zhang, Ziqi Zhang, Zongyang Ma, Yuxin Chen,\\nZhongang Qi, Chunfen Yuan, Bing Li, Junfu Pu,\\nYuxuan Zhao, Zehua Xie, Jin Ma, Ying Shan, and\\nWeiming Hu. 2024f. mr2ag: Multimodal retrieval-\\nreflection-augmented generation for knowledge-based\\nvqa. ArXiv, abs/2411.15041.\\nTao Zhang, Ziqi Zhang, Zongyang Ma, Yuxin Chen,\\nZhongang Qi, Chunfeng Yuan, Bing Li, Junfu Pu,\\nYuxuan Zhao, Zehua Xie, Jin Ma, Ying Shan, and\\nWeiming Hu. 2024g. mr 2ag: Multimodal retrieval-\\nreflection-augmented generation for knowledge-based\\nvqa. Preprint, arXiv:2411.15041.\\nTianjun Zhang, Shishir G Patil, Naman Jain, Sheng Shen,\\nMatei Zaharia, Ion Stoica, and Joseph E. Gonzalez.\\n2024h. RAFT: Adapting language model to domain\\nspecific RAG. In First Conference on Language\\nModeling.\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Wein-\\nberger, and Yoav Artzi. 2020. Bertscore: Evaluating\\ntext generation with bert. Preprint, arXiv:1904.09675.\\nXin Zhang, Yanzhao Zhang, Wen Xie, Mingxin Li, Ziqi\\nDai, Dingkun Long, Pengjun Xie, Meishan Zhang,\\nWenjie Li, and Min Zhang. 2024i. Gme: Improving\\nuniversal multimodal retrieval by multimodal llms.\\nPreprint, arXiv:2412.16855.\\nZhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao,\\nGeorge Karypis, and Alex Smola. 2023b. Multimodal\\nchain-of-thought reasoning in language models. arXiv\\npreprint arXiv:2302.00923.\\nRuochen Zhao, Hailin Chen, Weishi Wang, Fangkai\\nJiao, Xuan Long Do, Chengwei Qin, Bosheng Ding,\\nXiaobao Guo, Minzhi Li, Xingxuan Li, and Shafiq\\nJoty. 2023a. Retrieving multimodal information for\\naugmented generation: A survey. In Findings of the\\nAssociation for Computational Linguistics: EMNLP\\n2023, pages 4736–4756, Singapore. Association for\\nComputational Linguistics.\\nXi Zhao, Bolong Zheng, Xiaomeng Yi, Xiaofan Luan,\\nCharles Xie, Xiaofang Zhou, and Christian S. Jensen.\\n2023b. Fargo: Fast maximum inner product search\\nvia global multi-probing. Proc. VLDB Endow. ,\\n16(5):1100–1112.\\nXiangyu Zhao, Yuehan Zhang, Wenlong Zhang, and\\nXiao-Ming Wu. 2024. Unifashion: A unified vision-\\nlanguage model for multimodal fashion retrieval and\\ngeneration. In Proceedings of the 2024 Conference on\\nEmpirical Methods in Natural Language Processing,\\npages 1490–1507, Miami, Florida, USA. Association\\nfor Computational Linguistics.\\nChaofan Zheng, Xinyu Lyu, Lianli Gao, Bo Dai, and\\nJingkuan Song. 2023. Prototype-Based Embed-\\nding Network for Scene Graph Generation . In 2023\\nIEEE/CVF Conference on Computer Vision and Pat-\\ntern Recognition (CVPR), pages 22783–22792, Los\\nAlamitos, CA, USA. IEEE Computer Society.\\nQi Zhi Lim, Chin Poo Lee, Kian Ming Lim, and Ah-\\nmad Kamsani Samingan. 2024. Unirag: Unification,\\nretrieval, and generation for multimodal question an-\\nswering with pre-trained language models. IEEE\\nAccess, 12:71505–71519.\\nTing Zhong, Jian Lang, Yifan Zhang, Zhangtao Cheng,\\nKunpeng Zhang, and Fan Zhou. 2024. Predicting\\nmicro-video popularity via multi-modal retrieval aug-\\nmentation. In Proceedings of the 47th International\\nACM SIGIR Conference on Research and Development\\nin Information Retrieval, SIGIR ’24, page 2579–2583,\\nNew York, NY , USA. Association for Computing\\nMachinery.\\nXu Zhong, Jianbin Tang, and Antonio Jimeno Yepes.\\n2019. Publaynet: largest dataset ever for document\\nlayout analysis. In 2019 International conference on\\ndocument analysis and recognition (ICDAR), pages\\n1015–1022. IEEE.\\nJunjie Zhou, Zheng Liu, Ze Liu, Shitao Xiao, Yueze\\nWang, Bo Zhao, Chen Jason Zhang, Defu Lian, and\\nYongping Xiong. 2024a. Megapairs: Massive data\\nsynthesis for universal multimodal retrieval. Preprint,\\narXiv:2412.14475.\\nJunjie Zhou, Zheng Liu, Shitao Xiao, Bo Zhao, and\\nYongping Xiong. 2024b. VISTA: Visualized text\\nembedding for universal multi-modal retrieval. In\\nProceedings of the 62nd Annual Meeting of the As-\\nsociation for Computational Linguistics (Volume 1:\\nLong Papers), pages 3185–3200, Bangkok, Thailand.\\nAssociation for Computational Linguistics.\\nLuowei Zhou, Chenliang Xu, and Jason J Corso. 2018.\\nTowards automatic learning of procedures from web\\ninstructional videos. Proceedings of the AAAI Confer-\\nence on Artificial Intelligence, 32(1):1–10.\\nRen Zhou. 2024. Advanced embedding techniques in\\nmultimodal retrieval augmented generation a compre-\\nhensive study on cross modal ai applications. Journal\\nof Computing and Electronic Information Manage-\\nment, 13(3):16–22.\\nShuyan Zhou, Uri Alon, Frank F. Xu, Zhengbao Jiang,\\nand Graham Neubig. 2023. Docprompting: Gener-\\nating code by retrieving the docs. In The Eleventh\\nInternational Conference on Learning Representa-\\ntions.\\nTianshuo Zhou, Sen Mei, Xinze Li, Zhenghao Liu,\\nChenyan Xiong, Zhiyuan Liu, Yu Gu, and Ge Yu.\\n25'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Mohammad Mahdi Abootorabi; Amirhosein Zobeiri; Mahdi Dehghani; Mohammadali Mohammadkhani; Bardia Mohammadi; Omid Ghahroodi; Mahdieh Soleymani Baghshah; Ehsaneddin Asgari', 'doi': 'https://doi.org/10.48550/arXiv.2502.08826', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2502.08826v3', 'source': '/content/Multimodal_RAG.pdf', 'total_pages': 34, 'page': 25, 'page_label': '26'}, page_content='2024c. Marvel: unlocking the multi-modal capabil-\\nity of dense retrieval via visual module plugin. In\\nProceedings of the 62nd Annual Meeting of the As-\\nsociation for Computational Linguistics (Volume 1:\\nLong Papers), pages 14608–14624.\\nYujia Zhou, Yan Liu, Xiaoxi Li, Jiajie Jin, Hongjin Qian,\\nZheng Liu, Chaozhuo Li, Zhicheng Dou, Tsung-Yi\\nHo, and Philip S. Yu. 2024d. Trustworthiness in\\nretrieval-augmented generation systems: A survey.\\nvolume abs/2409.10102.\\nZhongliang Zhou, Jielu Zhang, Zihan Guan, Mengxuan\\nHu, Ni Lao, Lan Mu, Sheng Li, and Gengchen Mai.\\n2024e. Img2loc: Revisiting image geolocalization\\nusing multi-modality foundation models and image-\\nbased retrieval-augmented generation. In Proceedings\\nof the 47th International ACM SIGIR Conference on\\nResearch and Development in Information Retrieval,\\npages 2749–2754.\\nWenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu,\\nShujian Huang, Lingpeng Kong, Jiajun Chen, and\\nLei Li. 2024a. Multilingual machine translation\\nwith large language models: Empirical results and\\nanalysis. In Findings of the Association for Computa-\\ntional Linguistics: NAACL 2024, pages 2765–2781,\\nMexico City, Mexico. Association for Computational\\nLinguistics.\\nYinghao Zhu, Changyu Ren, Zixiang Wang, Xiaochen\\nZheng, Shiyun Xie, Junlan Feng, Xi Zhu, Zhoujun\\nLi, Liantao Ma, and Chengwei Pan. 2024b. Emerge:\\nEnhancing multimodal electronic health records pre-\\ndictive modeling with retrieval-augmented generation.\\nIn Proceedings of the 33rd ACM International Con-\\nference on Information and Knowledge Management,\\nCIKM ’24, page 3549–3559, New York, NY , USA.\\nAssociation for Computing Machinery.\\nYinghao Zhu, Changyu Ren, Shiyun Xie, Shukai Liu,\\nHangyuan Ji, Zixiang Wang, Tao Sun, Long He, Zhou-\\njun Li, Xi Zhu, and Chengwei Pan. 2024c. Realm:\\nRag-driven enhancement of multimodal electronic\\nhealth records analysis via large language models.\\nPreprint, arXiv:2402.07016.\\nZhengyuan Zhu, Daniel Lee, Hong Zhang, Sai Sree\\nHarsha, Loic Feujio, Akash Maharaj, and Yunyao\\nLi. 2025. Murar: A simple and effective multi-\\nmodal retrieval and answer refinement framework for\\nmultimodal question answering. In Proceedings of\\nthe 31st International Conference on Computational\\nLinguistics: System Demonstrations, pages 126–135.\\nJialong Zuo, Hanyu Zhou, Ying Nie, Feng Zhang, Tianyu\\nGuo, Nong Sang, Yunhe Wang, and Changxin Gao.\\n2024. Ufinebench: Towards text-based person re-\\ntrieval with ultra-fine granularity. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and\\nPattern Recognition, pages 22010–22019.\\nA Taxonomy\\nIn this section, we provide more details regarding the\\ntaxonomy of multimodal RAG systems, previously\\nmentioned in Figure 2. Additionally, we present\\na classification of multimodal RAG application\\ndomains in Figure 3.\\nFigure 2 provides an overview of recent advances\\nin multimodal RAG systems. The taxonomy is\\norganized into several key categories.\\n• Retrieval strategies cover efficient search and\\nsimilarity retrieval methods (including maxi-\\nmum inner product search (MIPS) variants and\\ndifferent multimodal encoders) and modality-\\ncentric techniques that distinguish between text-\\n, vision-, audio-, and video-centric as well as\\ndocument retrieval models. Re-ranking strate-\\ngies further refine these methods via optimized\\nexample selection, relevance scoring, and fil-\\ntering.\\n• Fusion mechanisms cover score fusion and\\nalignment techniques, including CLIP score\\nfusion and prototype-based embeddings that\\nunify multimodal representations, attention-\\nbased methods such as cross-attention and co-\\nattention transformers that dynamically weight\\ncross-modal interactions, and unified frame-\\nworks and projections like hierarchical fusion\\nand dense-to-sparse projections that consoli-\\ndate multimodal inputs.\\n• Augmentation techniques address context\\nenrichment as well as adaptive and iterative\\nretrieval.\\n• Generation methods include in-context learn-\\ning, reasoning, instruction tuning, source attri-\\nbution, and agentic frameworks.\\n• training strategies are characterized by ap-\\nproaches to alignment and robustness.\\nDetailed discussions of these categories are provided\\nin the corresponding sections.\\nFigure 3 presents the taxonomy of application do-\\nmains for multimodal RAG systems. The identified\\ndomains include healthcare and medicine, software\\nengineering, fashion and e-commerce, entertainment\\nand social computing, and emerging applications.\\nThis classification offers a concise overview of the\\ndiverse applications and serves as a framework for\\nthe more detailed analyses that follow.\\n26'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Mohammad Mahdi Abootorabi; Amirhosein Zobeiri; Mahdi Dehghani; Mohammadali Mohammadkhani; Bardia Mohammadi; Omid Ghahroodi; Mahdieh Soleymani Baghshah; Ehsaneddin Asgari', 'doi': 'https://doi.org/10.48550/arXiv.2502.08826', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2502.08826v3', 'source': '/content/Multimodal_RAG.pdf', 'total_pages': 34, 'page': 26, 'page_label': '27'}, page_content='B Dataset and Benchmark\\nMultimodal RAG research employs diverse datasets\\nand benchmarks to evaluate retrieval, integration,\\nand generation across heterogeneous sources. Im-\\nage–text tasks, including captioning and retrieval,\\ncommonly use MS-COCO (Lin et al., 2014),\\nFlickr30K (Young et al., 2014), and LAION-400M\\n(Schuhmann et al., 2021), while visual question an-\\nswering (QA) with external knowledge is supported\\nby OK-VQA (Marino et al., 2019) and WebQA\\n(Chang et al., 2022). For complex multimodal\\nreasoning, MultimodalQA (Talmor et al., 2021) inte-\\ngrates text, images, and tables, whereas video-text\\ntasks leverage ActivityNet (Caba Heilbron et al.,\\n2015) and YouCook2 (Zhou et al., 2018). In the med-\\nical domain, MIMIC-CXR (Johnson et al., 2019)\\nand CheXpert (Irvin et al., 2019) facilitate tasks\\nsuch as medical report generation. It should be\\nnoted that a number of these datasets are unimodal\\n(e.g., solely text-based or image-based). Unimodal\\ndatasets are frequently employed to represent a spe-\\ncific modality and are subsequently integrated with\\ncomplementary datasets from other modalities. This\\nmodular approach allows each dataset to contribute\\nits domain-specific strengths, thereby enhancing the\\noverall performance of the multimodal retrieval and\\ngeneration processes.\\nBenchmarks assess multimodal RAG systems on vi-\\nsual reasoning, external knowledge integration, and\\ndynamic retrieval. The M 2RAG (Ma et al., 2024d)\\nbenchmark provides a unified evaluation framework\\nthat combines fine-grained text-modal and multi-\\nmodal metrics to jointly assess both the quality of\\ngenerated language and the effective integration\\nof visual elements. In addition, (Liu et al., 2025d)\\nintroduce another specialized benchmark for multi-\\nmodal RAG that evaluates performance across image\\ncaptioning, multi-modal question answering, fact\\nverification, and image reranking in an open-domain\\nretrieval setting. Vision-focused evaluations, includ-\\ning MRAG-Bench (Hu et al., 2024c), VQAv2 (Goyal\\net al., 2017a) and VisDoMBench (Suri et al., 2024),\\ntest models on complex visual tasks. Dyn-VQA\\n(Li et al., 2024d), MMBench (Liu et al., 2025c),\\nand ScienceQA (Lu et al., 2022) evaluate dynamic\\nretrieval and multi-hop reasoning across textual, vi-\\nsual, and diagrammatic inputs. Knowledge-intensive\\nbenchmarks, such as TriviaQA (Joshi et al., 2017)\\nand Natural Questions (Kwiatkowski et al., 2019),\\ntogether with document-oriented evaluations such\\nas OmniDocBench (Ouyang et al., 2024), measure\\nintegration of unstructured and structured data. Ad-\\nvanced retrieval benchmarks such as RAG-Check\\n(Mortaheb et al., 2025a) evaluate retrieval relevance\\nand system reliability, while specialized assessments\\nlike Counterfactual VQA (Niu et al., 2021) test\\nrobustness against adversarial inputs. Additionally,\\nOCR impact studies such as OHRBench (Zhang\\net al., 2024d) examine the cascading effects of errors\\non RAG systems.\\nThe choice of dataset significantly influences the\\nevaluation focus, ranging from foundational pre-\\ntraining on large-scale image-text corpora like\\nLAION-5B (Schuhmann et al., 2022) (5.85 bil-\\nlion pairs) or MINT-1T (Awadalla et al., 2024) (3.4\\nbillion images with 1 trillion text tokens), to more\\nspecialized tasks such as video understanding with\\nHowTo100M (Miech et al., 2019) (136 million video\\nclips) or medical report generation using MIMIC-\\nCXR (Johnson et al., 2019) (125,417 image-report\\npairs).\\nDatasets are often tailored for specific downstream\\ntasks. For visual question answering, VQA (Antol\\net al., 2015) and A-OKVQA (Schwenk et al., 2022)\\nspecifically require external knowledge, making\\nthem suitable for evaluating RAG systems’ ability\\nto retrieve and reason over such knowledge. For\\ndocument understanding, datasets such as DocVQA\\n(Mathew et al., 2021) and M3DocVQA (Cho et al.,\\n2024) are essential. As discussed in the benchmarks\\noverview above, unified evaluation frameworks like\\nM 2RAG (Ma et al., 2024d) provide a comprehen-\\nsive assessment across multiple tasks, including\\nimage captioning, visual question answering, and\\nfact verification.\\nEvaluating complex reasoning capabilities in multi-\\nmodal RAG systems has become increasingly im-\\nportant. Datasets such as MultimodalQA (Talmor\\net al., 2021), WebQA (Chang et al., 2022), and Sci-\\nenceQA (Lu et al., 2022) are specifically designed\\nto benchmark multi-hop reasoning abilities crucial\\nfor advanced RAG systems, with Dyn-VQA (Li\\net al., 2024d) additionally focusing on robustness to\\nchanging information.\\nComparative Analysis of Datasets Understanding\\nthe strategic trade-offs in dataset design is crucial for\\nmultimodal RAG development, as different dataset\\ncharacteristics serve distinct purposes across the\\nmodel development pipeline.\\n(i) Scale and Diversity vs. Curation: Large-scale\\ndatasets such as LAION-5B (Schuhmann et al.,\\n2022) and Conceptual Captions (Sharma et al., 2018)\\n27'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Mohammad Mahdi Abootorabi; Amirhosein Zobeiri; Mahdi Dehghani; Mohammadali Mohammadkhani; Bardia Mohammadi; Omid Ghahroodi; Mahdieh Soleymani Baghshah; Ehsaneddin Asgari', 'doi': 'https://doi.org/10.48550/arXiv.2502.08826', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2502.08826v3', 'source': '/content/Multimodal_RAG.pdf', 'total_pages': 34, 'page': 27, 'page_label': '28'}, page_content='provide substantial scale essential for pre-training,\\nenabling models to learn generalizable represen-\\ntations across diverse domains. However, their\\nreliance on web-crawled data introduces inherent\\nnoise that can compromise training quality. Con-\\nversely, smaller, meticulously curated datasets like\\nFlickr30K (Young et al., 2014) (31,000 images\\nwith human annotations) and domain-specific collec-\\ntions such as Fashionpedia (Jia et al., 2020) (48,000\\nimages with segmentation masks) prioritize anno-\\ntation quality over scale, making them essential\\nfor fine-tuning models and assessing specialized\\nperformance.\\n(ii) Modality Focus and Combination:While many\\nsystems aggregate unimodal datasets to construct\\nmultimodal contexts, datasets explicitly designed\\nfor multimodal tasks demonstrate superior align-\\nment between modalities. Foundational datasets\\nlike MS-COCO (Lin et al., 2014) and VQA (An-\\ntol et al., 2015) establish benchmarks for image-\\ntext understanding, while specialized collections\\nsuch as AudioSet (Gemmeke et al., 2017) (2 mil-\\nlion audio clips) and AudioCaps (Kim et al., 2019)\\n(46,000 audio clips with captions) address audio-\\nlanguage integration. Emerging modalities like 3D\\n(e.g., ShapeNet (Chang et al., 2015)) remain under-\\nrepresented, yet are essential for expanding RAG\\napplications into spatial reasoning domains.\\nTable 1 and Table 2 present a comprehensive\\noverview of datasets and benchmarks commonly\\nemployed in multimodal RAG research. The table\\nis organized into five columns:\\n• Category: This column categorizes each\\ndataset or benchmark based on its primary\\ndomain or modality. The datasets are grouped\\ninto eight categories: Image–Text General,\\nVideo–Text, Audio–Text, Medical, Fashion, 3D,\\nKnowledge & QA, and Other. The benchmarks\\nare grouped into two categories: Cross-Modal\\nUnderstanding and Text-Focused. This clas-\\nsification facilitates a clearer understanding\\nof each dataset or benchmark’s role within a\\nmultimodal framework.\\n• Name: The official name of the dataset or\\nbenchmarks is provided along with a citation\\nfor reference.\\n• Statistics and Description: This column sum-\\nmarizes key details such as dataset size, the\\nnature of the content (e.g., image–text pairs,\\nvideo captions, QA pairs), and the specific\\ntasks or applications for which the dataset or\\nbenchmarks are used. These descriptions are\\nintended to convey the dataset’s scope and its\\nrelevance to various multimodal RAG tasks.\\n• Modalities: The modalities covered by each\\ndataset or benchmark are indicated (e.g., Image,\\nText, Video, Audio, or 3D). Notably, several\\ndatasets are unimodal; however, within multi-\\nmodal RAG systems, these are combined with\\nothers to represent distinct aspects of a broader\\nmultimodal context.\\n• Link: A hyperlink is provided to direct readers\\nto the official repository or additional resources\\nfor the dataset or benchmark, thereby facili-\\ntating further exploration of its properties and\\napplications.\\nLimitations of Existing Datasets and Benchmarks\\nWhile the datasets and benchmarks discussed above\\nhave significantly advanced multimodal RAG re-\\nsearch, several limitations persist that offer important\\navenues for future work:\\n(i) Bias and Fairness: Large datasets, especially\\nthose scraped from the web, can inherit societal\\nbiases related to gender, race, or culture. This can\\nlead to skewed model behavior and unfair outcomes.\\nEfforts to create more balanced datasets are crucial,\\nbut comprehensive bias auditing across modalities\\nremains a challenge.\\n(ii) Annotation Quality and Noise: The trade-off\\nbetween dataset scale and annotation quality re-\\nmains a persistent challenge. While large datasets\\nfacilitate broad learning, their often noisy or weakly\\nsupervised labels (e.g., alt-text for images) can hin-\\nder precise model training. As demonstrated by\\nOHRBench (Zhang et al., 2024d), OCR errors ex-\\nemplify how noise in one modality can cascade and\\naffect overall RAG system performance.\\n(iii) Coverage and Generalization Gaps: Many\\ndatasets are domain-specific, which can limit the\\ngeneralization of models to out-of-domain scenar-\\nios. There is a need for more datasets covering a\\nwider array of real-world contexts and less common\\nmodalities.\\n(iv) Real-World Complexity and Long-Context\\nUnderstanding: Current datasets inadequately cap-\\nture real-world multimodal information complexity.\\nChallenges include efficient sampling of relevant\\nvideo frames, handling multi-page documents with\\nnumerous images, and processing dynamic infor-\\nmation environments; benchmarks like Dyn-VQA\\n28'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Mohammad Mahdi Abootorabi; Amirhosein Zobeiri; Mahdi Dehghani; Mohammadali Mohammadkhani; Bardia Mohammadi; Omid Ghahroodi; Mahdieh Soleymani Baghshah; Ehsaneddin Asgari', 'doi': 'https://doi.org/10.48550/arXiv.2502.08826', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2502.08826v3', 'source': '/content/Multimodal_RAG.pdf', 'total_pages': 34, 'page': 28, 'page_label': '29'}, page_content='(Li et al., 2024d) are, however, beginning to address\\nthis latter challenge.\\n(v) Lack of Adversarial and Robustness Testing:\\nWhile benchmarks like Counterfactual VQA (Niu\\net al., 2021) specifically test robustness against\\ncertain perturbations, there is a general scarcity of\\ndatasets containing multimodal adversarial examples\\nor structured negative instances. Such datasets are\\nvital for developing more robust and reliable RAG\\nsystems that can handle out-of-distribution inputs or\\nmisleading information.\\n(vi) Retrieval-Generation Integration: Many\\nbenchmarks evaluate retrieval and generation compo-\\nnents separately rather than assessing their synergis-\\ntic interplay. More holistic evaluation frameworks\\nare needed that jointly measure retrieval accuracy,\\nrelevance of retrieved multimodal context, and final\\noutput quality, as aimed by benchmarks like MRAG-\\nBench (Hu et al., 2024c) for visual integration and\\nRAG-Check (Mortaheb et al., 2025a) for retrieval\\nrelevance.\\n(vii) Limited Support for \"Any-to-Any\" Modali-\\nties: While current research primarily focuses on\\ntext, image, video, and audio, future RAG systems\\nare envisioned to support any-to-any modality inter-\\nactions. Existing datasets offer limited support for\\nsuch comprehensive multimodality.\\nC Evaluation and Metrics\\nEvaluating multimodal RAG models is complex due\\nto their varied input types and complex structure.\\nThe evaluation combines metrics from VLMs, gener-\\native AI, and retrieval systems to assess capabilities\\nlike text/image generation and information retrieval.\\nOur review found about 60 different metrics used\\nin the field. In the following paragraphs, we will\\nexamine the most important and widely used metrics\\nfor evaluating multimodal RAG.\\nRetrieval Evaluation Retrieval performance is\\nmeasured through accuracy, recall, and precision\\nmetrics, with an F1 score combining recall and pre-\\ncision. Accuracy is typically defined as the ratio of\\ncorrectly predicted instances to the total instances.\\nIn retrieval-based tasks, Top-K Accuracy is defined\\nas:\\nTop-K Accuracy(y, ˆf) = 1\\nn\\nn−1X\\ni=0\\nkX\\nj=1\\n⊮( ˆfi,j = yi)\\n(1)\\nRecall@K, which examines relevant items in top\\nK results, is preferred over standard recall. Mean\\nReciprocal Rank (MRR) serves as another key met-\\nric for evaluation, which is utilized by (Adjali et al.,\\n2024; Nguyen et al., 2024). MRR measures the rank\\nposition of the first relevant result in the returned\\nlist. The formula for calculating MRR is:\\nMRR = 1\\nQ\\nQX\\nq=1\\n1\\nrankq\\n(2)\\nwhere Q is the total number of queries. rankq is\\nthe rank of the first relevant result for query q.\\nModality Evaluation Modality-based evaluations\\nprimarily focus on text and image, assessing their\\nalignment, text fluency, and image caption quality.\\nFor text evaluation, metrics include Exact Match\\n(EM), BLEU (Papineni et al., 2002), ROUGE (Lin,\\n2004), and METEOR (Banerjee and Lavie, 2005).\\nThe ROUGE metric is commonly used to evaluate\\ntext summarization and generation. ROUGE-N mea-\\nsures the overlap of N-grams between the generated\\nand reference text. The formula for ROUGE-N is:\\nROUGE-N =\\nP\\ngramN ∈Ref Countmatch(gramN)\\nP\\ngramN ∈Ref Count(gramN)\\n(3)\\nROUGE-L measures the longest common subse-\\nquence (LCS) between generated and reference text.\\nThe formula for ROUGE-L is:\\nROUGE-L = LCS(X, Y )\\n|Y | (4)\\nBLEU is another metric used for assessing text\\ngeneration. The formula for calculating BLEU is:\\nBLEU(pn, BP) = BP · exp\\n NX\\nn=1\\nwn log pn\\n!\\n(5)\\nHere, pn represents the precision of n-grams, wn\\ndenotes the weight assigned to the n-gram precision,\\nand the Brevity Penalty (BP) is defined as:\\nBP =\\n(\\n1 length > rl\\nexp\\n\\x00\\n1 − rl\\ncl\\n\\x01\\nlength ≤ rl (6)\\nHere, rl represents the reference length and cl\\nrepresents the candidate length.\\n29'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Mohammad Mahdi Abootorabi; Amirhosein Zobeiri; Mahdi Dehghani; Mohammadali Mohammadkhani; Bardia Mohammadi; Omid Ghahroodi; Mahdieh Soleymani Baghshah; Ehsaneddin Asgari', 'doi': 'https://doi.org/10.48550/arXiv.2502.08826', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2502.08826v3', 'source': '/content/Multimodal_RAG.pdf', 'total_pages': 34, 'page': 29, 'page_label': '30'}, page_content='Multimodal RAG Application Domains\\nHealthcare and Medicine (§E)MMED-RAG (Xia et al., 2024a), RULE (Xia et al., 2024b), AsthmaBot (Bahaj and Ghogho, 2024),Realm (Zhu et al., 2024c), Su et al. (2024a), FactMM-RAG (Sun et al., 2024b), RA-RRG (Choi et al.,2025)\\nSoftware Engineering (§E) DocPrompting (Zhou et al., 2023), RACE (Shi et al., 2022), CEDAR (Nashid et al., 2023), RED-CODER (Parvez et al., 2021)\\nFashion and E-Commerce (§E)Unifashion (Zhao et al., 2024), Dang (2024), Fashion-RAG (Sanguigni et al., 2025), LLM4DESIGN (Chenet al., 2024d)\\nEntertainment and Social Computing (§E)SoccerRAG (Strand et al., 2024), MMRA (Zhong et al., 2024)\\nEmerging Applications (§E) RAG-Driver (Yuan et al., 2024), ENW AR (Nazar et al., 2024), Riedler and Langer (2024), Img2Loc (Zhouet al., 2024e)\\nFigure 3: Taxonomy of application domains for Multimodal Retrieval-Augmented Generation systems.\\nMultiRAGen (Shohan et al., 2024) uses Multilingual\\nROUGE for multilingual settings.\\nFor image captioning, CIDEr (Consensus-Based\\nImage Description Evaluation) (Vedantam et al.,\\n2015) measures caption quality using TF-IDF and\\ncosine similarity (Yasunaga et al., 2023; Zhao et al.,\\n2024; Luo et al., 2024a; Yuan et al., 2024; Shar-\\nifymoghaddam et al., 2024; Hu et al., 2023; Rao\\net al., 2024; Xu et al., 2024a; Kim et al., 2024;\\nZhang et al., 2024c), while SPICE (Semantic Propo-\\nsitional Image Caption Evaluation) (Anderson et al.,\\n2016) focuses on semantics. SPIDEr (Liu et al.,\\n2017), used in (Zhang et al., 2024c), combines both\\nmetrics.\\nFor semantic alignment, BERTScore (Zhang et al.,\\n2020) compares BERT embeddings (Sun et al.,\\n2024b; Shohan et al., 2024), and evaluates fluency\\n(Chen et al., 2022a; Zhi Lim et al., 2024; Ma et al.,\\n2024d).\\nCLIP Score (Hessel et al., 2021), used in (Shari-\\nfymoghaddam et al., 2024; Zhang et al., 2024c),\\nmeasures image-text similarity using CLIP (Radford\\net al., 2021). The formula for calculating CLIPScore\\nis:\\nCLIPScore = t.i\\n∥t∥∥i∥ (7)\\nwhere t and i are text and image embedding,\\nrespectively.\\nFor image quality, FID (Fréchet Inception Distance)\\n(Heusel et al., 2017) compares feature distributions\\n(Yasunaga et al., 2023; Zhao et al., 2024; Sharify-\\nmoghaddam et al., 2024; Zhang et al., 2024c), while\\nKID (Kernel Inception Distance) (Bi´nkowski et al.,\\n2018) provides an unbiased alternative. The formula\\nfor FID is:\\nFID = ∥µr − µg∥2 + tr(Σr + Σg − 2\\np\\nΣrΣg)\\n(8)\\nwhere µr and Σr are the mean and covariance of\\nreal images’ feature representations, respectively.\\nµg and Σg are the mean and covariance of generated\\nimages’ feature representations, respectively. To\\nextract features, InceptionV3 (Szegedy et al., 2016)\\nis typically used.\\nInception Score (IS) evaluates image diversity and\\nquality through classification probabilities (Zhi Lim\\net al., 2024). For audio evaluation, Zhang et al.\\n(2024c) use human annotators to assess sound qual-\\nity (OVL) and text relevance (REL), while also\\nemploying Fréchet Audio Distance (FAD) (Kilgour\\net al., 2019), an audio-specific variant of FID.\\nSystem efficiency is measured through FLOPs, exe-\\ncution time, response time, and retrieval time per\\nquery (Nguyen et al., 2024; Strand et al., 2024;\\nDang, 2024; Zhou, 2024). Domain-specific metrics\\ninclude geodesic distance for geographical accuracy\\n(Zhou et al., 2024e), and Clinical Relevance for\\nmedical applications (Lahiri and Hu, 2024).\\nD Robustness Advancements and Loss\\nFunctions\\nD.1 Robustness and Noise Management\\nMultimodal training faces challenges such as noise\\nand modality-specific biases (Buettner and Ko-\\nvashka, 2024). Managing noisy retrieval inputs is\\ncritical for maintaining model performance. MORE\\n(Cui et al., 2024) injects irrelevant results dur-\\ning training to enhance focus on relevant inputs.\\nAlzheimerRAG (Lahiri and Hu, 2024) uses progres-\\nsive knowledge distillation to reduce noise while\\nmaintaining multimodal alignment. RAGTrans\\n(Cheng et al., 2024) leverages hypergraph-based\\nknowledge aggregation to refine multimodal repre-\\nsentations, ensuring more effective propagation of\\nrelevant information. RA-BLIP (Ding et al., 2024b)\\nintroduces the Adaptive Selection Knowledge Gener-\\nation (ASKG) strategy, which leverages the implicit\\n30'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Mohammad Mahdi Abootorabi; Amirhosein Zobeiri; Mahdi Dehghani; Mohammadali Mohammadkhani; Bardia Mohammadi; Omid Ghahroodi; Mahdieh Soleymani Baghshah; Ehsaneddin Asgari', 'doi': 'https://doi.org/10.48550/arXiv.2502.08826', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2502.08826v3', 'source': '/content/Multimodal_RAG.pdf', 'total_pages': 34, 'page': 30, 'page_label': '31'}, page_content='capabilities of LLMs to filter relevant knowledge for\\ngeneration through a denoising-enhanced loss term,\\neliminating the need for fine-tuning. This approach\\nachieves strong performance compared to baselines\\nwhile significantly reducing computational overhead\\nby minimizing trainable parameters. RagVL (Chen\\net al., 2024e) improves robustness through noise-\\ninjected training by adding hard negative samples\\nat the data level and applying Gaussian noise with\\nloss reweighting at the token level, enhancing the\\nmodel’s resilience to multimodal noise. Finally,\\nRA-CM3 (Yasunaga et al., 2023) enhances gener-\\nalization using Query Dropout, which randomly\\nremoves query tokens during retrieval, serving as\\na regularization method that improves generator\\nperformance.\\nD.2 Loss Function\\nInfoNCE (Information Noise Contrastive Esti-\\nmation): The InfoNCE loss is commonly used in\\nself-supervised learning, especially in contrastive\\nlearning methods. The formula for InfoNCE loss is:\\nLInfoNCE = − log exp(sim(zi, zj)/τ)PK\\nk=1 exp(sim(zi, zk)/τ)\\n(9)\\nwhere zi and zj are the embeddings of a positive\\npair and τ is a temperature parameter.\\nGAN (Generative Adversarial Network): The\\nGAN loss consists of two parts: the discriminator\\nloss and the generator loss. The discriminator loss\\nformula is:\\nLD=−Ex∼pdata(x)[log D(x)]−Ez∼pz (z)[log(1−D(G(z)))]\\n(10)\\nwhere x is a real sample from the data distribution.\\nG(z) is the generated sample from the generator,\\nwhere z is a noise vector. D(x) is the probability\\nthat x is real.\\nThe Generator loss formula is:\\nLG = Ez∼pz(z)[log(1 − D(G(z)))] (11)\\nTriplet Loss: Triplet Loss is used in metric learn-\\ning to ensure that similar data points are closer\\ntogether while dissimilar ones are farther apart in\\nthe embedding space. The Triplet loss formula is:\\nL=PN\\ni=1 max(0,∥f (xi\\na)−f (xi\\np)∥2−∥f (xi\\na)−f (xi\\nn)∥2+α)\\n(12)\\nwhere xi\\na is the anchor sample. xi\\np and xi\\nn are the\\npositive and negative samples, respectively. f(x) is\\nthe neural network.\\nE Applications and Relevant Tasks\\nMultimodal RAG extends traditional RAG beyond\\nunimodal settings to cross-modal tasks. In content\\ngeneration, it enhances image captioning (Zhi Lim\\net al., 2024; Hu et al., 2023; Rao et al., 2024)\\nand text-to-image synthesis (Yasunaga et al., 2023;\\nChen et al., 2022b) by retrieving relevant contex-\\ntual information. It also improves coherence in\\nvisual storytelling and factual alignment in multi-\\nmodal summarization (Tonmoy et al., 2024). In\\nknowledge-intensive applications, multimodal RAG\\nsupports open-domain QA (Chen et al., 2024e; Ding\\net al., 2024b; Yuan et al., 2023), video-based QA\\n(Luo et al., 2024b), fact verification (Khaliq et al.,\\n2024), and zero-shot image–text retrieval (Yang\\net al., 2024b), grounding responses in retrieved\\nknowledge and thereby mitigating hallucinations.\\nAdditionally, the incorporation of chain-of-thought\\nreasoning (Zhai, 2024; Khaliq et al., 2024) further\\nenhances complex problem-solving and inference.\\nFinally, their integration into AI assistants such as\\nGemini (Team et al., 2024) enables natural language-\\ndriven visual search, document understanding, and\\nmultimodal reasoning.\\nMultimodal RAGs are increasingly applied across\\ndiverse domains, including healthcare, software en-\\ngineering, and creative industries (e.g., fashion and\\ndesign automation). The taxonomy of application\\ndomains can be seen in Figure 3. The following sec-\\ntions explore domain-specific adaptations of these\\ntechniques in greater depth.\\nHealthcare and Medicine Multimodal RAG en-\\nhances clinical decision-making through integrated\\nanalysis of medical imaging, electronic health\\nrecords, and biomedical literature. Systems like\\nMMED-RAG (Xia et al., 2024a) address diagnostic\\nuncertainty in medical visual question answering\\nby aligning radiology images with contextual pa-\\ntient data. In automated report generation, RULE\\n(Xia et al., 2024b) mitigates hallucinations through\\ndynamic retrieval of clinically similar cases. Simi-\\nlarly, RA-RRG (Choi et al., 2025) first leverages an\\nLLM to extract key textual phrases from a report\\ncorpus, then employs a multimodal retriever to link\\nthe visual features to these relevant phrases. The\\ncoherent report is generated after being retrieved\\nby another LLM without fine-tuning, thereby re-\\nducing hallucinations. FactMM-RAG (Sun et al.,\\n2024b) further automates radiology report drafting\\nby retrieving biomarker correlations from medi-\\ncal ontologies, exemplifying the paradigm’s ca-\\n31'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Mohammad Mahdi Abootorabi; Amirhosein Zobeiri; Mahdi Dehghani; Mohammadali Mohammadkhani; Bardia Mohammadi; Omid Ghahroodi; Mahdieh Soleymani Baghshah; Ehsaneddin Asgari', 'doi': 'https://doi.org/10.48550/arXiv.2502.08826', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2502.08826v3', 'source': '/content/Multimodal_RAG.pdf', 'total_pages': 34, 'page': 31, 'page_label': '32'}, page_content='pacity to operationalize expert knowledge at scale.\\nAsthmaBot (Bahaj and Ghogho, 2024) introduces\\na multimodal RAG-based approach for supporting\\nasthma patients across multiple languages, enabling\\nstructured, language-specific semantic searches. Pre-\\ndictive frameworks such as Realm (Zhu et al., 2024c)\\ndemonstrate robust risk assessment by fusing het-\\nerogeneous patient data streams, while Su et al.\\n(2024a) advance privacy-preserving architectures\\nfor federated clinical data integration.\\nSoftware Engineering Code generation systems\\nleverage multimodal RAG to synthesize context-\\naware solutions from technical documentation and\\nversion histories. DocPrompting (Zhou et al., 2023)\\nimproves semantic coherence in code completion by\\nretrieving API specifications and debugging patterns.\\nCommit message generation models like RACE\\n(Shi et al., 2022) contextualize code diffs against\\nhistorical repository activity, while CEDAR (Nashid\\net al., 2023) optimizes few-shot learning through\\nretrieval-based prompt engineering. REDCODER\\n(Parvez et al., 2021) enhances code summarization\\nvia semantic search across open-source repositories,\\npreserving syntactic conventions across program-\\nming paradigms.\\nFashion and E-Commerce Cross-modal align-\\nment drives advancements in product discovery\\nand design automation. UniFashion (Zhao et al.,\\n2024) enables style-aware retrieval by jointly em-\\nbedding garment images and textual descriptors,\\nwhile Dang (2024) reduces search friction through\\nmultimodal query expansion. For fashion image\\nediting, Fashion-RAG (Sanguigni et al., 2025) em-\\nploys a retrieval-augmented approach, retrieving\\ngarments that match textual descriptions and inte-\\ngrating their attributes into image generation via\\ntextual inversion techniques within diffusion mod-\\nels, ensuring personalized and contextually relevant\\noutputs. LLM4DESIGN (Chen et al., 2024d) demon-\\nstrates architectural design automation by retrieving\\ncompliance constraints and environmental impact\\nassessments, underscoring RAG’s adaptability to\\ncreative domains.\\nEntertainment and Social Computing Multime-\\ndia analytics benefit from RAG’s capacity to cor-\\nrelate heterogeneous signals. SoccerRAG (Strand\\net al., 2024) derives tactical insights by linking\\nmatch footage with player statistics. MMRA (Zhong\\net al., 2024) predicts content virality through joint\\nmodeling of visual aesthetics and linguistic engage-\\nment patterns.\\nEmerging Applications Autonomous systems\\nadopt multimodal RAG for explainable decision-\\nmaking, as seen in RAG-Driver’s (Yuan et al., 2024)\\nreal-time retrieval of traffic scenarios during nav-\\nigation. ENWAR (Nazar et al., 2024) enhances\\nwireless network resilience through multi-sensor\\nfusion, while Riedler and Langer (2024) streamline\\nequipment maintenance by retrieving schematics\\nduring fault diagnosis. Geospatial systems such\\nas Img2Loc (Zhou et al., 2024e) advance image\\ngeolocalization through cross-modal landmark cor-\\nrelation.\\nF Additional Future Directions\\nHigh computational costs in video frame sampling\\nand memory bottlenecks in processing multi-page\\ndocuments with images remain key challenges in\\nlong-context processing. Fixed extraction rates strug-\\ngle to capture relevant frames, requiring adaptive\\nselection based on content complexity and move-\\nment (Kandhare and Gisselbrecht, 2024). Addi-\\ntionally, retrieval speed-accuracy trade-offs in edge\\ndeployments and redundant computations in cross-\\nmodal fusion layers emphasize the need for efficient,\\nscalable architectures. Personalization mechanisms,\\nlike user-specific retrieval (e.g., adapting to medical\\nhistory), remain underexplored. As these systems\\nevolve, ensuring privacy and preventing sensitive\\ndata leakage in multimodal outputs is critical. Lastly,\\nthe lack of datasets with complex reasoning tasks\\nand multimodal adversarial examples limits robust\\nevaluation.\\n32'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Mohammad Mahdi Abootorabi; Amirhosein Zobeiri; Mahdi Dehghani; Mohammadali Mohammadkhani; Bardia Mohammadi; Omid Ghahroodi; Mahdieh Soleymani Baghshah; Ehsaneddin Asgari', 'doi': 'https://doi.org/10.48550/arXiv.2502.08826', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2502.08826v3', 'source': '/content/Multimodal_RAG.pdf', 'total_pages': 34, 'page': 32, 'page_label': '33'}, page_content='Table 1: Overview of Popular Datasets in Multimodal RAG Research.\\nCategory\\nName Statistics and Description Modalities Link\\nImage-Text General\\nLAION-400M (Schuhmann et al., 2021) 400M image–text pairs; used for pre-training multimodal models. Image, Text LAION-400M\\nConceptual-Captions (CC) (Sharma et al., 2018) More than 3M image–caption pairs; multilingual English–German image descriptions. Image, Text Conceptual Captions\\nCIRR (Liu et al., 2021) 36,554 triplets from 21,552 images; focuses on natural image relationships. Image, Text CIRR\\nMS-COCO (Lin et al., 2014) 330K images with captions; used for caption–to–image and image–to–caption generation. Image, Text MS-COCO\\nFlickr30K (Young et al., 2014) 31K images annotated with five English captions per image. Image, Text Flickr30K\\nMulti30K (Elliott et al., 2016) 30k German captions from native speakers and human–translated captions. Image, Text Multi30K\\nNoCaps (Agrawal et al., 2019) For zero–shot image captioning evaluation; 15K images. Image, Text NoCaps\\nLaion-5B (Schuhmann et al., 2022) 5.85B image–text pairs used as external memory for retrieval. Image, Text LAION-5B\\nCOCO-CN (Li et al., 2019) 20,341 images for cross-lingual tagging and captioning with Chinese sentences. Image, Text COCO-CN\\nCIRCO (Baldrati et al., 2023) 1,020 queries with an average of 4.53 ground truths per query; for composed image retrieval. Image, Text CIRCO\\nMINT-1T (Awadalla et al., 2024) 1T text tokens and 3.4B images; 10x larger than existing open-source datasets. Image, Text MINT-1T\\nShareGPT4V (Chen et al., 2024c) 1.2M images with GPT-4-generated captions, including spatial and factual details. Image, Text ShareGPT4V\\nOmniCorpus (Li et al., 2025b) 8.6B images and 1.7T tokens across 2.2B web documents; interleaved text-image layout. Image, Text OmniCorpus\\nVideo-Text\\nBDD-X (Kim et al., 2018) 77 hours of driving videos with expert textual explanations; for explainable driving behavior. Video, Text BDD-X\\nYouCook2 (Zhou et al., 2018) 2,000 cooking videos with aligned descriptions; focused on video–text tasks. Video, Text YouCook2\\nActivityNet (Caba Heilbron et al., 2015) 20,000 videos with multiple captions; used for video understanding and captioning. Video, Text ActivityNet\\nSoccerNet (Giancola et al., 2018) Videos and metadata for 550 soccer games; includes transcribed commentary and key event annotations. Video, Text SoccerNet\\nMSVD (Chen and Dolan, 2011) 1,970 videos with approximately 40 captions per video. Video, Text MSVD\\nLSMDC (Rohrbach et al., 2015) 118,081 video–text pairs from 202 movies; a movie description dataset. Video, Text LSMDC\\nDiDemo (Anne Hendricks et al., 2017) 10,000 videos with four concatenated captions per video; with temporal localization of events. Video, Text DiDemo\\nCOIN (Tang et al., 2019) 11,827 instructional YouTube videos across 180 tasks; for comprehensive instructional video analysis. Video, Text COIN\\nMSRVTT-QA (Xu et al., 2017) Video question answering benchmark. Video, Text MSRVTT-QA\\nActivityNet-QA (Yu et al., 2019) 58,000 human–annotated QA pairs on 5,800 videos; benchmark for video QA models. Video, Text ActivityNet-QA\\nEpicKitchens-100 (Damen et al., 2022) 700 videos (100 hours of cooking activities) for online action prediction; egocentric vision dataset. Video, Text EPIC-KITCHENS-100\\nEgo4D (Grauman et al., 2022) 4.3M video–text pairs for egocentric videos; massive–scale egocentric video dataset. Video, Text Ego4D\\nHowTo100M (Miech et al., 2019) 136M video clips with captions from 1.2M YouTube videos; for learning text–video embeddings. Video, Text HowTo100M\\nCharadesEgo (Sigurdsson et al., 2018) 68,536 activity instances from ego–exo videos; used for evaluation. Video, Text Charades-Ego\\nActivityNet Captions (Krishna et al., 2017) 20K videos with 3.7 temporally localized sentences per video; dense–captioning events in videos. Video, Text ActivityNet Captions\\nV ATEX (Wang et al., 2019) 34,991 videos, each with multiple captions; a multilingual video–and–language dataset. Video, Text V ATEX\\nWebVid (Bain et al., 2021) 10M video–text pairs (refined to WebVid-Refined-1M). Video, Text WebVid\\nInternVid (Wang et al., 2023b) 7M YouTube videos (760K hours), 234M clips, 4.1B words; used for video-text pretraining and representation learning. Video, Text InternVid\\nOpenVid-1M (Nan et al., 2024a) 1 million video-text pairs for multimodal learning. Video, Text OpenVid-1M\\nYouku-mPLUG (Xu et al., 2023) Chinese dataset with 10M video–text pairs (refined to Youku-Refined-1M). Video, Text Youku-mPLUG\\nAudio-Text\\nLibriSpeech (Panayotov et al., 2015) 1,000 hours of read English speech with corresponding text; ASR corpus based on audiobooks. Audio, Text LibriSpeech\\nSpeechBrown (Abootorabi and Asgari, 2024) 55K paired speech-text samples; 15 categories covering diverse topics from religion to fiction. Audio, Text SpeechBrown\\nAudioCaps (Kim et al., 2019) 46K audio clips paired with human-written text captions. Audio, Text AudioCaps\\nMusicCaps (Agostinelli et al., 2023) It is composed of 5.5k music-text pairs, with rich text descriptions provided by human experts. Audio, Text MusicCaps\\nClotho (Drossos et al., 2020) Audio captioning dataset with diverse soundscapes. Audio, Text Clotho\\nWavCaps (Mei et al., 2024) Large-scale weakly-labeled audio-text dataset, comprising approximately 400k audio clips with paired captions. Audio, Text WavCaps\\nSpoken SQuAD (Li et al., 2018) Audio version of the SQuAD dataset for spoken question answering, focusing on the listening comprehension task. Audio, Text Spoken SQuAD\\nAudioSet (Gemmeke et al., 2017) 2,084,320 human–labeled 10–second sound clips from YouTube; 632 audio event classes. Audio, Text AudioSet\\nMedical\\nMIMIC-CXR (Johnson et al., 2019) 125,417 training pairs of chest X–rays and reports. Image, Text MIMIC-CXR\\nCheXpert (Irvin et al., 2019) 224,316 chest radiographs of 65,240 patients; focused on medical analysis. Image, Text CheXpert\\nMIMIC-III (Johnson et al., 2016) Health-related data from over 40K patients (text data). Text MIMIC-III\\nIU-Xray (Pavlopoulos et al., 2019) 7,470 pairs of chest X–rays and corresponding diagnostic reports. Image, Text IU X-ray\\nPubLayNet (Zhong et al., 2019) 100,000 training samples and 2,160 test samples built from PubLayNet (tailored for the medical domain). Image, Text PubLayNet\\nQuilt-1M (Ikezogwo et al., 2023) 438K medical images with 768K text pairs; includes microscopic images and UMLS entities. Image, Text Quilt-1M\\nFashion\\nFashion-IQ (Wu et al., 2021) 77,684 images across three categories; evaluated with Recall@10 and Recall@50. Image, Text Fashion IQ\\nFashionGen (Rostamzadeh et al., 2018) 260.5K image–text pairs of fashion images and item descriptions. Image, Text Fashion-Gen\\nVITON-HD (Choi et al., 2021) 83K images for virtual try–on; high–resolution clothing items. Image, Text VITON-HD\\nFashionpedia (Jia et al., 2020) 48,000 fashion images annotated with segmentation masks and fine-grained attributes. Image, Text Fashionpedia\\nDeepFashion (Liu et al., 2016) Approximately 800K diverse fashion images for pseudo triplet generation. Image, Text DeepFashion\\n3D ShapeNet (Chang et al., 2015) Covering 55 common object categories with 51,300 unique 3D models. Text, 3D ShapeNet\\nKnowledge & QA\\nVQA (Antol et al., 2015) 400K QA pairs with images for visual question answering. Image, Text VQA\\nPAQ (Lewis et al., 2021) 65M text–based QA pairs; a large–scale dataset. Text PAQ\\nELI5 (Fan et al., 2019) 270K complex and diverse questions augmented with web pages and images. Text ELI5\\nMultimodalQA (Talmor et al., 2021) 29,918 questions requiring multi-modal multi-hop reasoning over text, tables, and images. Image, Text, Table MultimodalQA\\nViQuAE (Lerner et al., 2022) 11.8M passages from Wikipedia covering 2,397 unique entities; knowledge–intensive QA. Text ViQuAE\\nOK-VQA (Marino et al., 2019) 14K questions requiring external knowledge for VQA. Image, Text OK-VQA\\nWebQA (Chang et al., 2022) 46K queries that require reasoning across text and images. Text, Image WebQA\\nInfoseek (Chen et al., 2023) Fine-grained visual knowledge retrieval using a Wikipedia–based knowledge base ( 6M passages). Image, Text Infoseek\\nClueWeb22 (Overwijk et al., 2022) 10 billion web pages organized into three subsets; a large–scale web corpus. Text ClueWeb22\\nMOCHEG (Yao et al., 2023) 15,601 claims annotated with truthfulness labels and accompanied by textual and image evidence. Text, Image MOCHEG\\nVQA v2 (Goyal et al., 2017b) 1.1M questions (augmented with VG–QA questions) for fine-tuning VQA models. Image, Text VQA v2\\nA-OKVQA (Schwenk et al., 2022) Benchmark for visual question answering using world knowledge; around 25K questions. Image, Text A-OKVQA\\nXL-HeadTags (Shohan et al., 2024) 415K news headline-article pairs consist of 20 languages across six diverse language families. Text XL-HeadTags\\nDocVQA (Mathew et al., 2021) 12,767 diverse document images with 50K QA pairs, categorized by reasoning type to evaluate DocVQA methods. Image, Text DocVQA\\nChartQA (Masry et al., 2022) 9.6K human-written QA pairs + 23.1K generated from chart summaries. Image, Text ChartQA\\nDVQA (Kafle et al., 2018) 3.5M QA pairs on 300K diagrams, evaluating structure, data retrieval, and reasoning. Image, Text DVQA\\nRETVQA (Penamakuri et al., 2023) 416,000 QA samples where retrieval from a large image set is needed to answer questions; emphasizes RAG pipeline. Image, Text RETVQA\\nSEED-Bench (Li et al., 2023a) 19K multiple–choice questions with accurate human annotations across 12 evaluation dimensions. Text SEED-Bench\\nM3DocVQA (Cho et al., 2024) 2,441 multi-hop questions across 3,368 PDF documents; evaluates open-domain DocVQA. Image, Text M3DocVQA\\nMMLongBench-Doc (Ma et al., 2024c) 135 lengthy PDFs with 1,091 questions; focuses on multi-hop reasoning in single documents. Image, Text MMLongBench-Doc\\nOther\\nGeoDE (Ramaswamy et al., 2023) 61,940 images from 40 classes across 6 world regions; emphasizes geographic diversity in object recognition. Image GeoDE\\nRU-AI (Huang et al., 2025) 1.47M samples of real vs AI-generated content for fake detection robustness. Image, Text, Audio RU-AI\\nMIMIC-IT (Li et al., 2025a) 2.8M multimodal instruction-response pairs for model alignment. Image, Video, Text MIMIC-IT\\nMMVQA (Ding et al., 2024c) 262K question-answer pairs across 3,146 multipage research PDFs for robust multimodal information retrieval. Image, Text MMVQA\\n33'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Mohammad Mahdi Abootorabi; Amirhosein Zobeiri; Mahdi Dehghani; Mohammadali Mohammadkhani; Bardia Mohammadi; Omid Ghahroodi; Mahdieh Soleymani Baghshah; Ehsaneddin Asgari', 'doi': 'https://doi.org/10.48550/arXiv.2502.08826', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2502.08826v3', 'source': '/content/Multimodal_RAG.pdf', 'total_pages': 34, 'page': 33, 'page_label': '34'}, page_content='Table 2: Overview of Popular Benchmarks in Multimodal RAG Research.\\nCategoryName Statistics and Description Modalities Link\\nCross-Modal Understanding\\nMRAG-Bench (Hu et al., 2024c) Evaluates visual retrieval, integration, and robustness to irrelevant visual information. Images MRAG-Bench\\nM2RAG(Ma et al., 2024d) Benchmarks multimodal RAG; evaluates retrieval, multi-hop reasoning, and integration. Images + TextM2RAG\\nDyn-VQA (Li et al., 2024d) Focuses on dynamic retrieval, multi-hop reasoning, and robustness to changing information. Images + Text Dyn-VQA\\nMMBench (Liu et al., 2025c) Covers VQA, captioning, retrieval; evaluates cross-modal understanding across vision, text, and audio. Images + Text + Audio MMBench\\nScienceQA (Lu et al., 2022) Contains 21,208 questions; tests scientific reasoning with text, diagrams, and images. Images + Diagrams + Text ScienceQA\\nSK-VQA (Su et al., 2024b) Offers 2 million question-answer pairs; focuses on synthetic knowledge, multimodal reasoning, and external knowledge integration. Images + Text SK-VQA\\nSMMQG (Wu et al., 2024a) Includes 1,024 question-answer pairs; focuses on synthetic multimodal data and controlled question generation. Images + Text SMMQG\\nText-Focused\\nTriviaQA (Joshi et al., 2017) Provides 650K question-answer pairs; reading comprehension dataset, adaptable for multimodal RAG. Text TriviaQA\\nNatural Questions (Kwiatkowski et al., 2019) Contains 307,373 training examples; real-world search queries, adaptable with visual contexts. Text Natural Questions\\n34')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "splitting the text into chunks\n"
      ],
      "metadata": {
        "id": "iz_05J40wU3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-text-splitters"
      ],
      "metadata": {
        "id": "kUs1_D1NllR9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(pages))"
      ],
      "metadata": {
        "id": "sqBQFUKrxt5x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fa11c25-1b62-439d-a0fb-5f29cd7e1441"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "docs = text_splitter.split_documents(pages)"
      ],
      "metadata": {
        "id": "Qn5EhUepwZlj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[52].page_content)"
      ],
      "metadata": {
        "id": "TP2v5CLHypDM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5752108-a8fd-439c-e56b-6f414660a37e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(e.g., ASRs). Despite progress, mapping multimodal\n",
            "knowledge into a unified space remains an open\n",
            "challenge with substantial potential.\n",
            "Agent-Based and Self-Guided Systems Recent\n",
            "trends indicate a shift towards agent-based multi-\n",
            "modal RAGs that integrate retrieval, reasoning, and\n",
            "generation across diverse domains. Unlike static\n",
            "RAGs, future systems should incorporate interac-\n",
            "tive feedback and self-guided decision-making to\n",
            "iteratively refine outputs. Existing feedback mecha-\n",
            "nisms often fail to determine whether errors stem\n",
            "from retrieval, generation, or other stages (Dong\n",
            "et al., 2024b). The incorporation of reinforcement\n",
            "learning and end-to-end human-aligned feedback\n",
            "remains largely overlooked but holds significant\n",
            "potential for assessing whether retrieval is necessary,\n",
            "evaluating the relevance of retrieved content, and\n",
            "dynamically determining the most suitable modal-\n",
            "ities for response generation. Robust support for\n",
            "any-to-any modality is crucial for open-ended tasks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'docs' is your list of LangChain Document objects\n",
        "\n",
        "for doc in docs:\n",
        "    if 'ptex.fullbanner' in doc.metadata:\n",
        "        # 1. Rename the property using an underscore\n",
        "        doc.metadata['ptex_fullbanner'] = doc.metadata.pop('ptex.fullbanner')\n",
        "        # This removes the old key and adds the new, valid key"
      ],
      "metadata": {
        "id": "StNenw3H1Hvw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_db=WeaviateVectorStore.from_documents(\n",
        "    documents=docs,\n",
        "    embedding=embeddings,\n",
        "    client=client,\n",
        "    index_name=\"RAG_knowledgebase\",\n",
        "    text_key=\"text\",\n",
        "    by_text=False\n",
        ")"
      ],
      "metadata": {
        "id": "MwaH5NpByvsK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vector_db.similarity_search(\"what multimodal RAG\",k=3)[2].page_content)"
      ],
      "metadata": {
        "id": "7eTXvWIj0NeI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb4d34c5-8b12-4654-b399-b19387cdb899"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "field with many open questions, such as optimiz-\n",
            "ing fusion strategies for diverse modalities and ad-\n",
            "dressing scalability challenges. As new paradigms\n",
            "emerge, our taxonomy and conclusions will in-\n",
            "evitably evolve. To address these gaps, we plan\n",
            "to continuously monitor developments and update\n",
            "this survey and the corresponding repository to in-\n",
            "corporate overlooked contributions and refine our\n",
            "perspectives.\n",
            "7 Ethical Statement\n",
            "This survey provides a comprehensive review of\n",
            "research on multimodal RAG systems, offering in-\n",
            "sights that we believe will be valuable to researchers\n",
            "in this evolving field. All the studies, datasets,\n",
            "and benchmarks analyzed in this work are publicly\n",
            "available, with only a very small number of pa-\n",
            "pers requiring institutional access. Additionally,\n",
            "this survey does not involve personal data or user\n",
            "interactions, and we adhere to ethical guidelines\n",
            "throughout.\n",
            "Since this work is purely a survey of existing litera-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1hm22bdg3xQw"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template=\"\"\"You are an assistantfor question and answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know, don't try to make up an answer.Use 25 to 50 sentences maximum and keep the answer coincise.\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "Answer:\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "uzH8VsUB1ygL"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=ChatPromptTemplate.from_template(template)\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "rRqWsjXK33tN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0a0bb53-df86-42ee-b02c-c0cfb175beb9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['context', 'question'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistantfor question and answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know, don't try to make up an answer.Use 25 to 50 sentences maximum and keep the answer coincise.\\nQuestion: {question}\\nContext: {context}\\nAnswer:\\n\"), additional_kwargs={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the moodel through api key"
      ],
      "metadata": {
        "id": "kLhQnIhF4xtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-huggingface"
      ],
      "metadata": {
        "id": "pyJdeEdk52SR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_huggingface import HuggingFaceEndpoint\n",
        "# import os\n",
        "# from getpass import getpass\n",
        "\n",
        "# # Set your API token as an environment variable\n",
        "# # It's highly recommended to set it before running the script\n",
        "# if not os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"):\n",
        "#     os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass(\"Enter your Hugging Face API key: \")"
      ],
      "metadata": {
        "id": "aEfAtpvz7hp6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint"
      ],
      "metadata": {
        "id": "uHXqBjA-7WgZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# 1. Retrieve the token value from Colab Secrets\n",
        "hf_token_value = userdata.get('HF_TOKEN')\n",
        "\n",
        "# 2. Set it as an environment variable\n",
        "if hf_token_value:\n",
        "    os.environ[\"HF_TOKEN\"] = hf_token_value\n",
        "    print(\"HF_TOKEN successfully loaded from Colab secrets.\")\n",
        "else:\n",
        "    print(\"HF_TOKEN secret not found in Colab secrets.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5IeXBTDIh3m",
        "outputId": "c1276740-10d5-4c71-e3a8-ed8894629d28"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HF_TOKEN successfully loaded from Colab secrets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-core"
      ],
      "metadata": {
        "id": "Z97t-UEBJcHh"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "if the model supports conversational"
      ],
      "metadata": {
        "id": "gC3tO9e3SSFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# llm_endpoint = HuggingFaceEndpoint(\n",
        "#     repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "#     temperature=1.0,\n",
        "#     max_new_tokens=180\n",
        "# )\n"
      ],
      "metadata": {
        "id": "9gmaofUKPEa1"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "if the model supports text-generation but does not conversational"
      ],
      "metadata": {
        "id": "_Esd7IGcSbrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "\n",
        "llm_endpoint = HuggingFaceEndpoint(\n",
        "    repo_id=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    temperature=1.0,\n",
        "    max_new_tokens=180\n",
        ")\n",
        "\n",
        "chat_llm = ChatHuggingFace(llm=llm_endpoint)"
      ],
      "metadata": {
        "id": "uBElWsWPRfHy"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from langchain_core.schema.runnable import RunnablePassthrough\n",
        "#from langchain_core.schema.output_parser import StrOutputParser"
      ],
      "metadata": {
        "id": "wrjMz15d8yXk"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n"
      ],
      "metadata": {
        "id": "gAkyeJOwKRZR"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_parser=StrOutputParser()"
      ],
      "metadata": {
        "id": "iOKCuUTT-BLG"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever=vector_db.as_retriever()"
      ],
      "metadata": {
        "id": "lIL4SmiW8yAF"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain=(\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | chat_llm\n",
        "    | output_parser\n",
        ")"
      ],
      "metadata": {
        "id": "8OOEO76B8yKV"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_chain.invoke(\"What is rag and multimoadal rag?\")"
      ],
      "metadata": {
        "id": "h2ynlChV6H8G"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "bk58AoA_-gBU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65cb19bd-8c58-423e-e31f-230d13d4e949"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The context provided does not give information about what 'Rag' and 'Multimodal Rag' are.Neither term appears to be explained in the context, nor are they inverted or reverse dictionary terms, that I am corresponding to. I do not know the meaning of 'Rag' and 'Multimodal Rag' and do not have enough information to provide a definition.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(rag_chain.invoke(\"Can you give me brief summary about multimodal RAG\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLXbpGaoa1zM",
        "outputId": "89c1c264-9710-42fa-8e40-3091d2563372"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multimodal Retrieval-Augmented Generation (RAG) is a paradigm in AI research. It's a new approach to help the system effectively remember details for answering questions in different formats such that there's a knowledge retrieval and a knowledge generation part: it uses models to learn these details, use it for generating responses or better understanding a given format of data.\n",
            "\n",
            "Moreover, multimodal RAG task generally consists of combining information at different parts. And Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation  describes such a model in general architecture.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(rag_chain.invoke(\"Explain the Generation Techniques used in multimodal RAG?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDg79gefbAeS",
        "outputId": "45a8d7cf-c778-41a8-cf54-1cb038e66568"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't have information about the specific generation techniques used in multimodal RAG (Retrieval-Augmented Generation) in the provided context. The context appears to describe the structure and content of a document, possibly a survey or a comprehensive overview of multimodal RAG, but it doesn't mention generation techniques. Without more context or information, I'm unable to provide a detailed answer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4glGOvy3bkLE"
      },
      "execution_count": 49,
      "outputs": []
    }
  ]
}